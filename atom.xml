<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Wisdom from zhangpf</title>
  
  <subtitle>express clear viewpoint with plain words</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.zhangpf.com/"/>
  <updated>2020-02-08T15:21:27.437Z</updated>
  <id>https://blog.zhangpf.com/</id>
  
  <author>
    <name>zhangpf</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【译文】使用BPF控制内核的ops结构体</title>
    <link href="https://blog.zhangpf.com/2020/02/08/Translation-Kernel-operations-structures-in-BPF/"/>
    <id>https://blog.zhangpf.com/2020/02/08/Translation-Kernel-operations-structures-in-BPF/</id>
    <published>2020-02-08T01:55:59.000Z</published>
    <updated>2020-02-08T15:21:27.437Z</updated>
    
    <content type="html"><![CDATA[<!-- One of the more eyebrow-raising features to go into the 5.6 kernel is the ability to load TCP congestion-control algorithms as BPF programs; networking developer Toke Høiland-Jørgensen described it as a continuation of the kernel's "march towards becoming BPF runtime-powered microkernel". On its face, congestion control is a significant new functionality to hand over to BPF, taking it far beyond its existing capabilities. When one looks closer, though, one's eyebrow altitude may well increase further; the implementation of this feature breaks new ground in a couple of areas. --><p>Linux内核5.6版本的众多令人惊喜的功能之一是：TCP拥塞控制算法（congestion control algorithm）可作为用户空间的<a href="https://lwn.net/Articles/740157/" target="_blank" rel="noopener">BPF(Berkeley Packet Filter)</a>程序进行加载和执行。网络开发者Toke Høiland-Jørgensen将这项功能<a href="https://lwn.net/ml/bufferbloat/87bls8bnsm.fsf@toke.dk/" target="_blank" rel="noopener">描述为</a>“<strong>内核正向成为支持BPF运行时的微内核迈进(march towards becoming BPF runtime-powered microkernel)</strong>”的延续性动作。从外表上看，这是赋予给BPF的一项重要的新功能，使得拥塞控制将远远超过现有能力。但当我们深入研究后发现，其令人惊喜之处远不止这些，因为该功能的实现在多个方面都取得了新的进展。</p><!-- The use case for this feature seems clear enough. There are a number of such algorithms in use, each of which is suited for a different networking environment. There may be good reasons to distribute an updated or improved version of an algorithm and for recipients to be able to make use of it without building a new kernel or even rebooting. Networking developers can certainly benefit from being able to play with congestion-control code on the fly. One could argue that congestion control is not conceptually different from other tasks, such as flow dissection or IR protocol decoding, that can be done with BPF now — but congestion control does involve a rather higher level of complexity. --><p>该功能的使用场景和用例似乎都比较明确，因为有大量不同的拥塞控制算法已在使用中，且每种算法都适合于不同的网络环境。利用该功能，我们有充足方法来分发更新或改进后的控制算法，因为使用者能够在无需重新构建内核甚至无需重启的情况下使用新算法，使得网络功能开发者可从运行中的拥塞控制代码中获得好处。有人可能会质疑，拥塞控制功能在概念上与BPF现有的其它功能（例如<a href="https://lwn.net/Articles/764200/" target="_blank" rel="noopener">flow dissection</a>或<a href="https://lwn.net/Articles/759188/" target="_blank" rel="noopener">Infrared协议解码</a>）没有本质的不同，但需要指出的是，拥塞控制确实涉及到相当高的复杂性。</p><!-- A look at the patch set posted by Martin KaFai Lau reveals that what has been merged for 5.6 is not just a mechanism for hooking in TCP congestion-control algorithms; it is far more general than that. To be specific, this new infrastructure can be used to allow a BPF program to replace any "operations structure" — a structure full of function pointers — in the kernel. It is, at this point, only capable of replacing the tcp_congestion_ops structure used for congestion control; experience suggests, though, that other uses will show up sooner rather than later. --><p>如果看一下Martin KaFai Lau发布的<a href="https://lwn.net/ml/netdev/20191231062037.280596-1-kafai@fb.com/" target="_blank" rel="noopener">patch集合</a>，你就会发现5.6版本内核将要合并的代码不仅仅是一项能够hook住TCP拥塞控制的机制，其真实威力远不止于此。具体地说，这种新架构可用于允许BPF程序替换内核中的任何“ops结构(<code>struct xxx_ops</code>)”——一个由函数指针组成的结构。目前，虽然它只能替换用于拥塞控制的<a href="https://elixir.bootlin.com/linux/v5.5/source/include/net/tcp.h#L1043" target="_blank" rel="noopener"><code>struct tcp_congestion_ops</code>结构</a>，但大量的经验表明，在内核其他地方的应用将很快涌现。</p><!-- ## The user-space API --><h2 id="用户空间API"><a href="#用户空间API" class="headerlink" title="用户空间API"></a>用户空间API</h2><!-- On the user-space side, loading a new operations structure requires a few steps, the first of which is to use the [`bpf()` system call](http://www.man7.org/linux/man-pages/man2/bpf.2.html) to load an implementation of each function as a separate BPF program. The new `BPF_PROG_TYPE_STRUCT_OPS` type has been defined for these programs. In the attributes passed with each program, user space must provide the BPF type format (BTF) ID corresponding to the structure being replaced (specifying the actual function being implemented comes later). BTF is a relatively recent addition that describes the functions and data structures in the running kernel; it is currently used for [type-checking of tracing functions](https://lwn.net/Articles/803258/) among other purposes. --><p>在用户空间中，加载新的<code>ops</code>结构需要如下几个步骤。首先，使用<a href="http://www.man7.org/linux/man-pages/man2/bpf.2.html" target="_blank" rel="noopener"><code>bpf()</code></a>系统调用以单独的BPF程序对每个函数的实现进行加载，这些BPF程序已经可以使用新的<code>BPF_PROG_TYPE_STRUCT_OPS</code>类型定义ops。用户空间在每个程序提供的属性中，必须提供与要替换的结构相对应的BPF类型格式（BPF Type Format，BTF）的ID（同时用于指定稍后要实现的实际功能）。 BTF是一项较新的特性，它描述了正在运行的内核中的函数和数据结构，目前用于<a href="https://lwn.net/Articles/803258/" target="_blank" rel="noopener">追踪函数的类型检查</a>。</p><!-- User space must also specify an integer offset identifying the function this program will replace. For example, the ssthresh() member of `struct tcp_congestion_ops` is the sixth field defined there, so this offset will be passed as five (since offsets start at zero). How this API might interact with [structure layout randomization](https://lwn.net/Articles/722293/) is not entirely clear. --><p>用户空间还必须指定一个整数偏移量，以标识此程序将要替换的函数。例如，<code>struct tcp_congestion_ops</code>的函数指针字段<code>ssthresh()</code>在结构中位于第六个字段，因此将5作为偏移量进行传递（偏移量从0开始）。目前还不明确该API如何与<a href="https://lwn.net/Articles/722293/" target="_blank" rel="noopener">结构布局随机化（structure layout randomization）</a>进行交互。</p><!-- As the programs for each structure member are loaded, the kernel will return a file descriptor corresponding to each. Then, user space must populate a structure that looks like this: --><p>在加载每个结构字段对应的程序时，内核将返回与每个结构字段相对应的文件描述符。为了使用此描述符，用户空间还必须填充如下的结构：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">bpf_tcp_congestion_ops</span> &#123;</span></span><br><span class="line">    <span class="keyword">refcount_t</span> refcnt;</span><br><span class="line">    <span class="keyword">enum</span> bpf_struct_ops_state state;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">tcp_congestion_ops</span> <span class="title">data</span>;</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><!-- The `data` field has the type of the structure to be replaced — `struct tcp_congestion_ops` in this case. Rather than containing function pointers, though, this structure should contain the file descriptors for the programs that have been loaded to implement those functions. The non-function fields of that structure should be set as needed, though the kernel can override things as described below. --><p>上面的代码中，<code>data</code>字段的类型是将要替换的结构——在拥塞控制中也就是<code>struct tcp_congestion_ops</code>，但是，此结构应包含已加载用于实现对应拥塞控制功能的程序的文件描述符，而非函数指针。尽管内核可以按如下所述覆盖内容，但也应根据需要设置该结构中的非函数字段。</p><!-- The last step is to load this structure into the kernel. One might imagine a number of ways of doing this; the actual implementation is almost certainly something else. User space must create a special BPF map with the new `BPF_MAP_TYPE_STRUCT_OPS` type. Associated with this map is the BTF type ID of a special structure in the kernel (described below); that is how the map is connected with the structure that is to be replaced. Actually replacing the structure is accomplished by storing the `bpf_tcp_congestion_ops` structure filled in above into element zero of the map. It is also possible to query the map (to see the reference-count and state fields) or to remove the structure by deleting element zero. --><p>最后一步，是将该结构加载到内核中，有多种方式来达到该目的，因此实际的实现几乎可以肯定是另外的方式。用户空间必须使用新添加的<code>BPF_MAP_TYPE_STRUCT_OPS</code>类型创建一个特殊的BPF映射，与该映射相关联的是内核中特殊结构的BTF类型ID（如下所述），这就是将映射与要替换的结构连接在一起的方式。实际的结构替换是通过将上面的<code>bpf_tcp_congestion_ops</code>结构存储到零填充的映射中来完成的，此外还支持的操作包括：查询映射（以获取引用计数和状态字段）和通过删除元素0来删除结构。</p><!-- BPF maps have grown in features and capability over the years. Even so, this seems likely to be the first place where map operations have this kind of side effect elsewhere in the kernel. It is arguably not the most elegant of interfaces; most user-space developers will never see most of it, though, since it is, like most of the BPF API, hidden behind a set of macros and magic object-file sections in the `libbpf` library. --><p>近年来，BPF映射相关的功能和特性不断的出现，即便如此，这次添加的新功能似乎是映射操作首次在内核产生类似副作用的方法。也许本功能不是最优雅的接口，但大多数用户空间的开发者将永远看不到它背后的大部分细节，因为它就像其他大多数BPF的API一样，隐藏在<code>libbpf</code>库中的一系列宏和对象的背后。</p><!-- ## The kernel side --><h2 id="内核空间"><a href="#内核空间" class="headerlink" title="内核空间"></a>内核空间</h2><!-- Replacing an operations structure requires support in the kernel; there is no ability for user space to replace arbitrary structures at will. To make it possible to replace a specific type of structure, kernel code must create a structure like this: --><p>由于用户空间无权限任意替换结构，所以替换ops结构需要内核的支持，为了支持这样的替换，内核态必须新添加如下结构：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BPF_STRUCT_OPS_MAX_NR_MEMBERS 64</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">bpf_struct_ops</span> &#123;</span></span><br><span class="line">    <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">bpf_verifier_ops</span> *<span class="title">verifier_ops</span>;</span></span><br><span class="line">    <span class="keyword">int</span> (*init)(struct btf *btf);</span><br><span class="line">    <span class="keyword">int</span> (*check_member)(<span class="keyword">const</span> struct btf_type *t,</span><br><span class="line">                        <span class="keyword">const</span> struct btf_member *member);</span><br><span class="line">    <span class="keyword">int</span> (*init_member)(<span class="keyword">const</span> struct btf_type *t,</span><br><span class="line">                        <span class="keyword">const</span> struct btf_member *member,</span><br><span class="line">                        <span class="keyword">void</span> *kdata, <span class="keyword">const</span> <span class="keyword">void</span> *udata);</span><br><span class="line">    <span class="keyword">int</span> (*reg)(<span class="keyword">void</span> *kdata);</span><br><span class="line">    <span class="keyword">void</span> (*unreg)(<span class="keyword">void</span> *kdata);</span><br><span class="line">    <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">btf_type</span> *<span class="title">type</span>;</span></span><br><span class="line">    <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">btf_type</span> *<span class="title">value_type</span>;</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">char</span> *name;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">btf_func_model</span> <span class="title">func_models</span>[<span class="title">BPF_STRUCT_OPS_MAX_NR_MEMBERS</span>];</span></span><br><span class="line">    u32 type_id;</span><br><span class="line">    u32 value_id;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><!-- There are more details here than can be easily covered in this article, and some of the fields of this structure are automatically filled in by macros. The `verifier_ops` structure has a number of functions used to verify that the individual replacement functions are safe to execute. There is a new field added to that structure in this patch set, `struct_access()`, which regulates which parts, if any, of the operations structure itself can be changed by BPF functions. --><p>本文无法包含所有这些代码的细节，并且由于宏的存在，它自动填充此结构的某些字段。 值得说明的是，<code>verifier_ops</code>结构中有多个函数，可用于验证各个替换功能是否可安全执行。在即将合并的补丁集中，该结构中添加了一个新字段：<code>struct_access()</code>，其用于控制BPF函数可以更改ops结构本身的哪些部分（如果有的话）。</p><!-- The `init()` function will be called first to do any needed global setup. `check_member()` determines whether a specific member of the target structure is allowed to be implemented in BPF, while `init_member()` verifies the exact value of any fields in that structure. In particular, `init_member()` can validate non-function fields (flags fields, for example). The `reg()` function actually registers the replacement structure after the checks have passed; in the congestion-control case, it will install the `tcp_congestion_ops` structure (with the appropriate BPF trampolines used for the function pointers) where the network stack will use it. `unreg()` undoes that action. --><p>内核在获取到用户空间的请求后，首先调用<code>init()</code>函数，来进行一切所必需的全局设置。<code>check_member()</code>函数决定是否允许目标结构的特定成员在BPF中实现，而<code>init_member()</code>则用于验证该结构中所有字段的确切值，特别地，<code>init_member()</code>可以验证非函数字段（例如flag字段）。 在检查通过后，则通过<code>reg()</code>函数进行实际地注册替换结构，具体地，在拥塞控制的场景下，该函数将<code>tcp_congestion_ops</code>结构（和用于函数指针的BPF相关的trampoline）安装在网络栈中将要使用的位置。相反地，<code>unreg()</code>则用于撤消操作。</p><!-- One structure of this type should be created with a specific name: the type of the structure to be replaced with `bpf_` prepended. So the operations structure for the replacement of a `tcp_congestion_ops` structure is named [`bpf_tcp_congestion_ops`](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/net/ipv4/bpf_tcp_ca.c#n241). This is the "special structure" that user space must reference (via its BTF ID) when loading a new operations structure. Finally, a line is added to `kernel/bpf/bpf_struct_ops_types.h`:     --><p>这种类型的结构应使用特定名称创建，即添加<code>bpf_</code>前缀。因此，用于替换<code>tcp_congestion_ops</code>结构的ops结构的名字为<a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/net/ipv4/bpf_tcp_ca.c#n241" target="_blank" rel="noopener"><code>bpf_tcp_congestion_ops</code></a>，这是加载新的ops结构时用户空间必须（通过BTF的ID）引用的“特殊结构”。最后，在<code>kernel/bpf/bpf_struct_ops_types.h</code>中添加如下的一行代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BPF_STRUCT_OPS_TYPE(tcp_congestion_ops)</span><br></pre></td></tr></table></figure><!-- The lack of a trailing semicolon is necessary. By virtue of some macro magic and including this file four times into [`bpf_struct_ops.c`](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/kernel/bpf/bpf_struct_ops.c), everything is set up without the need of a special function to register this structure type. --><p>借助宏操作，以及将此文件四次include到<a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/kernel/bpf/bpf_struct_ops.c" target="_blank" rel="noopener"><code>bpf_struct_ops.c</code></a>中，便可处理好所有设置，而无需特殊的函数注册该结构类型。</p><!-- ## In closing --><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><!-- For the curious, the kernel-side implementation of `tcp_congestion_ops` replacement can be found in [`net/ipv4/bpf_tcp_ca.c`](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/net/ipv4/bpf_tcp_ca.c). There are two actual algorithm implementations ([DCTCP](https://git.kernel.org/linus/09903869f69f) and [CUBIC](https://git.kernel.org/linus/6de4a9c430b5)) in the tree as well.The ability to replace an arbitrary operations structure in the kernel potentially holds a lot of power; a huge portion of kernel code is invoked through at least one such structure. If one could replace all or part of the [`security_hook_heads` structure](https://elixir.bootlin.com/linux/v5.5/source/include/linux/lsm_hooks.h#L1831), one could modify security policies in arbitrary ways, similar to what is proposed with [KRSI](https://lwn.net/Articles/808048/), for example. Replacing a [`file_operations` structure](https://elixir.bootlin.com/linux/v5.5/source/include/linux/fs.h#L1821) could rewire just about any part of the kernel's I/O subsystem. And so on. --><p><code>tcp_congestion_ops</code>替换机制中内核态的实现可以在<a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/net/ipv4/bpf_tcp_ca.c" target="_blank" rel="noopener"><code>net/ipv4/bpf_tcp_ca.c</code></a>文件中找到，源码树中已有两种不同控制算法的实现（<a href="https://git.kernel.org/linus/09903869f69f" target="_blank" rel="noopener">DCTCP</a>和<a href="https://git.kernel.org/linus/6de4a9c430b5" target="_blank" rel="noopener">CUBIC</a>）。</p><p>可替换内核中任意ops结构是一项潜在的强大功能，因为内核中很大一部分代码是通过这种类型的结构调用的。比如说，如果可以替换全部或部分<a href="https://elixir.bootlin.com/linux/v5.5/source/include/linux/lsm_hooks.h#L1831" target="_blank" rel="noopener"><code>security_hook_heads</code>结构</a>，则可以以任意方式修改安全策略，例如，实现类似于<a href="https://lwn.net/Articles/808048/" target="_blank" rel="noopener">KRSI</a>的功能。还有，替换<a href="https://elixir.bootlin.com/linux/v5.5/source/include/linux/fs.h#L1821" target="_blank" rel="noopener"><code>file_operations</code>结构</a>几乎可以重写内核I/O子系统的任何部分。</p><!-- Nobody is proposing to do any of these things — yet — but this sort of capability is sure to attract interested users. There could come a time when just about any kernel functionality is amenable to being hooked or replaced with BPF code from user space. In such a world, users will have a lot of power to change how their systems operate, but what we think of as a "Linux kernel" will become rather more amorphous, dependent on which code has been loaded from user space. The result is likely to be interesting. --><p>目前还没有任何人提出类似的方法，但是这样的功能肯定会吸引感兴趣的开发者。将来会有某个时刻，几乎任何内核功能都可以被用户空间的BPF代码hook或替换，那时用户将拥有改变系统运行方式的强大能力，但是我们认为“Linux内核”将变得更加充满不确定性，这也取决于从用户空间加载了哪些代码。结果可能会很有趣。</p><p><em>(译者注：本文原地址为 <a href="https://lwn.net/Articles/811631/" target="_blank" rel="noopener">https://lwn.net/Articles/811631/</a>)</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- One of the more eyebrow-raising features to go into the 5.6 kernel is the ability to load TCP congestion-control algorithms as BPF prog
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="Linux" scheme="https://blog.zhangpf.com/tags/Linux/"/>
    
      <category term="eBPF" scheme="https://blog.zhangpf.com/tags/eBPF/"/>
    
      <category term="ops结构" scheme="https://blog.zhangpf.com/tags/ops%E7%BB%93%E6%9E%84/"/>
    
      <category term="microkernel" scheme="https://blog.zhangpf.com/tags/microkernel/"/>
    
  </entry>
  
  <entry>
    <title>【译文】将Restartable Sequcences (rseq)引入Linux的五年之旅</title>
    <link href="https://blog.zhangpf.com/2020/01/23/Translation-The-5-year-journey-to-bring-restartable-sequences-to-Linux/"/>
    <id>https://blog.zhangpf.com/2020/01/23/Translation-The-5-year-journey-to-bring-restartable-sequences-to-Linux/</id>
    <published>2020-01-23T01:34:01.000Z</published>
    <updated>2020-01-29T02:19:30.132Z</updated>
    
    <content type="html"><![CDATA[<!-- Concurrency control algorithms, paired with per-CPU data, are integral to ensuring low-level libraries and high-performance applications scale properly on today's hardware. These algorithms guarantee that user-space data structures are kept consistent in the face of simultaneous accesses, and that modifications are done wholly so that threads see either the before or after state, but not in between. --><p>并发控制算法与per-CPU数据成对出现，是确保底层库和高性能应用程序在当今硬件上正确扩展不可或缺的一部分。这些算法保证在并发访问时，用户空间的数据结构始终保持一致，并且用户数据的修改是执行完全的，使得线程观察到的是之前或之后的状态，而非中间状态。</p><!-- There are a number of ways to design these algorithms. The most common, though least scalable, is with mutual exclusion which works by allowing only one thread to modify the data structure at any one time — and that's the thread holding the mutex. --><p>设计这些算法的方法有很多种，最常见的也是扩展性最差的一种是mutex。它的工作原理是，在任何时间，只允许一个线程持有mutex并修改共享数据结构。</p><!-- But mutual exclusion doesn't scale well, particularly with per-CPU data. Since only one thread can be within the critical section bookended by the mutex lock and unlock sequence, it's possible to have many threads waiting to acquire the lock. And time waiting for locks is time not spent doing useful work. --><p>但是，mutex并不能很好地扩展，尤其是对于per-CPU数据而言。由于在mutex加锁和解锁的临界区内只能有一个线程可访问，因此可能存在大量线程在等待锁，而等待的时间内线程没有做任何有用的事。</p><!--The next step up on the list of scalable algorithms is atomic compare and swap. In this model, a single instruction is used such as cmpxchg or the lock prefix in the case of x86.--><p>接下来的可扩展并发控制算法是，原子比较和交换。在这个模型中，通常使用单个指令进行同步控制，例如<code>cmpxchg</code>指令或x86体系结构的指令lock前缀。</p><!--The problem here though is that atomic instructions are relatively expensive on modern processors. The x86 lock prefix can easily add many cycles to the cost of the same instruction without the prefix. To make matters worse, regardless of whether there's only one thread executing the instruction or not the lock prefix will be executed.--><p>但这里的问题是，在现代处理器上原子指令代价很高，相比于没有前缀的相同指令，x86的lock前缀很容易在执行成本上增加多个指令周期。更糟糕的是，无论是否只有一个线程在竞争，lock前缀都会无条件执行。</p><!--And not all architectures provide a single instruction for atomically updating data. For instance, ARM uses link-load/store-conditional instructions to read the old data, modify it, and write the new data. If two threads try to write the same data simultaneously only one will succeed and the other will fail and restart the read-modify-write sequence.--><p>并且，并不是所有体系结构都为原子数据更新提供单独的指令。例如，ARM使用link-load/store-conditional（简称LL/SC）组合指令来读取原始数据、修改原始数据并写入新数据，如果两个线程同时尝试写入，只有一个线程将会成功，另一个线程失败并重启LL/SC指令序列。</p><!--The Linux kernel uses other methods of protecting per-CPU data, such as disabling preemption or interrupts, or using “percpu” local atomic operations. Unfortunately, these mechanisms are either not readily available to user-space, or they're comparatively slow (as is the case with the atomic instructions). Instead, what's needed is a lightweight mechanism for user-space to protect per-CPU data. This is the motivation for restartable sequences.--><p>Linux内核使用其他方法来保护per-CPU数据，例如禁用抢占或中断，或使用per-CPU本地的原子操作。遗憾的是，这些方法要么不易被用户空间使用，要么相对较慢（例如原子指令）。所以，我们需要一种轻量型的机制，用于在用户空间内保护per-CPU数据，这就是restartable sequences方法（rseq）的产生的动机。</p><!-- ## How Restartable Sequences Work --><h2 id="rseq是如何工作的"><a href="#rseq是如何工作的" class="headerlink" title="rseq是如何工作的"></a>rseq是如何工作的</h2><!-- Restartable sequences are built with a new system call, rseq(2), that tells the kernel where the restartable sequence's thread-local storage ABI (a struct rseq object) is located for the current thread. This object contains a rseq_cs field which is a pointer to the currently active restartable sequence critical section descriptor (a struct rseq_cs object). Only one critical section can be active at any time. This ABI has two purposes: user-space restartable sequences and a way to quickly read the current CPU number. --><p>rseq由新的系统调用<code>rseq(2)</code>所组成，该调用告诉内核当前线程rseq相关的thread-local ABI（<code>sturct rseq</code>对象）在内存中的位置。<code>sturct rseq</code>对象包含一个<code>rseq_cs</code>类型字段，该字段是指向当前被激活的rseq临界区的描述符（<code>sturct rseq_cs</code>对象）的指针，而在任何时候，只能有一个临界区被激活。此ABI有两个用途：用户空间的rseq和快速读取当前CPU编号。</p><!--The critical section is subdivided into preparatory and commit stages, where the commit step is a single CPU instruction. The critical section is said to have been interrupted if any of the following occur before the commit step:The thread is migrated to another CPUA signal is delivered to the threadThe thread is preempted--><p>临界区可细分为准备阶段（preparatory stage）和提交阶段（commit stages），其中提交阶段是仅是单条CPU指令。如果在提交阶段之前发生以下任何情况之一，则认为当前临界区被中断：</p><ol><li>线程已迁移到另一个CPU上</li><li>信号（signal）被传递到该线程</li><li>线程被抢占</li></ol><!--Additionally, because a fallback mechanism is required when the thread is interrupted, the kernel sets the instruction pointer to the start of an abort handler which can take some corrective action, for example retrying the preparatory step.--><!-- With this design, the optimistic case (where the thread isn't interrupted) is extremely fast because expensive locks and atomic instructions can be entirely avoided. --><!-- Getting down into the details, the current restartable sequence critical section is described by a struct rseq_cs object which is referenced by a struct rseq object. Here's a diagram to illustrate the relationship and the structure members. --><p>此外，由于线程被中断时需要回退机制，内核将程序寄存器（instruction pointer）指向中断处理程序的首地址，该处理程序需要执行一些纠正性的措施，例如重新发起准备阶段。在这样的设计下，乐观情况下（线程未被中断）执行速度非常快，因为开销较高的<code>struct rseq_cs</code>的上锁和原子指令可完全避免。</p><p>更详细地说，当前rseq临界区是由<code>struct rseq_cs</code>对象所描述，该对象被<code>struct rseq</code>对象所引用。下面用如下的图来说明它们的关系和结构体的字段。</p><img src="/2020/01/23/Translation-The-5-year-journey-to-bring-restartable-sequences-to-Linux/restartable-sequences-diagram.png" title="rseq_cs结构体和字段描述(来自于 www.efficios.com)"> <!-- The start and end of the restartable sequence are denoted by the start_ip and post_commit_ip (which points to the instruction after the commit instruction), and the abort_ip points to the first instruction of the abort handler. --><p>rseq的开始和结束由<code>start_ip</code>和<code>post_commit_ip</code>（指向提交阶段后的首地址指令）所表示，而<code>abort_ip</code>指向中断处理程序的首地址指令。</p><!-- There are restrictions on the implementation of both the critical section and the abort handler. For one, the abort handler must be outside the critical section. Secondly, system calls are not permitted within a critical section, and attempting to execute a system call will cause the process to be terminated via a segmentation fault. --><p>值得注意的是，临界区和中断处理程序的实现都有所限制。例如，中止处理程序必须处于临界区以外，以及在临界区内不允许有系统调用，尝试执行系统调用将导致进程发生segmentation fault而终止。</p><!-- The cpu_id field is updated by the kernel whenever a thread migrates to a new CPU and user-space programs are expected to read cpu_id_start at the beginning of the critical section and compare the two values. If they differ then the running thread was interrupted and the restartable sequence needs to be retried. --><p>每当线程迁移到其他CPU上执行，且在用户空间程序临界区的开始处读取<code>cpu_id_start</code>并比较这两个值时，内核就会更新<code>cpu_id</code>字段。如果它们的值不同，则正在运行的线程将被中断，且需要重新尝试rseq序列。</p><!-- The rseq_cs field is modified by both the kernel and user-space. When beginning a restartable sequence, user-space code needs to set the pointer to the current critical section descriptor. The kernel sets this pointer to NULL whenever it preempts or delivers a signal while executing a region of code that lays outside of the critical section range described by the current rseq_cs descriptor. --><p>内核和用户空间均可修改<code>rseq_cs</code>字段。当启动rseq时，用户空间代码需要将指针设置为当前临界区的描述符。每当在执行当前<code>rseq_cs</code>描述符所描述的临界区范围之外的代码时，或发生抢断或传递信号时，就会将该指针设置为<code>NULL</code>。</p><h2 id="rseq的简史"><a href="#rseq的简史" class="headerlink" title="rseq的简史"></a>rseq的简史</h2><!-- Support for restartable sequences was merged in Linux 4.18. The concept of restartable sequences, or rseq, was originally proposed by Paul Turner and Andrew Hunter in 2013 as a way to safely access per-CPU data from user-space data without locks or expensive atomic instructions. But at the time no patches were available. --><p>Linux在4.18内核版本中合并了对rseq的支持。作为一种无需锁或开销较高的原子指令，即可从用户空间数据中安全访问per-CPU数据的方法，restartable sequences的概念最初是由<a href="https://blog.linuxplumbersconf.org/2013/ocw/system/presentations/1695/original/LPC%20-%20PerCpu%20Atomics.pdf" target="_blank" rel="noopener">Paul Turner和Andrew Hunter在2013年所提出</a>，但在当时还没有可用的patch。</p><!-- After two years — and in an effort to cajole them into posting their patches to the Linux kernel mailing list — Mathieu Desnoyers submitted his own patches for per-CPU critical sections in May 2015. A month later, the very first patch series for restartable sequences was posted by Paul. After momentum died down on Paul's version, Mathieu picked it up in 2016 and submitted a new series and covered them in a talk at Linux Plumbers Conference 2016. He had hoped to get the patches merged for version 4.15 of the Linux kernel but there was a snag… --><p>两年后，为了促使他们将其补丁发布到Linux kernel的mailing list中，Mathieu Desnoyers于2015年5月提交了针对<a href="https://lwn.net/Articles/645717/" target="_blank" rel="noopener">per-CPU临界区</a>的patch。一个月后，Paul发布了<a href="https://lwn.net/Articles/649288/" target="_blank" rel="noopener">rseq的第一个patch集合</a>。虽然Paul在发布该版本之后便停了下来，Mathieu于2016年又重新接手，提交了新的<a href="https://lwn.net/Articles/697756/" target="_blank" rel="noopener">patch集合</a>，并在<a href="https://blog.linuxplumbersconf.org/2016/ocw/system/presentations/3873/original/presentation-rseq-lpc2016.pdf" target="_blank" rel="noopener">LPC 2016</a>上介绍了这一工作。他原本希望将patch合并到Linux内核的4.15版本中，但发现存在如下的障碍：</p><!-- While benchmark data was available with pretty much every version of the patch series, Linus made it clear that hypothetical use cases were not reason enough to merge the restartable sequences feature, and that concrete performance numbers were necessary. --><p>虽然几乎每个版本的patch集都有benchmark数据，但Linus明确表示，这种假设的用例<a href="https://lwn.net/Articles/697991/" target="_blank" rel="noopener">不足以</a>合并rseq的相关功能，并需要具体的<a href="https://www.mail-archive.com/linux-kernel@vger.kernel.org/msg1213874.html" target="_blank" rel="noopener">性能数据</a>作为支撑。</p><!-- Facebook had already provided results for using the patches with the jemalloc memory allocator. So, Mathieu set about collecting more benchmark results and getting rseq support ready for other projects such as LTTng-UST, Userspace RCU, and glibc. --><!-- Finally, after five years the series was merged into the Linux kernel, and Mathieu gave a talk at the Open Source Summit Europe 2018 entitled Improve Linux User-Space Core Libraries with Restartable Sequences which covered the multi-year effort of bringing restartable sequences to Linux. --><p>后来，Facebook提供了在jemalloc内存分配器上使用patch的<a href="https://lwn.net/Articles/661839/" target="_blank" rel="noopener">数据结果</a>。因此，Mathieu收集了更多类似的benchmark结果，并在其他项目（如<a href="https://github.com/lttng/lttng-ust" target="_blank" rel="noopener">LTTng-UST</a>、<a href="https://liburcu.org/" target="_blank" rel="noopener">Userspace RCU</a>和<a href="https://www.gnu.org/s/libc/" target="_blank" rel="noopener">glibc</a>）上提供了rseq的支持。</p><p>最终，在最初开始的五年之后，该patch集终于被合并到Linux内核中，Mathieu在Open Source Summit Europe 2018上作了名为<a href="https://events.linuxfoundation.org/wp-content/uploads/2017/12/Improve-Linux-User-Space-Core-Libraries-with-Restartable-Sequences-Mathieu-Desnoyers-EfficiOS.pdf" target="_blank" rel="noopener">Improve Linux User-Space Core Libraries with Restartable Sequences</a>的演讲，其中介绍了将rseq带入Linux的多年努力。</p><!-- ## How to Use rseq in your Library or Application --><h2 id="如何在库和程序中使用rseq"><a href="#如何在库和程序中使用rseq" class="headerlink" title="如何在库和程序中使用rseq"></a>如何在库和程序中使用rseq</h2><!-- The preferred approach to using restartable sequences is to use the librseq library which provides all the per-CPU operations you're likely to need such as making rseq(2) available to the current thread (rseq_register_current_thread()), looking up the CPU number of the current thread (rseq_current_cpu()), and updating per-CPU data (rseq_cmpeqv_storev()). --><p>使用rseq的首选方法是使用<a href="https://github.com/compudj/librseq" target="_blank" rel="noopener">librseq</a>，该库提供了可能会用到的所有per-CPU操作，例如使<code>rseq(2)</code>调用对当前线程可用（<code>rseq_register_current_thread()</code>），查询当前线程的CPU编号（<code>rseq_current_cpu()</code>），以及更新per-CPU数据（<code>rseq_cmpeqv_storev()</code>）。</p><!-- But if you want to roll your own operations, read on for a more detailed explanation. --><p>但如果要实现自己需要的特定操作，请继续阅读以获得更详细的说明。</p><!-- Using rseq(2) requires two steps. First, you need to enable the functionality for the current thread with the rseq(2) system call. The system call has the following prototype: --><p>使用<code>rseq(2)</code>需要以下两步。首先，使用<code>rseq(2)</code>为当前线程启用该功能，该系统调用具有以下的函数原型：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sys_rseq(struct rseq *rseq, <span class="keyword">uint32_t</span> rseq_len, <span class="keyword">int</span> flags, <span class="keyword">uint32_t</span> sig)</span><br></pre></td></tr></table></figure><!-- The purpose of the system call is to register a struct rseq object with the kernel. The flags argument is 0 for registration and rseq_FLAG_UNREGISTER for unregistration. The sig argument is a signature that can be used to validate the rseq context, in other words the signature used for registration must be the same one used for unregistration. --><p>该系统调用的目的是向内核注册<code>struct rseq</code>对象，其中<code>flags</code>参数为0表示注册，<code>rseq_FLAG_UNREGISTER</code>表示注销。<code>sig</code>参数是可用于验证rseq上下文的签名，也就是说，用于注册的签名必须与用于注销的签名相同。</p><!--Let's assume you want to increment a per-CPU counter using rseq(2). To do that, you need to get the CPU number of the current thread (stored in the cpu_id_start field of struct rseq) and modify the per-CPU counter using a restartable sequence. This is done with a mixture of C and assembly. Here's the code to do that.--><p>比如说，你想使用<code>rseq(2)</code>来增加per-CPU计数器的值，为此，需要获取当前线程的CPU编号（存储在<code>struct rseq</code>的<code>cpu_id_start</code>字段中），并使用rseq修改per-CPU计数器的值。因此，需要通过C和汇编混写的代码实现，下面是完成该操作的代码。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> _GNU_SOURCE</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/rseq.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;syscall.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdint.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/syscall.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> __thread <span class="keyword">volatile</span> <span class="class"><span class="keyword">struct</span> <span class="title">rseq</span> __<span class="title">rseq_abi</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> rseq_SIG0x53053053</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">sys_rseq</span><span class="params">(<span class="keyword">volatile</span> struct rseq *rseq_abi, <span class="keyword">uint32_t</span> rseq_len,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> flags, <span class="keyword">uint32_t</span> sig)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> syscall(__NR_rseq, rseq_abi, rseq_len, flags, sig);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">register_thread</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> rc;</span><br><span class="line">rc = sys_rseq(&amp;__rseq_abi, <span class="keyword">sizeof</span>(struct rseq), <span class="number">0</span>, rseq_SIG);</span><br><span class="line"><span class="keyword">if</span> (rc) &#123;</span><br><span class="line"><span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Failed to register rseq\n"</span>);</span><br><span class="line"><span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> rseq_ACCESS_ONCE(x)     (*(__volatile__  __typeof__(x) *)&amp;(x))</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">rseq_addv</span><span class="params">(<span class="keyword">intptr_t</span> *v, <span class="keyword">intptr_t</span> count, <span class="keyword">int</span> cpu)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">__asm__ __<span class="function">volatile__ <span class="title">goto</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">".pushsection __rseq_table, \"aw\"\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">".balign 32\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"cs_obj:\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">".long 0, 0\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="comment">/* start_ip, post_commit_ip, abort_ip */</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">".quad 1f, 2f, 4f\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">".popsection\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"1:\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"leaq cs_obj(%%rip), %%rax\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"movq %%rax, %[rseq_cs]\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"cmpl %[cpu_id], %[current_cpu_id]\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"jnz 4f\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"addq %[count], %[v]\n\t"</span><span class="comment">/* final store */</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"2:\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">".pushsection __rseq_failure, \"ax\"\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="comment">/* Disassembler-friendly signature: nopl &lt;sig&gt;(%rip). */</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">".byte 0x0f, 0x1f, 0x05\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">".long 0x53053053\n\t"</span><span class="comment">/* rseq_FLAGS */</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"4:\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"jmp abort\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">".popsection\n\t"</span></span></span></span><br><span class="line"><span class="function"><span class="params">: <span class="comment">/* gcc asm goto does not allow outputs */</span></span></span></span><br><span class="line">       : [cpu_id]              "r" (cpu),</span><br><span class="line">[current_cpu_id]      <span class="string">"m"</span> (__rseq_abi.cpu_id),</span><br><span class="line">[rseq_cs]             <span class="string">"m"</span> (__rseq_abi.rseq_cs),</span><br><span class="line"><span class="comment">/* final store input */</span></span><br><span class="line">[v]                   <span class="string">"m"</span> (*v),</span><br><span class="line">[count]               <span class="string">"er"</span> (count)</span><br><span class="line">: <span class="string">"memory"</span>, <span class="string">"cc"</span>, <span class="string">"rax"</span></span><br><span class="line">: <span class="built_in">abort</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"><span class="built_in">abort</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> cpu, ret;</span><br><span class="line"><span class="keyword">intptr_t</span> *cpu_data;</span><br><span class="line"><span class="keyword">long</span> nr_cpus = sysconf(_SC_NPROCESSORS_ONLN);</span><br><span class="line"></span><br><span class="line">cpu_data = <span class="built_in">calloc</span>(nr_cpus, <span class="keyword">sizeof</span>(*cpu_data));</span><br><span class="line"><span class="keyword">if</span> (!cpu_data) &#123;</span><br><span class="line">perror(<span class="string">"calloc"</span>);</span><br><span class="line"><span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">register_thread();</span><br><span class="line">cpu = rseq_ACCESS_ONCE(__rseq_abi.cpu_id_start);</span><br><span class="line">ret = rseq_addv(&amp;cpu_data[cpu], <span class="number">1</span>, cpu);</span><br><span class="line"><span class="keyword">if</span> (ret)</span><br><span class="line"><span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Failed to increment per-cpu counter\n"</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"cpu_data[%d] == %ld\n"</span>, cpu, cpu_data[cpu]);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- The code in rseq_addv() begins by filling out a struct rseq_cs object that describes the segment of the restartable sequence denoted by the start symbol 1, the post-commit address 2, and the abort handler 4. If the thread does not complete the sequence between labels 1 and 2 control jumps to the 4 label and then to the abort label in C. --><p><code>rseq_addv()</code>中的代码以<code>struct rseq_cs</code>对象填充作为开始，该对象描述了rseq中的字段，其中start的label为<code>1</code>，post-commit为<code>2</code>，中断处理程序为<code>4</code>。如果线程未完成<code>1</code>和<code>2</code>之间的序列，那么将直接控制跳转到标签<code>4</code>，然后跳转到C中的<code>abort</code>位置处。</p><!-- A word of caution: you must ensure that the CPU number is only read once. Compilers need to be coerced into guaranteeing that with the volatile keyword. The rseq_ACCESS_ONCE() macro above guarantees this. --><p>注意：必须确保CPU编号只读取一次，在编译器层面需要强制使用<code>volatile</code>关键字来保证这一点，而在上面的例子中，<code>rseq_ACCESS_ONCE()</code>宏对此提供了保证。</p><!-- ## Exactly How Much Faster is rseq? --><h2 id="rseq到底有多快"><a href="#rseq到底有多快" class="headerlink" title="rseq到底有多快?"></a>rseq到底有多快?</h2><!-- One of the main use cases for restartable sequence is getting the CPU number that the current thread is executing on — usually to then be used as an index into a per-CPU data structure. The existing method of using sched_getcpu() to discover the CPU number requires a system call on ARM and invokes the VDSO on x86. But rseq(2) allows you to read a cached CPU number value in the struct rseq object shared between kernel and user-space. --><p>rseq的主要使用场景之一是获取执行当前线程的CPU编号，通常也就是指向per-CPU数据结构的索引值。当前使用<code>sched_getcpu()</code>来获取CPU编号的方法，在ARM上需进行系统调用，在 x86上需调用VDSO，而<code>rseq(2)</code>则允许程序直接读取内核和用户空间之间共享的<code>struct rseq</code>对象中缓存的CPU编号值。</p><!-- The rseq approach results in a 20x speedup on x86 and 35x speedup on ARM. --><p>在该场景下，rseq在X86平台可获得20倍加速，而在ARM平台上则是35倍加速。</p><!-- Here's a graph demonstrating the relative speed improvements of the alternative methods of getting the CPU number for the current thread. In all the following graphs, smaller values are better and reflect a speed increase. --><p>下图展示了获取执行当前线程的CPU编号的rseq方法的速度提升，值越小越好，其反映了速度的提升。</p><table><thead><tr><th style="text-align:center"><img src="/2020/01/23/Translation-The-5-year-journey-to-bring-restartable-sequences-to-Linux/rseq-arm32-getcpu.png"></th><th style="text-align:center"><img src="/2020/01/23/Translation-The-5-year-journey-to-bring-restartable-sequences-to-Linux/rseq-x86-64-getcpu.png"></th></tr></thead><tbody><tr><td style="text-align:center"> 在arm32上读取当前CPU编号benchmark(来自于 <a href="http://www.efficios.com" target="_blank" rel="noopener">www.efficios.com</a>)</td><td style="text-align:center">在x86_64上读取当前CPU编号benchmark(来自于 <a href="http://www.efficios.com" target="_blank" rel="noopener">www.efficios.com</a>)</td></tr></tbody></table><!-- As discussed above, there are a number of use-cases that rseq is suitable for. One of the most common uses of per-CPU data is for storing counters. The following graph shows the speed improvements when incrementing a per-CPU counter with rseq(2) versus using sched_getcpu() and atomic instructions. ARM shows an 11x speedup and x86 shows 7.7x. --><p>如上所述，rseq也适用于其他多种使用per-CPU数据的场景，其中之一是存储计数器值。下图展示了使用<code>rseq(2)</code>增加per-CPU计数器时，相对于使用<code>sched_getcpu()</code>和原子指令的速度提升。在ARM平台上显示有11倍的提升，而在x86显示是7.7倍提升。</p><table><thead><tr><th style="text-align:center"><img src="/2020/01/23/Translation-The-5-year-journey-to-bring-restartable-sequences-to-Linux/rseq-arm32-percpu-stats.png"></th><th style="text-align:center"><img src="/2020/01/23/Translation-The-5-year-journey-to-bring-restartable-sequences-to-Linux/rseq-x86-64-percpu-stats.png">    </th></tr></thead><tbody><tr><td style="text-align:center"> 在arm32上统计增加计数器benchmark(来自于 <a href="http://www.efficios.com" target="_blank" rel="noopener">www.efficios.com</a>)</td><td style="text-align:center">在x86_64上统计增加计数器benchmark(来自于 <a href="http://www.efficios.com" target="_blank" rel="noopener">www.efficios.com</a>)</td></tr></tbody></table><!--  <img src="/2020/01/23/Translation-The-5-year-journey-to-bring-restartable-sequences-to-Linux/rseq-arm32-percpu-stats.png" title="在arm32上统计计数器增加benchmark（来自 www.efficios.com）">     <img src="/2020/01/23/Translation-The-5-year-journey-to-bring-restartable-sequences-to-Linux/rseq-x86-64-percpu-stats.png" title="在x86_64上统计计数器增加benchmark（来自 www.efficios.com）">     –&gt;<!-- LTTng-UST uses per-CPU buffers to store events. The graph below shows the performance improvement when storing 32-bit header and 32-bit event payload into a per-CPU buffer using sched_getcpu() and atomic instructions or rseq. ARM received a 1.1x speedup and x86 saw a 1.2x improvement. --><p>LTTng-UST使用per-CPU的buffer来存储event。下图展示了使用<code>rseq(2)</code>在per-CPU缓存中存储32位header和event时，相对于使用<code>sched_getcpu()</code>和原子指令的速度提升。在ARM平台上显示有1.1x的提升，而在x86显示是1.2x提升。</p><table><thead><tr><th style="text-align:center"><img src="/2020/01/23/Translation-The-5-year-journey-to-bring-restartable-sequences-to-Linux/rseq-arm32-lttng-ust.png"></th><th style="text-align:center"><img src="/2020/01/23/Translation-The-5-year-journey-to-bring-restartable-sequences-to-Linux/rseq-x86-64-lttng-ust.png"></th></tr></thead><tbody><tr><td style="text-align:center"> 在arm32的LTTng-UST上将event写入per-CPU缓存的benchmark(来自于 <a href="http://www.efficios.com" target="_blank" rel="noopener">www.efficios.com</a>)</td><td style="text-align:center">在x86_64的LTTng-UST上将event写入per-CPU缓存的benchmark(来自于 <a href="http://www.efficios.com" target="_blank" rel="noopener">www.efficios.com</a>)</td></tr></tbody></table><!-- And finally, using rseq in liburcu from the Userspace RCU project results in a 5.8x speedup on ARM and a 1.9x speedup on x86. --><p>最后，在Userspace RCU项目中，在liburcu库中使用rseq后，在ARM有5.8倍加速，而在x86上有1.9倍加速。</p><table><thead><tr><th style="text-align:center"><img src="/2020/01/23/Translation-The-5-year-journey-to-bring-restartable-sequences-to-Linux/rseq-arm32-liburcu.png"></th><th style="text-align:center"><img src="/2020/01/23/Translation-The-5-year-journey-to-bring-restartable-sequences-to-Linux/rseq-x86-64-liburcu.png"></th></tr></thead><tbody><tr><td style="text-align:center"> 在arm32的liburcu的per-CPU上加/解锁，解引用读/比较的benchmark(来自 于<a href="http://www.efficios.com" target="_blank" rel="noopener">www.efficios.com</a>)</td><td style="text-align:center">在x86_64的liburcu的per-CPU上加/解锁，解引用读/比较的benchmark(来自于<a href="http://www.efficios.com" target="_blank" rel="noopener">www.efficios.com</a>)</td></tr></tbody></table><!-- ## What's Next? --><h2 id="下一步计划"><a href="#下一步计划" class="headerlink" title="下一步计划"></a>下一步计划</h2><!-- While patches to use rseq(2) exist for LTTng, Userspace RCU, and glibc, however they're only at the proof of concept stage. The next phase is to get them merged into their respective projects. For glibc, that means patches to automatically register with rseq(2) at thread start time, unregister on thread exit, and at NPTL initialization for the main thread. --><!-- LTTng-UST is in a slightly different situation because of a shortcoming with the current design: it's not possible to move data between per-CPU data structures without also needing to change the thread's affinity mask. To solve this problem Mathieu has proposed a new cpu_opv system call that executes a vector of fixed operations (comparison, memcpy, and add) on a specific CPU, similar to the struct iovec concept with readv(2) and writev(2). Another issue with rseq(2) that is solved with cpu_opv is that if you single-step through a critical section the debugger will loop forever. The new cpu_opv system call will allow debuggers to work with existing applications even if libraries make use of rseq(2). --><!-- Mathieu had originally hoped to get the new cpu_opv system call merged in time for the Linux kernel 4.21 release, but Linus Torvalds has made it clear that he wants to see users of rseq(2) implemented first. Meaning those work in progress rseq(2) patches for glibc need to be merged. --><p>虽然使用<code>rseq(2)</code>的patch适用于LTTng、Userspace RCU和glibc，但它们现在仅处于概念验证阶段。下一阶段的工作，则是将它们合并到各自项目的代码中。对于glibc而言，这意味着patch在线程开始时自动通过<code>rseq(2)</code>注册，在线程退出，以及主线程的NPTL初始化时自动注销。</p><p>LTTng-UST的问题有点不同：不更改线程的affinity mask，就无法在per-CPU数据结构之间移动数据。为了解决这个问题，Mathieu提出了一个新的<a href="https://lore.kernel.org/lkml/20181101095844.24462-1-mathieu.desnoyers@efficios.com/T/#u" target="_blank" rel="noopener"><code>cpu_opv</code>系统调用</a>，类似<code>readv(2)</code>和<code>writev(2)</code>的<code>struct iovec</code>概念，该调用在特定CPU上执行固定向量操作（比较、memcpy 和add）。<code>cpu_opv</code>解决的<code>rseq(2)</code>的另一个问题是，如果单步执行到临界区，调试器将死循环。即使库使用了<code>rseq(2)</code>，新的<code>cpu_opv</code>系统调用也允许调试器与现有应用程序共存。</p><p>Mathieu最初希望能及时将新的<code>cpu_opv</code>系统调用合并到Linux内核的4.21版本，但Linus Torvalds已经表示，他希望看到<a href="https://lore.kernel.org/lkml/CAHk-=wjk-2c4XvWjdzc-bs9Hbgvy-p7ASSnKKphggr5qDoXRDQ@mail.gmail.com/T/#u" target="_blank" rel="noopener"><code>rseq(2)</code>的使用者</a>首先出现，这意味着glibc需要合并那些正在进行的<code>rseq(2)</code>的patch工作。</p><p><em>(译者注：本文原地址为 <a href="https://www.efficios.com/blog/2019/02/08/linux-restartable-sequences/" target="_blank" rel="noopener">https://www.efficios.com/blog/2019/02/08/linux-restartable-sequences/</a>)</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- 
Concurrency control algorithms, paired with per-CPU data, are integral to ensuring low-level libraries and high-performance applicatio
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="Linux" scheme="https://blog.zhangpf.com/tags/Linux/"/>
    
      <category term="Restartable Sequcences" scheme="https://blog.zhangpf.com/tags/Restartable-Sequcences/"/>
    
      <category term="Concurrency" scheme="https://blog.zhangpf.com/tags/Concurrency/"/>
    
      <category term="并发" scheme="https://blog.zhangpf.com/tags/%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>【译文】在.NET上通过Wasmtime使用WebAssembly</title>
    <link href="https://blog.zhangpf.com/2020/01/18/Translation-Using-WebAssembly-from-NET-with-Wasmtime/"/>
    <id>https://blog.zhangpf.com/2020/01/18/Translation-Using-WebAssembly-from-NET-with-Wasmtime/</id>
    <published>2020-01-18T08:06:49.000Z</published>
    <updated>2020-01-29T02:19:30.147Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><!-- Wasmtime, the WebAssembly runtime from the Bytecode Alliance, recently added an early preview of an API for .NET Core, Microsoft’s free, open-source, and cross-platform application runtime. This API enables developers to programmatically load and execute WebAssembly code directly from their .NET programs..NET Core is already a cross-platform runtime, so why should .NET developers pay any attention to WebAssembly?There are several reasons to be excited about WebAssembly if you’re a .NET developer, such as sharing the same executable code across platforms, being able to securely isolate untrusted code, and having a seamless interop experience with the upcoming WebAssembly interface types proposal. --><p>来自<a href="https://bytecodealliance.org/articles/announcing-the-bytecode-alliance" target="_blank" rel="noopener">字节码联盟（Bytecode Alliance）</a>的WebAssembly（以下简称wasm）运行时——<a href="https://github.com/bytecodealliance/wasmtime/" target="_blank" rel="noopener">Wasmtime</a>，最近添加了针对.NET Core的早期预览版本API，开发者可以在他们的.NET程序中使用该API直接编程加载和运行wasm代码。</p><p>那么问题来了，.NET Core已经是跨平台的运行时，为什么.NET开发者还需关注wasm？</p><p>如果你是.NET开发者，那么这里有几个让你对wasm感到兴奋的地方，这包括<strong>在所有平台上使用同一份可执行代码</strong>，<strong>安全地隔离不可信代码</strong>，以及<strong>通过即将来临的wasm接口类型提案（WebAssembly interface type proposal）来获得无缝的互操作体验</strong>等。</p><!-- # Share more code across platforms --><h3 id="在平台间共享更多的代码"><a href="#在平台间共享更多的代码" class="headerlink" title="在平台间共享更多的代码"></a>在平台间共享更多的代码</h3><!-- .NET assemblies can already be built for cross-platform use, but using a native library (for example, a library written in C or Rust) can be difficult because it requires native interop and distributing a platform-specific build of the library for each supported platform.However, if the native library were compiled to WebAssembly, the same WebAssembly module could be used across many different platforms and programming environments, including .NET; this would simplify the distribution of the library and the applications that depend on it. --><p>.NET编译生成二进制代码已经可以跨平台使用，但使用本地库（例如，通过C或Rust写成的库）却依然比较困难，因为它需要原生的互操作，并且为每一个所支持的平台提供单独的构建。</p><p>然而，如果C或者Rust库被编译成wasm模块，那么同一个模块可以被不同的平台和编程环境所使用，其中也包括.NET环境，这将大大简化库和使用这些库的应用的分发。</p><!-- ## Securely isolate untrusted code --><h3 id="安全地隔离不可信代码"><a href="#安全地隔离不可信代码" class="headerlink" title="安全地隔离不可信代码"></a>安全地隔离不可信代码</h3><!-- The .NET Framework attempted to sandbox untrusted code with technologies such as Code Access Security and Application Domains, but ultimately these failed to properly isolate untrusted code. As a result, Microsoft deprecated their use for sandboxing and ultimately removed them from .NET Core.Have you ever wanted to load untrusted plugins in your application but couldn’t figure out a way to prevent the plugin from invoking arbitrary system calls or from directly reading your process’ memory? You can do this with WebAssembly because it was designed for the web, an environment where untrusted code executes every time you visit a website.A WebAssembly module can only call the external functions it explicitly imports from a host environment, and may only access a region of memory given to it by the host. We can leverage this design to sandbox code in a .NET program too! --><p>.NET曾设计使用代码访问安全性（Code Access Security）和应用程序域（Application Domain）技术来沙箱隔离不可信代码，但最终这些技术都未能有效地对不可信代码进行隔离。结果微软最后放弃了沙箱化，并最终将这些技术从.NET Core中移除。</p><p>可是，你是否曾经在你的应用中加载不可信插件时，却找不到一种方法来防止插件进行任意系统调用或者直接读取进程的内存。现在，可以通过wasm来达到该目的，因为wasm最初是为任意性很强的Web环境所设计，在Web环境中，每当用户访问网站时，不可信代码都无时不刻在执行。</p><!-- ## Improved interoperability with interface types --><h3 id="通过接口类型改进互操作性"><a href="#通过接口类型改进互操作性" class="headerlink" title="通过接口类型改进互操作性"></a>通过接口类型改进互操作性</h3><!-- The WebAssembly interface types proposal introduces a way for WebAssembly to better integrate with programming languages by reducing the amount of glue code that is necessary to pass more complex types back and forth between the hosting application and a WebAssembly module.When support for interface types is eventually implemented by the Wasmtime for .NET API, it will enable a seamless experience for exchanging complex types between WebAssembly and .NET. --><p>wasm的接口类型提案引入了一种新方法，该方法可以减少在托管应用程序和wasm模块之间来回传递更复杂类型所需的粘合代码。新方法的目的是为了wasm更好地与编程语言所集成。</p><p>当接口类型最终被wasmtime为.NET所支持后，它将为在wasm和.NET之间交换复杂类型提供无缝的编程体验。 </p><h2 id="深入研究通过-NET使用wasm"><a href="#深入研究通过-NET使用wasm" class="headerlink" title="深入研究通过.NET使用wasm"></a>深入研究通过.NET使用wasm</h2><!-- In this article we’ll dive into using a Rust library compiled to WebAssembly from .NET with the Wasmtime for .NET API, so it will help to be a little familiar with the C# programming language to follow along.The API described here is fairly low-level. That means that there is quite a bit of glue code required for conceptually simple operations, such as passing or receiving a string value.In the future we’ll also provide a higher-level API based on WebAssembly interface types which will significantly reduce the code required for the same operations. Using that API will enable interacting with a WebAssembly module from .NET as easily as you would a .NET assembly.Note also that the API is still under active development and will change in backwards-incompatible ways. We’re aiming to stabilize it as we stabilize Wasmtime itself.If you’re reading this and you aren’t a .NET developer, that’s okay! Check out the Wasmtime Demos repository for corresponding implementations for Python, Node.js, and Rust too! --><p>接下来，我们将深入研究，如何使用Wasmtime API在.NET中加载和使用编译为wasm模块的Rust库，因此对C#编程语言稍微熟悉会有所帮助。</p><p>这里描述的API相当底层，它意味着，在概念上简单的操作（例如传递或接受字符串）需要大量的粘合代码。</p><p>将来，我们还将基于<a href="https://hacks.mozilla.org/2019/08/webassembly-interface-types/" target="_blank" rel="noopener">wasm接口类型</a>提供更高级别的API，这将大大减少相同操作所需的代码。使用该API将使你可以像正常.NET程序一样轻松地在.NET中与wasm模块之间进行交互。</p><p>还请注意的是，该API仍在开发中，因为我们的目标是保持Wasmtime本身的稳定性，并且可能以向后不兼容的方式发生改变。</p><p>如果你不是.NET开发者，那也没问题，请查看<a href="https://github.com/bytecodealliance/wasmtime-demos" target="_blank" rel="noopener">Wasmtime的demo代码库</a>，以获取相应的Python，Node.js和Rust等版本的实现。</p><h2 id="创建wasm模块"><a href="#创建wasm模块" class="headerlink" title="创建wasm模块"></a>创建wasm模块</h2><!-- We’ll start by building a Rust library that can be used to render Markdown to HTML. However, instead of compiling the Rust library for your processor architecture, we’ll be compiling it to WebAssembly so we can use it from .NET.You don’t need to be familiar with the Rust programming language to follow along, but it will help to have a Rust toolchain installed if you want to build the WebAssembly module. See the homepage for Rustup for an easy way to install a Rust toolchain.Additionally, we’re going to use cargo-wasi, a command that bootstraps everything we need for Rust to target WebAssembly: --><p>我们将从构建Rust库（<a href="https://github.com/raphlinus/pulldown-cmark" target="_blank" rel="noopener"><code>pulldown_cmark</code></a>）开始，该库可用于将<a href="https://commonmark.org/" target="_blank" rel="noopener">markdown</a>文档渲染为HTML。前面已经提到，我们不会将Rust库编译为特定目标体系结构，而是将其编译为wasm格式，使得它们可以在.NET使用。</p><p>你并不需要对<a href="https://www.rust-lang.org/" target="_blank" rel="noopener">Rust编程语言</a>熟悉，但是如果是构建wasm模块，那么安装相应的Rust工具链是有用的。有关安装Rust工具链的简便方法，请参考<a href="https://rustup.rs/" target="_blank" rel="noopener">Rustup</a>主页。</p><p>此外，我们将使用<a href="https://github.com/bytecodealliance/cargo-wasi" target="_blank" rel="noopener">cargo-wasi</a>，该命令可创建将Rust编译wasm所需的基础代码和编译环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cargo install cargo-wasi</span><br></pre></td></tr></table></figure><p>然后，克隆Wasmtime的demo代码库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/bytecodealliance/wasmtime-demos.git</span><br><span class="line"><span class="built_in">cd</span> wasmtime-demos</span><br></pre></td></tr></table></figure><!-- This repository includes the markdown directory that contains a Rust library. The library wraps a well-known Rust crate that can render Markdown as HTML. (Note for .NET developers: a crate is like a NuGet package, in a way).Let’s build the markdown WebAssembly module using cargo-wasi: --><p>该代码库包括markdown文件目录和相应的Rust代码，其中Rust代码只是封装了<code>pulldown_cmark</code></p><p>而后使用<code>cargo-wasi</code>构建markdown的wasm模块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> markdown</span><br><span class="line">cargo wasi build --release</span><br></pre></td></tr></table></figure><p>此时，<code>target/wasm32-wasi/release</code>目录中应有编译后的<code>markdown.wasm</code>文件。</p><p>如果你对所实现的rust代码感兴趣，请参看<code>src/lib.rs</code>文件，它包含如下内容：</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> pulldown_cmark::&#123;html, Parser&#125;;</span><br><span class="line"><span class="keyword">use</span> wasm_bindgen::prelude::*;</span><br><span class="line"></span><br><span class="line"><span class="meta">#[wasm_bindgen]</span></span><br><span class="line"><span class="keyword">pub</span> <span class="function"><span class="keyword">fn</span> <span class="title">render</span></span>(input: &amp;<span class="built_in">str</span>) -&gt; <span class="built_in">String</span> &#123;</span><br><span class="line">    <span class="keyword">let</span> parser = Parser::new(input);</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut</span> html_output = <span class="built_in">String</span>::new();</span><br><span class="line">    html::push_html(&amp;<span class="keyword">mut</span> html_output, parser);</span><br><span class="line">    <span class="keyword">return</span> html_output;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该rust代码的功能是export函数<code>render</code>，该函数的功能是将markdown格式字符串作为输入，处理并返回渲染后的HTML格式字符串。</p><p>让我们稍微暂停一下，简单地了解这里所做的事情：我们使用了一个现有的Rust crate，并用几行代码将其封装，其功能作为wasm函数进行了export，然后将其编译为可在.NET加载的wasm模块，而这里我们不用再考虑该模块将在什么平台（或体系结构）上运行，很酷啊兄弟，不是么？！</p><h3 id="检视wasm模块内部"><a href="#检视wasm模块内部" class="headerlink" title="检视wasm模块内部"></a>检视wasm模块内部</h3><p>现在我们已经有了可使用的wasm模块，那么host需要为它需提供怎样的环境，它又为host提供了怎样的功能？</p><p>为了弄清楚这一点，让我们使用<a href="https://github.com/WebAssembly/wabt" target="_blank" rel="noopener">WebAssembly Binary Toolkit</a>里的<code>wasm2wat</code>工具，将模块反汇编成可读文本的表示形式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wasm2wat markdown.wasm --<span class="built_in">enable</span>-multi-value &gt; markdown.wat</span><br></pre></td></tr></table></figure><p><em>注意：<code>--enable-multi-value</code>选项提供对多个返回值函数的支持，这对于反编译<code>markdown.wasm</code>模块是必须的。</em></p><!--### What the module needs from a host--><h3 id="模块需要host所提供的环境支持"><a href="#模块需要host所提供的环境支持" class="headerlink" title="模块需要host所提供的环境支持"></a>模块需要host所提供的环境支持</h3><p>模块的<code>import</code>方式定义了host应为模块提供哪些功能，下面是<code>markdown</code>模块的<code>import</code>段：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(import &quot;wasi_unstable&quot; &quot;fd_write&quot; (func $fd_write (param i32 i32 i32 i32) (result i32)))</span><br><span class="line">(import &quot;wasi_unstable&quot; &quot;random_get&quot; (func $random_get (param i32 i32) (result i32)))</span><br></pre></td></tr></table></figure><p>该段申明告诉我们该模块需要host提供两个函数的接口：<code>fd_write</code>和<code>random_get</code>。这两个函数实际上是具有明确行为定义的<a href="https://github.com/WebAssembly/WASI" target="_blank" rel="noopener">WebAssembly System Interface</a>（简称WASI）函数：<code>fd_write</code>用于将数据写入特定的文件描述符中，<code>random_get</code>将用随机数据填充某个缓冲区。</p><p>很快我们将为.NET的host环境实现这些函数，但更重要的是要明白<strong>模块只能从host调用这些函数</strong>，host可以决定如何实现这些函数甚至是是否实现这些函数。</p><h3 id="模块为主机提供了怎样的功能"><a href="#模块为主机提供了怎样的功能" class="headerlink" title="模块为主机提供了怎样的功能"></a>模块为主机提供了怎样的功能</h3><p>模块的export段定义了它为host提供的功能函数，以下<code>markdown</code>模块的export段：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(export &quot;memory&quot; (memory 0))</span><br><span class="line">(export &quot;render&quot; (func $render_multivalue_shim))</span><br><span class="line">(export &quot;__wbindgen_malloc&quot; (func $__wbindgen_malloc))</span><br><span class="line">(export &quot;__wbindgen_realloc&quot; (func $__wbindgen_realloc))</span><br><span class="line">(export &quot;__wbindgen_free&quot; (func $__wbindgen_free))</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">(func $render_multivalue_shim (param i32 i32) (result i32 i32) ...)</span><br><span class="line">(func $__wbindgen_malloc (param i32) (result i32) ...)</span><br><span class="line">(func $__wbindgen_realloc (param i32 i32 i32) (result i32) ...)</span><br><span class="line">(func $__wbindgen_free (param i32 i32) ...)</span><br></pre></td></tr></table></figure><p>首先，模块export了它自身的<code>memory</code>内存段，wasm内存是模块可访问的线性地址空间，<strong>并且是模块可以读写的唯一内存区域</strong>。由于该模块无法直接访问host地址空间的任何其他区域内存，因此这段export的内存就是host与wasm模块交换数据的区域。</p><p>其次，模块export了我们用Rust实现的<code>render</code>函数，但是这里有个问题是，为什么在前面Rust实现的函数只有一个参数和一个返回值，而wasm对应的函数有两个参数和两个返回值？</p><p>在Rust中，当编译为wasm时，字符串切片类型（<code>&amp;str</code>）和字符串（<code>String</code>）均表示为初地址和长度（以字节为单位）对的形式。因此，wasm版本的函数由于更底层，便直接采用了这种底层的初地址和长度对形式来表示参数和返回值。值得注意的是，这里的初地址表示的是export内存中的字节偏移量。</p><p>那么我们回头看之前的代码，由于Rust代码返回一个<code>String</code>，它是一个<em>owned</em>自有类型，因此<code>render</code>的调用者负责释放包含渲染字符串的返回内存值。</p><p>在.NET的host实现过程中，我们将逐一讨论其余的export项。</p><h2 id="创建-NET工程"><a href="#创建-NET工程" class="headerlink" title="创建.NET工程"></a>创建.NET工程</h2><p>我们使用<a href="https://dotnet.microsoft.com/download" target="_blank" rel="noopener">.NET Core SDK</a>来创建.NET Core工程，所以请确保系统已安装了<strong>3.0或更高版本</strong>的.NET Core SDK。</p><p>为工程创建一个新的目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir WasmtimeDemo</span><br><span class="line">cd WasmtimeDemo</span><br></pre></td></tr></table></figure><p>接下来，在目录中创建.NET Core命令行工程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dotnet new console</span><br></pre></td></tr></table></figure><p>最后，添加<a href="https://www.nuget.org/packages/Wasmtime" target="_blank" rel="noopener">Wasmtime NuGet包</a>的依赖关系：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dotnet add package wasmtime --version 0.8.0-preview2</span><br></pre></td></tr></table></figure><p>现在，我们已做好使用Wasmtime的.NET API来加载并执行markdown模块的准备。</p><h2 id="为wasm导入-NET代码"><a href="#为wasm导入-NET代码" class="headerlink" title="为wasm导入.NET代码"></a>为wasm导入.NET代码</h2><p>为wasm导入.NET实现的函数，跟.NET中实现<a href="https://peterhuene.github.io/wasmtime.net/api/Wasmtime.IHost.html" target="_blank" rel="noopener">IHost</a>接口一样简单，只需一个公有的[Instance]属性来表示和host绑定的wasm模块。</p><p><a href="https://peterhuene.github.io/wasmtime.net/api/Wasmtime.ImportAttribute.html" target="_blank" rel="noopener">Import</a>属性被用于标记函数和域，正如wasm模块中的import那样。</p><p>我们之前提到，模块需要从host环境中import两个函数：<code>fd_write</code>和<code>random_get</code>，所以接下来对这两个函数进行实现：</p><p>在工程目录中创建一个名为<code>Host.cs</code>的文件，并添加如下的代码：</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> System.Security.Cryptography;</span><br><span class="line"><span class="keyword">using</span> Wasmtime;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> <span class="title">WasmtimeDemo</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">class</span> <span class="title">Host</span> : <span class="title">IHost</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// These are from the current WASI proposal.</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> WASI_ERRNO_NOTSUP = <span class="number">58</span>;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> WASI_ERRNO_SUCCESS = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> Instance Instance &#123; <span class="keyword">get</span>; <span class="keyword">set</span>; &#125;</span><br><span class="line"></span><br><span class="line">        [<span class="meta">Import(<span class="meta-string">"fd_write"</span>, Module = <span class="meta-string">"wasi_unstable"</span>)</span>]</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">WriteFile</span>(<span class="params"><span class="keyword">int</span> fd, <span class="keyword">int</span> iovs, <span class="keyword">int</span> iovs_len, <span class="keyword">int</span> nwritten</span>)</span></span><br><span class="line"><span class="function">        </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> WASI_ERRNO_NOTSUP;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        [<span class="meta">Import(<span class="meta-string">"random_get"</span>, Module = <span class="meta-string">"wasi_unstable"</span>)</span>]</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">GetRandomBytes</span>(<span class="params"><span class="keyword">int</span> buf, <span class="keyword">int</span> buf_len</span>)</span></span><br><span class="line"><span class="function">        </span>&#123;</span><br><span class="line">            _random.GetBytes(Instance.Externs.Memories[<span class="number">0</span>].Span.Slice(buf, buf_len));</span><br><span class="line">            <span class="keyword">return</span> WASI_ERRNO_SUCCESS;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> RNGCryptoServiceProvider _random = <span class="keyword">new</span> RNGCryptoServiceProvider();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>fd_write</code>实现仅仅只是简单地返回一个错误，表示不支持该操作。它可被模块用于将错误代码写入<code>stderr</code>中，而在我们的demo中则永远不会真正调用。</p><p><code>random_get</code>的实现使用的是随机字节填充请求缓冲区的方式。它将代表整个模块export内存的<a href="https://peterhuene.github.io/wasmtime.net/api/Wasmtime.Memory.html#Wasmtime_Memory_Span" target="_blank" rel="noopener"><code>Span</code></a>切片，以便.NET的实现可以<em>直接</em>写入请求的缓冲区，而无需进行任何的中间复制操作。Rust标准库中<code>HashMap</code>的实现正是通过调用<code>random_get</code>函数来实现。</p><p>以上就是使用Wasmtime的API将.NET函数import到wasm模块的全部步骤。不过，在加载wasm模块并在.NET使用它们之前，我们需要讨论如何将字符串作为参数，将其从.NET的host传递到<code>render</code>函数中。</p><!--# Being a good host--><h2 id="良好的宿主环境"><a href="#良好的宿主环境" class="headerlink" title="良好的宿主环境"></a>良好的宿主环境</h2><!--Based on the exports of the module, we know it exports a memory. From the host’s perspective, think of a WebAssembly module’s exported memory as being granted access to the address space of a foreign process, even though the module shares the same process of the host itself.--><p>基于模块化的export，我们知道它export了一块<em>memory</em>区域。从host的角度上来看，即使该模块与host本身共享相同的进程内存，也可以将wasm模块的export内存授权为对外部进程地址空间的权限。</p><!--If you randomly write data to a foreign address space, Bad Things Happen™ because it’s quite easy to corrupt the state of the other program and cause undefined behavior, such as a crash or the total protonic reversal of the universe. So how can a host pass data to the WebAssembly module in a safe manner?--><!--Internally the Rust program uses a memory allocator to manage its memory. So, for .NET to be a good host to the WebAssembly module, it must also use the same memory allocator when allocating and freeing memory accessible to the WebAssembly module.--><!--Thankfully, wasm-bindgen, used by the Rust program to export itself as WebAssembly, also exported two functions for that purpose: __wbindgen_malloc and __wbindgen_free. These two functions are essentially malloc and free from C, except __wbindgen_free needs the size of the previous allocation in addition to the memory address.--><!--With this in mind, let us write a simple wrapper for these exported functions in C# so we can easily allocate and free memory accessible to the WebAssembly module.--><!--Create a file named Allocator.cs in the project directory and add the following content:--><p>如果你将数据随机写入外部地址空间，则会发生意想不到的后果，因为它很容易对其他程序的状态造成破坏并引起未定义的行为，例如程序崩溃或字节反转。那么主机应如何以安全的方式将数据传递到wasm模块中呢？</p><p>Rust程序在内部使用内存分配器来管理其内存，因此，为了使.NET成为wasm模块良好的宿主，在分配和释放wasm模块可访问的内存时，必须使用相同的内存分配器。</p><p>值得庆幸的是，Rust程序用来将自身导出为wasm模块的<a href="https://rustwasm.github.io/docs/wasm-bindgen" target="_blank" rel="noopener">wasm-bindgen</a>工具也为此export了两个函数：<code>__wbindgen_malloc</code>和<code>__wbindgen_free</code>。除了<code>__wbindgen_free</code>需要知道内存地址和之前分配的内存大小之外，这两个函数本质上和C语言的<code>malloc</code>和<code>free</code>函数一样。</p><p>考虑到这一点，让我们为C#编写这些export函数的一个简单的封装，以便我们可以轻松分配和释放wasm模块可访问的内存大小。因此，在工程目录中创建一个名为<code>Allocator.cs</code>的文件，并添加如下代码：</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> System;</span><br><span class="line"><span class="keyword">using</span> System.Collections.Generic;</span><br><span class="line"><span class="keyword">using</span> System.Linq;</span><br><span class="line"><span class="keyword">using</span> System.Text;</span><br><span class="line"><span class="keyword">using</span> Wasmtime.Externs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> <span class="title">WasmtimeDemo</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">class</span> <span class="title">Allocator</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">Allocator</span>(<span class="params">ExternMemory memory, IReadOnlyList&lt;ExternFunction&gt; functions</span>)</span></span><br><span class="line"><span class="function">        </span>&#123;</span><br><span class="line">            _memory = memory ??</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> ArgumentNullException(<span class="keyword">nameof</span>(memory));</span><br><span class="line"></span><br><span class="line">            _malloc = functions</span><br><span class="line">                .Where(f =&gt; f.Name == <span class="string">"__wbindgen_malloc"</span>)</span><br><span class="line">                .SingleOrDefault() ??</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> ArgumentException(<span class="string">"Unable to resolve malloc function."</span>);</span><br><span class="line"></span><br><span class="line">            _free = functions</span><br><span class="line">                .Where(f =&gt; f.Name == <span class="string">"__wbindgen_free"</span>)</span><br><span class="line">                .SingleOrDefault() ??</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> ArgumentException(<span class="string">"Unable to resolve free function."</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">Allocate</span>(<span class="params"><span class="keyword">int</span> length</span>)</span></span><br><span class="line"><span class="function">        </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> (<span class="keyword">int</span>)_malloc.Invoke(length);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> (<span class="keyword">int</span> Address, <span class="keyword">int</span> Length) AllocateString(<span class="keyword">string</span> str)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">var</span> length = Encoding.UTF8.GetByteCount(str);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> addr = Allocate(length);</span><br><span class="line"></span><br><span class="line">            _memory.WriteString(addr, str);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> (addr, length);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Free</span>(<span class="params"><span class="keyword">int</span> address, <span class="keyword">int</span> length</span>)</span></span><br><span class="line"><span class="function">        </span>&#123;</span><br><span class="line">            _free.Invoke(address, length);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> ExternMemory _memory;</span><br><span class="line">        <span class="keyword">private</span> ExternFunction _malloc;</span><br><span class="line">        <span class="keyword">private</span> ExternFunction _free;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码虽然看起来很复杂，但它所做的就是从模块中按名称查找所需的export函数，并将它们封装在易于使用的接口中。我们将使用该辅助<code>Allocator</code>类将输入字符串分配给export的<code>render</code>函数。</p><p>现在，我们准备开始渲染markdown。</p><h2 id="渲染markdown"><a href="#渲染markdown" class="headerlink" title="渲染markdown"></a>渲染markdown</h2><p>在工程目录中打开<code>Program.cs</code>，并将其替换为以下内容：</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> System;</span><br><span class="line"><span class="keyword">using</span> System.Linq;</span><br><span class="line"><span class="keyword">using</span> Wasmtime;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> <span class="title">WasmtimeDemo</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">class</span> <span class="title">Program</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">string</span> MarkdownSource = </span><br><span class="line">            <span class="string">"# Hello, `.NET`! Welcome to **WebAssembly** with [Wasmtime](https://wasmtime.dev)!"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">Main</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="function">        </span>&#123;</span><br><span class="line">            <span class="keyword">using</span> <span class="keyword">var</span> engine = <span class="keyword">new</span> Engine();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">using</span> <span class="keyword">var</span> store = engine.CreateStore();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">using</span> <span class="keyword">var</span> module = store.CreateModule(<span class="string">"markdown.wasm"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">using</span> <span class="keyword">var</span> instance = module.Instantiate(<span class="keyword">new</span> Host());</span><br><span class="line"></span><br><span class="line">            <span class="keyword">var</span> memory = instance.Externs.Memories.SingleOrDefault() ??</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> InvalidOperationException(<span class="string">"Module must export a memory."</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">var</span> allocator = <span class="keyword">new</span> Allocator(memory, instance.Externs.Functions);</span><br><span class="line"></span><br><span class="line">            (<span class="keyword">var</span> inputAddress, <span class="keyword">var</span> inputLength) = allocator.AllocateString(MarkdownSource);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">try</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">object</span>[] results = (instance <span class="keyword">as</span> <span class="keyword">dynamic</span>).render(inputAddress, inputLength);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">var</span> outputAddress = (<span class="keyword">int</span>)results[<span class="number">0</span>];</span><br><span class="line">                <span class="keyword">var</span> outputLength = (<span class="keyword">int</span>)results[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span></span><br><span class="line">                &#123;</span><br><span class="line">                    Console.WriteLine(memory.ReadString(outputAddress, outputLength));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">finally</span></span><br><span class="line">                &#123;</span><br><span class="line">                    allocator.Free(outputAddress, outputLength);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">finally</span></span><br><span class="line">            &#123;</span><br><span class="line">                allocator.Free(inputAddress, inputLength);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>让我们一步步地看看这段代码做了哪些工作：</p><ol><li><p>创建<code>Engine</code>对象，该<code>Engine</code>类代表了Wasmtime运行时本身。运行时支持从.NET加载和执行wasm模块；</p></li><li><p>然后创建<code>Store</code>对象，这个类是存放所有wasm对象（例如模块及其实例）的地方。<code>Engine</code>中可以有多个<code>Store</code>，但它们的关联对象不能相互影响；</p></li><li><p>接下来，基于<code>markdown.wasm</code>文件创建<code>Module</code>对象。<code>Module</code>代表wasm模块本身的数据，例如它import和export的数据。一个模块可以具有一个或多个<em>实例</em>，实例化是wasm模块的<em>运行时</em>的表示形式。它将模块的wasm指令编译为当前<em>CPU体系结构</em>的指令，分配模块可访问的实际内存，以及绑定从主机import的函数；</p></li><li><p>使用之前实现的<code>Host</code>类来实例化模块，绑定作为import项的.NET函数；</p></li><li><p>查找由模块export的<code>memory</code>段；</p></li><li><p>创建一个分配器，然后为需要渲染的markdown内容分配一个字符串；</p></li><li><p>以输入字符串为参数，通过将实例转换为<code>dynamic</code>的方式调用<code>render</code>函数。这本是C#的一项特性，在运行时动态绑定函数，可以将其简单地视为搜索并调用export后的<code>render</code>函数的快捷方式；</p></li><li><p>通过从wasm模块export的内存中读取返回的字符串，输出渲染后的HTML；</p></li><li>最后，释放分配的输入字符串和Rust提供给我们的返回字符串的内存。</li></ol><p>以上就是代码所实现的步骤，然后继续运行该代码。</p><h2 id="运行代码"><a href="#运行代码" class="headerlink" title="运行代码"></a>运行代码</h2><!--Before we can run the program, we need to copy markdown.wasm to the project directory, as this is where we’ll run the program from. You can find the markdown.wasm file in the target/wasm32-wasi/release directory from where you built it.From the Program.cs source above, we see that the program hard-coded some Markdown to render:--><p>在运行程序之前，需要将<code>markdown.wasm</code>复制到工程目录中，因为它是我们实际运行程序的地方。可以在构建目录的<code>target/wasm32-wasi/release</code>位置中找到该<code>markdown.wasm</code>文件。</p><p>从上面的<code>Program.cs</code>源码中，我们看到该程序对一些markdown进行了硬编码的渲染：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># Hello, `.NET`! Welcome to **WebAssembly** with [Wasmtime](https://wasmtime.dev)!</span></span><br></pre></td></tr></table></figure><p>运行程序，将其渲染为HTM格式L：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dotnet run</span><br></pre></td></tr></table></figure><p>如果一切正常，应该会出现下面的结果：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>Hello, <span class="tag">&lt;<span class="name">code</span>&gt;</span>.NET<span class="tag">&lt;/<span class="name">code</span>&gt;</span>! Welcome to <span class="tag">&lt;<span class="name">strong</span>&gt;</span>WebAssembly<span class="tag">&lt;/<span class="name">strong</span>&gt;</span> with <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"https://wasmtime.dev"</span>&gt;</span>Wasmtime<span class="tag">&lt;/<span class="name">a</span>&gt;</span>!<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br></pre></td></tr></table></figure><!-- # What’s next for Wasmtime for .NET?That was a surprisingly large amount of C# code that was necessary to implement this demo, wasn’t it?There are two major features we have planned that will help simplify this: --><h2 id="Wasmtime-for-NET的下一步计划是什么？"><a href="#Wasmtime-for-NET的下一步计划是什么？" class="headerlink" title="Wasmtime for .NET的下一步计划是什么？"></a>Wasmtime for .NET的下一步计划是什么？</h2><p>从这里例子中，我们可以看到，现在实现该demo还需大量的C#代码，不是吗？</p><p>我们计划了从两个主要的功能点来简化代码的实现：</p><!-- Exposing Wasmtime’s WASI implementation to .NET (and other languages)In our implementation of Host above, we had to manually implement fd_write and random_get, which are WASI functions.Wasmtime itself has a WASI implementation, but currently it isn’t accessible to the .NET API.Once the .NET API can access and configure the WASI implementation of Wasmtime, there will no longer be a need for .NET hosts to provide their own implementation of WASI functions.Implementing interface types for .NETAs discussed earlier, WebAssembly interface types enable a more idiomatic integration of WebAssembly with a hosting programming language.Once the .NET API implements the interface types proposal, there shouldn’t be a need to create an Allocator class like the one we implemented.Instead, functions that use types like string should simply work without having to write any glue code in .NET. --><ul><li><p><strong>将Wasmtime的WASI实现开放给.NET和其他语言</strong></p><p>在上面<code>Host</code>的实现中，必须手动去编写<code>fd_write</code>和<code>random_get</code>，但它们实际上是WASI中已有的函数。</p><p>Wasmtime本身包含了WASI的实现，只是目前无法通过.NET的API进行访问。</p><p>一旦.NET的API可以访问和配置Wasmtime的WASI版本实现，则.NET的host环境将无需提供自己的实现。</p></li></ul><ul><li><p><strong>实现.NET的接口类型</strong></p><p>前面提到，wasm接口类型可以使wasm更加自然地与托管编程语言进行集成。</p><p>一旦.NET的API实现了未来通过后的接口类型提案，便无需像前面那样去还要创建一个辅助功能的<code>Allocator</code>类。</p><p>到那时，使用诸如字符串等类型的函数可很容易办到，而不必在.NET中编写任何粘合代码。</p></li></ul><!-- The hope, then, is that this is what it might look like in the future to implement this demo from .NET: --><p>所以希望将来该demo是这样的：</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> System;</span><br><span class="line"><span class="keyword">using</span> Wasmtime;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> <span class="title">WasmtimeDemo</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">interface</span> <span class="title">Markdown</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function"><span class="keyword">string</span> <span class="title">Render</span>(<span class="params"><span class="keyword">string</span> input</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">class</span> <span class="title">Program</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">string</span> MarkdownSource =</span><br><span class="line">            <span class="string">"# Hello, `.NET`! Welcome to **WebAssembly** with [Wasmtime](https://wasmtime.dev)!"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">Main</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="function">        </span>&#123;</span><br><span class="line">            <span class="keyword">using</span> <span class="keyword">var</span> markdown = Module.Load&lt;Markdown&gt;(<span class="string">"markdown.wasm"</span>);</span><br><span class="line"></span><br><span class="line">            Console.WriteLine(markdown.Render(MarkdownSource));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- I think we can all agree that looks so much better! --><p>我们都认为这样看起来简洁多了！</p><!-- That’s a wrap!This is the exciting beginning of using WebAssembly outside of the web browser from many different programming environments, including Microsoft’s .NET platform.If you’re a .NET developer, we hope you’ll join us on this journey!The .NET demo code from this article can be found in the Wasmtime Demos repository. --><h2 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h2><p>这是在Web浏览器之外利用不同的编程环境（包括微软的.NET平台）使用wasm的兴奋之旅的开始，如果你是.NET开发者，希望您能加入我们的旅程！</p><p><em>本文的.NET示例代码可以在<a href="https://github.com/bytecodealliance/wasmtime-demos/tree/master/dotnet" target="_blank" rel="noopener">Wasmtime示例代码库</a>中找到。</em></p><p><em>(译者注：本文原地址为 <a href="https://hacks.mozilla.org/2019/12/using-webassembly-from-dotnet-with-wasmtime/" target="_blank" rel="noopener">https://hacks.mozilla.org/2019/12/using-webassembly-from-dotnet-with-wasmtime/</a> ，原作者为 <a href="https://hacks.mozilla.org/author/phuenemozilla-com/" target="_blank" rel="noopener">Peter Huene</a>)</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;!-- Wasmtime, the WebAssembly runtime from the Bytecode Alliance, recent
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="WebAssembly" scheme="https://blog.zhangpf.com/tags/WebAssembly/"/>
    
      <category term="Rust" scheme="https://blog.zhangpf.com/tags/Rust/"/>
    
      <category term="Wasmtime" scheme="https://blog.zhangpf.com/tags/Wasmtime/"/>
    
      <category term=".NET" scheme="https://blog.zhangpf.com/tags/NET/"/>
    
  </entry>
  
  <entry>
    <title>Midori博客系列翻译（6）——性能文化</title>
    <link href="https://blog.zhangpf.com/2019/03/13/midori/6-performance-culture/"/>
    <id>https://blog.zhangpf.com/2019/03/13/midori/6-performance-culture/</id>
    <published>2019-03-13T04:30:00.000Z</published>
    <updated>2020-01-29T02:19:38.942Z</updated>
    
    <content type="html"><![CDATA[<!-- In this essay, I'll talk about "performance culture."  Performance is one of the key pillars of software engineering,and is something that's hard to do right, and sometimes even difficult to recognize.  As a famous judge once said, "Iknow it when I see it."  I've spoken at length about [performance](/2010/09/06/the-premature-optimization-is-evil-myth/)and [culture](/2013/02/17/software-leadership-series/) independently before, however the intersection of the two iswhere things get interesting.  Teams who do this well have performance ingrained into nearly all aspects of how theteam operates from the start, and are able to proactively deliver loveable customer experiences that crush thecompetition.  There's no easy cookie-cutter recipe for achieving a good performance culture, however there are certainlysome best practices you can follow to plant the requisite seeds into your team.  So, let's go! --><p>在本文中，我将聊聊“性能文化”。性能是软件工程的关键支柱之一，并且很难做正确，有时甚至都难以识别，正如一位著名的法官曾经说过的那样，“我一眼就能看出来（I know it when I see it）”。我之前已详细地聊到了<a href="(http://joeduffyblog.com/2010/09/06/the-premature-optimization-is-evil-myth/">性能</a>和<a href="http://joeduffyblog.com/2013/02/17/software-leadership-series/" target="_blank" rel="noopener">文化</a>，但两者之间的交互才会变得更有趣。能够做好这一点的团队几乎从一开始就将性能贯穿于团队运作的各个方面，并且能够主动提供可以摧毁竞争对手的良好客户体验。 虽然没有千篇一律的简单方法来获得良好的性能文化，但是你依然可以遵循一些最佳实践来将必要条件的种子植入到团队中。因此，让我们一起出发吧！</p><!-- # Introduction --><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><!-- Why the big focus on performance, anyway? --><p>那么到底为什么要关注于性能呢？</p><!-- Partly it's my background.  I've worked on systems, runtimes, compilers, ... things that customers expect to be fast.It's always much easier to incorporate goals, metrics, and team processes at the outset of such a project, compared toattempting to recover it later on.  I've also worked on many teams, some that have done amazing at this, some that havedone terribly, and many in between.  The one universal truth is that the differentiating factor is always culture. --><p>其中部分原因是由于我的背景。我曾经研究过系统、运行时、编译器……这些都是客户期望运行得足够快的软件。 在这样的项目中，与在事后进行弥补相比，从一开始时就整合目标、指标和团队的流程总是容易得多。我也曾参与过很多团队的工作，有些团队在这方面做得很棒，有些团队却做得非常糟糕，而很多团队则介于两者之间。一个普遍的事实是，造成这之间差异的因素始终是文化。</p><!-- Partly it's because, no matter the kind of software, performance is almost always worse than our customers would likeit to be.  This is a simple matter of physics: it's impossible to speed up all aspects of a program, given finite time,and the tradeoffs involved between size, speed, and functionality.  But I firmly believe that on the average teams spendway less attention to developing a rigorous performance culture.  I've heard the "performance isn't a top priority forus" statement many times only to later be canceled out by a painful realization that without it the product won'tsucceed. --><p>另外部分原因是，无论哪种软件，性能几乎总是比我们的客户所期望的要更差。这是一个简单的物理问题：在给定有限时间以及在大小速度和功能之间进行折衷的情况下，不可能在所有方面提高程序的运行速度。但我坚信，就平均而言，团队普遍花费较少的注意力来发展严谨的性能文化。我已经很多次听说过这样的言论——“性能不是我们的首要任务”，而后来却又痛苦地意识到它的重要性。没有性能，产品就不会成功。</p><!-- And partly it's just been top of mind for all of us in DevDiv, as we focus on .NET core performance, ASP.NETscalability, integrating performance-motivated features into C# and the libraries, making Visual Studio faster, andmore.  It's particularly top of mind for me, as I've been comparing our experiences to my own in [Midori](/2015/11/03/blogging-about-midori/) (which heavily inspired this blog post). --><p>另外部分原因是，对于DevDiv团队而言，由于我们专注于.Net的核心性能、ASP.Net的扩展性以及将性能驱动的功能集成到C#和库中，使Visual Studio变得更快等等，使得性能是我们所有人摆在首位的指标。这对我来说尤其重要，所以我一直在将我们的经历与我在<a href="/2018/10/20/midori/0-blogging-about-midori/">Midori</a>中的经历进行比较（本文也受此很大的启发）。</p><!-- # Diagnosis and The Cure --><h1 id="诊断和治疗"><a href="#诊断和治疗" class="headerlink" title="诊断和治疗"></a>诊断和治疗</h1><!-- How can you tell whether your performance culture is on track?  Well, here are some signs that it's not: --><p>如何判断团队性能文化是否走上正轨？ 好吧，如果存在下面一些迹象，则表明尚未达到目标：</p><!-- * Answering the question, "how is the product doing on my key performance metrics," is difficult.* Performance often regresses and team members either don't know, don't care, or find out too late to act.* Blame is one of the most common responses to performance problems (either people, infrastructure, or both).* Performance tests swing wildly, cannot be trusted, and are generally ignored by most of the team.* Performance is something one, or a few, individuals are meant to keep an eye on, instead of the whole team.* Performance issues in production are common, and require ugly scrambles to address (and/or cannot be reproduced). --><ul><li>回答“产品在我的关键性能指标上发挥如何”这个问题很难；</li><li>性能经常退化，团队成员要么不知道，要么不关心，要么发现太晚而难以行动；</li><li>责备是对性能问题（人员，基础设施或两者皆有）的最常见反应之一；</li><li>性能测试大幅度摇摆不定，无法得到确认，并且通常被大多数团队成员所忽略；</li><li>性能是一个或几个人应该关注的事情，而不是整个团队；</li><li>产品常常出现性能问题，并且需要仓促行动才能解决（和/或不能重现问题）。</li></ul><!-- These may sound like technical problems.  It may come as a surprise, however, that they are primarily human problems. --><p>这些听起来像是技术问题，但出乎意料之外的是，它们主要是人的问题。</p><!-- The solution isn't easy, especially once your culture is in the weeds.  It's always easier to not dig a hole in thefirst place than it is to climb out of one later on.  But the first rule when you're in a hole is to stop digging!  Thecultural transformation must start from the top -- management taking an active role in performance, asking questions,seeking insights, demanding rigor -- while it simultaneously comes from the bottom -- engineers actively seeking tounderstand performance of the code they are writing, ruthlessly taking a zero-tolerance stance on regressions, andbeing ever-self-critical and on the lookout for proactive improvements. --><p>对此的解决方法并不容易，特别是一旦当性能文化处于十分糟糕的时候，从一开始就不挖坑始终比之后从坑里爬出来要更容易。可是当你陷入困境时，首要的原则就是停止挖坑！文化的转型必须同时从最高层开始——管理层需要在性能的提升中发挥积极的作用，提出问题，寻找方法，严谨要求——以及从下层开始，工程师积极寻求对他们正在编写的代码的性能理解，对性能退化采取无情的零容忍态度，并进行自我批判和寻求主动改进的积极性。</p><!-- This essay will describe some ways to ensure this sort of a culture, in addition to some best practices I've found helpto increase its effectiveness once you have one in place.  A lot of it may seem obvious, but believe me, it's prettyrare to see everything in here working in harmony in practice.  But when it is, wow, what a difference it can make. --><p>本文将介绍一些建立这种文化的方法，此外还有一些我发现的有助于提高效率的最佳实践。很多事情看起来很明显，但请相信我，在实践中很难看到所有一切都协调一致的进行。但是，如果一旦这样的文化得以建立，那它将产生巨大的影响力。</p><!-- *A quick note on OSS software.  I wrote this essay from the perspective of commercial software development.  Assuch, you'll see the word "management" a lot.  Many of the same principles work in OSS too.  So, if you like, anytimeyou see "management," mentally transform it into "management or the project's committers."* --><ul><li>对于OSS（开源）软件的简单说明——我是从商业软件开发的角度写下这篇文章，因此你会看到很多“管理层”这个词，不过这里的许多相同原则也同样适用于OSS。因此，如果你愿意，只要你看到“管理层”这个词，请在脑海中转化为“管理层或项目的提交者”。*</li></ul><!-- # It Starts, and Ends, with Culture --><h1 id="以文化开始，以文化结束"><a href="#以文化开始，以文化结束" class="headerlink" title="以文化开始，以文化结束"></a>以文化开始，以文化结束</h1><!-- The key components of a healthy performance culture are: --><p>健康的性能文化的关键组成部分包括：</p><!-- 1. Performance is part of the daily dialogue and "buzz" within the team.  Everybody plays a role.2. Management must care -- truly, not superficially -- about good performance, and understand what it takes.3. Engineers take a scientific, data-driven, inquisitive approach to performance.  (Measure, measure, measure!)4. Robust engineering systems are in place to track goals, past and present performance, and to block regressions. --><ol><li>性能是团队日常交流和闲聊的一部分，每个人都扮演着其中一个角色；</li><li>管理层必须（真正地，而不是表面上地）关心于优良的性能，并了解获得它们需要什么；</li><li>工程师采用科学的，数据驱动的，刨根问底的方法来提升性能（测量，测量，还是测量！）；</li><li>有健壮的工程系统来追踪目标以及它过去和现在的性能，并阻止其发生退化。</li></ol><!-- I'll spend a bit of time talking about each of these roughly in turn. --><p>我会花一点时间大体地谈论它们中的每一个。</p><!-- ## Dialogue, Buzz, and Communication --><h2 id="对话，闲聊和沟通"><a href="#对话，闲聊和沟通" class="headerlink" title="对话，闲聊和沟通"></a>对话，闲聊和沟通</h2><!-- The entire team needs to be on the hook for performance. --><p>整个团队需要保持对性能的追求。</p><!-- In many teams where I've seen this go wrong, a single person is anointed the go-to performance guy or gal.  Now, that'sfine and can help the team scale, can be useful when someone needs to spearhead an investigation, and having a vocaladvocate of performance is great, but it *must not* come at the expense of the rest of the team's involvement. --><p>在我见过的把事情搞砸的团队中，一个成员被指定为性能小伙/姑娘。这是不错的方法，它可以帮助团队进行扩展，当有人带头进行调查时也很有用，并也在团队中存在主张性能的声音。但这种方式<em>不能</em>以团队的其他成员为代价。</p><!-- This can lead to problems similar to those Microsoft use to have with the "test" discipline; engineers learned badhabits by outsourcing the basic quality of their code, assuming that someone else would catch any problems that arise.The same risks are present when there's a central performance czar: engineers on the team won't write performance tests,won't proactively benchmark, won't profile, won't ask questions about the competitive standing of the product, andgenerally won't do all the things you need all of the engineers doing to build a healthy performance culture. --><p>这可能会导致类似于过去Microsoft对待“测试”的原则问题：工程师通过外包代码的基本质量，并假设其他人（外包人员）会发现任何可能的问题，并养成了坏习惯。当存在中心化的性能独裁者时，也会有同样的风险存在：团队中的工程师不会编写性能测试，不会主动进行基准测试，不会对性能进行profile，不会询问有关产品当前所处竞争地位的问题，并且通常不会做任何一个工程师为建立健康性能文化而做的所有事情。</p><!-- Magical things happen when the whole team is obsessed about performance.  The hallways are abuzz with excitement, asnews of challenges and improvements spreads organically.  "Did you see Martin's hashtable rebalancing change thatreduced process footprint by 30%?"  "Jared just checked in a feature that lets you stack allocate arrays.  I wasthinking of hacking the networking stack this weekend to use it -- care to join in?"  Impromptu whiteboard sessions,off-the-cuff ideation, group learning.  It's really awesome to see.  The excitement and desire to win propels theteam forward, naturally, and without relying on some heavyweight management "stick." --><p>当整个团队都对性能着迷时，会发生神奇的事情。随着挑战和改进的消息在团队里有机地传播，走廊充满了兴奋的气氛。 “你有没有看到Martin的哈希表重平衡改变后，将进程占用空间减少了30%？”“Jared刚刚加入了一个允许堆叠分配数组功能，我本周末想在网络堆栈上使用它，你有兴趣加入吗？”即兴的白板交流，袖手旁观的想法，以小组的方式学习等等，看到这些真的是太棒了，对胜利的兴奋和渴望推动着团队前进，便自然且不依赖于一些重量级管理层的“驱使”。</p><!-- I hate blame and I hate defensiveness.  My number one rule is "no jerks," so naturally all critiques must be deliveredin the most constructive and respectful way.  I've found a high occurrence of blame, defensiveness, and [intellectualdishonesty](/2015/11/02/software-leadership-9-on-the-importance-of-intellectual-honesty/) in teams that do poorly onperformance, however.  Like jerks, these are toxic to team culture and must be weeded out aggressively.  It can easilymake or break your ability to develop the right performance culture.  There's nothing wrong with saying we need to dobetter on some key metric, especially if you have some good ideas on how to do so! --><p>我讨厌抱怨和防备，因此我的首要原则就是：“（团队里）没有笨蛋”，所以所有的批评都必须以最具建设性和尊重的方式提出来。然而，我发现在性能状况不佳的团队中往往出现过多的抱怨、防备和<a href="http://joeduffyblog.com/2015/11/02/software-leadership-9-on-the-importance-of-intellectual-honesty/" target="_blank" rel="noopener">机智的不诚实</a>，这就像笨蛋一样，对团队文化是有害的，因此必须积极将这种方式淘汰掉，它可以轻松地决定你发展正确性能文化的能力。我们需要在一些关键指标上做得更好这一点上是没有错的，特别是如果你对如何做到这一点上有一些好的想法！</p><!-- In addition to the ad-hoc communication, there of course needs to be structured communication also.  I'll describe sometechniques later on.  But getting a core group of people in a room regularly to discuss the past, present, and future ofperformance for a particular area of the product is essential.  Although the organic conversations are powerful,everyone gets busy, and it's important to schedule time as a reminder to keep pushing ahead. --><p>除了ad-hoc的交谈方式之外，当然还需要结构化的交流，我稍后会介绍一些技巧。但是，定期在一个房间内安排一组核心人群来讨论产品特定区域的过去，现在和未来的性能至关重要。尽管有机的讨论非常有用，但每个人都很忙碌，所以重要的是要安排时间作为继续推进的提醒。</p><!-- ## Management: More Carrots, Fewer Sticks --><h2 id="管理层：更多的胡萝卜和更少的大棒"><a href="#管理层：更多的胡萝卜和更少的大棒" class="headerlink" title="管理层：更多的胡萝卜和更少的大棒"></a>管理层：更多的胡萝卜和更少的大棒</h2><!-- In every team with a poor performance culture, it's management's fault.  Period.  End of conversation. --><p>在每个具有糟糕性能文化的团队中，都是因为管理层出了问题。</p><!-- Engineers can and must make a difference, of course, but if the person at the top and everybody in between aren't deeplyinvolved, budgeting for the necessary time, and rewarding the work and the superstars, the right culture won't takehold.  A single engineer alone can't possibly infuse enough of this culture into an entire team, and most certainly notif the entire effort is running upstream against the management team. --><p>当然，工程师可以而且必须有所作为，但如果顶层的管理者和中间层都没有深入参与，对必要时间进行预算，对工作和超级明星进行奖励，那么正确的文化就不会成功建立起来。单凭一名工程师不可能将这种文化注入到整个团队，而且如果整个工作都是与管理团队相抵触的话，那么肯定更不会成功。</p><!-- It's painful to watch managers who don't appreciate performance culture.  They'll often get caught by surprise and won'trealize why -- or worse, think that this is just how engineering works.  ("We can't predict where performance willmatter in advance!")  Customers will complain that the product doesn't perform as expected in key areas and, realizingit's too late for preventative measures, a manager whose team has a poor performance culture will start blaming things.Guess what?  The blame game culture spreads like wildfire, the engineers start doing it too, and accountability goes outthe window.  Blame doesn't solve anything.  Blaming is what jerks do. --><p>看到一个不会欣赏性能文化的管理者是痛苦的。他们经常会对此感到意外，并且不会意识到为什么会是这样，或者更糟糕地认为这就是工程的工作原理（“我们无法提前料到性能的重要性！”）。客户会抱怨产品在关键领域没有达到预期的效果，并且在意识到预防措施为时已晚时，具有糟糕性能文化的团队管理者便开始抱怨。你猜怎么着？这种抱怨的文化就像野火一样传播，使得工程师们也开始这样做，而真正的责任追究制却被抛之脑后。抱怨不能解决任何问题，是笨蛋才所做的事情。</p><!-- Notice I said management must be "*deeply* involved": this isn't some superficial level of involvement.  Sure, chartswith greens, reds, and trendlines probably need to be floating around, and regular reviews are important.  I suppose youcould say that these are pointy-haired manager things.  (Believe me, however they do help.)  A manager must go deeperthan this, however, proactively and regularly reviewing the state of performance across the product, alongside the otherbasic quality metrics and progress on features.  It's a core tenet of the way the team does its work.  It must betreated as such.  A manager must wonder about the competitive landscape and ask the team good, insightful questionsthat get them thinking. --><p>注意我说管理层必须“<em>深入</em>参与”的意思时，不能只是表面的参与而已。当然，绿色、红色加上趋势线的图表可能需要出现在周围，并且定期的评审也很重要，我想你可以说这些都是尖头发的经理所从事的事情（相信我，这确实是有帮助的）。然而，经理必须比这些更深入，主动并定期审查整个产品的性能状况，以及其他基本质量指标和功能的进展。这是团队工作方式的核心原则，并且必须这样对待。经理必须思考竞争的格局，并向团队询问引发他们思考，且优秀而富有洞察力的问题。</p><!-- Performance doesn't come for free.  It costs the team by forcing them to slow down at times, to spend energy on thingsother than cranking out features, and hence requires some amount of intelligent tradeoff.  How much really depends onthe scenario.  Managers need to coach the team to spend the right ratio of time.  Those who assume it will come for freeusually end up spending 2-5x the amount it would have taken, just at an inopportune time later on (e.g., during theendgame of shipping the product, in production when trying to scale up from 1,000 customers to 100,000, etc). --><p>性能不是免费的，它有时通过迫使团队放慢速度为代价，将精力花在除了完成功能之外的事情上，因此需要进行一些合理的权衡。在性能上花费多少精力真的取决于场景，而经理需要指导团队花费适当比例的时间。那些认为会免费获得性能的人通常在稍后不凑巧的时间点上（例如，在发布产品的最后阶段，以及在生产中尝试从1000扩大到100000个客户规模时等），最终会花费2-5倍的精力。</p><!-- A mentor of mine used to say "You get from your teams what you reward."  It's especially true with performance and theengineering systems surrounding them.  Consider two managers: --><p>我的一位导师曾经说过，“你从你的团队中获得你的奖励”，对于性能和围绕它们的工程系统来说尤其如此。考虑以下两种经理：</p><!-- * *Manager A* gives lip service to performance culture.  She, however, packs every sprint schedule with a steady stream  of features -- "we've got to crush competitor Z and *must* reach feature parity!" -- with no time for breaks  in-between.  She spends all-hands team meetings praising new features, demos aplenty, and even gives out a reward to  an engineer at each one for "the most groundbreaking feature."  As a result, her team cranks out features at an  impressive clip, delivers fresh demos to the board every single time, and gives the sales team plenty of ammo to  pursue new leads.  There aren't performance gates and engineers generally don't bother to think much about it.* *Manager B* takes a more balanced approach.  She believes that given the competitive landscape, and the need to  impress customers and board members with whizbang demos, new features need to keep coming.  But she is also wary of  building up too much debt in areas like performance, reliability, and quality for areas she expects to stick.  So she  intentionally puts her foot on the brake and pushes the team just as hard on these areas as she does features.  She  demands good engineering systems and live flighting of new features with performance telemetry built-in, for example.  This requires that she hold board members and product managers at bay, which is definitely unpopular and difficult.  In addition to a reward for "the most groundbreaking feature" award at each all-hands, she shows charts of performance  progress and delivers a "performance ninja" award too, to the engineer who delivered the most impactful performance  improvement.  Note that engineering systems improvements also qualify! --><ul><li><p>经理A为性能文化提出口头上的鼓励。然而，她为每个迭代周期提出了固定数量的功能计划，比如，“我们必须粉碎竞争对手Z并且<em>必须</em>实现相同的功能！”，却没有中途休息的时间。她花了很多时间参加团队会议，赞扬新功能和演示大量的demo，甚至为工程师颁发“最具突破性功能”的奖励。结果，她的团队以一个令人印象深刻的剪辑实现出功能，并每次都向董事会演示新出炉的demo，为销售团队提供大量资源以获取新的潜在客户。但没有为性能提供门路，工程师们通常也懒得去思考性能。</p></li><li><p>经理B采取更加平衡的方法。她认为，鉴于竞争的格局和通过杰出的演示给客户和董事会成员留下深刻印象的必要性，新功能是需要不断涌现的。但她也担心在性能，可靠性和质量等方面在她所希望坚持的领域积累太多债务。因此，她会故意放慢追求新功能的步伐，并尽可能地推动团队在这些方面上努力。例如，她需要良好的工程系统以及内置性能遥测的新功能，这会让董事会成员和产品经理处于困境，而这绝对是不受欢迎和有难度的。除了颁发“最具突破性功能”奖项之外，她还要展示了性能的进展图表，并为提供最具影响力性能改进的工程师颁发了“性能忍者”奖。请注意，工程系统的改进也是符合“性能忍者”奖颁发条件的！</p></li></ul><!-- Which manager do you think is going to ship a quality product, on time, that customers are in love with?  My money ison Manager B.  Sometimes you've got to [slow down to speed up](/2013/04/12/software-leadership-4-slow-down-to-speed-up/). --><p>你认为哪位经理会按时交付给客户所喜爱的优质产品呢？我会投给经理B。有时候<a href="http://joeduffyblog.com/2013/04/12/software-leadership-4-slow-down-to-speed-up/" target="_blank" rel="noopener">为了加快速度而必须放慢脚步</a>。</p><!-- Microsoft is undergoing two interesting transitions recently that are related to this point: on one hand, theelimination of "test" as a discipline mentioned earlier; and, on the other hand, a renewed focus on engineering systems.It's been a bumpy ride.  Surprisingly, one of the biggest hurdles to get over wasn't with the individual engineers atall -- it was the managers!  "Development managers" in the old model got used to focusing on features, features,features, and left most of the engineering systems work to contractors, and most of the quality work to testers.  As aresult, they were ill-prepared to recognize and reward the kind of work that is essential to building a greatperformance culture.  The result?  You guessed it: a total lack of performance culture.  But, more subtly, you alsoended up with "leadership holes"; until recently, there were virtually no high-ranking engineers working on themission-critical engineering systems that make the entire team more productive and capable.  Who wants to make a careerout of the mere grunt work assigned to contractors and underappreciated by management?  Again, you get what you reward. --><p>微软最近正在经历两个与这一点相关的有趣转变：一方面，消除了以“测试”作为前面提到的原则；另一方面，又重新关注于工程系统。这是一个坎坷的旅程，但令人惊讶的是，克服困难的最大障碍之一不是工程师个人，而是管理者！旧模型中的“开发经理”习惯专注于功能，功能，还是功能，并将大部分工程系统任务交给承包商，以及将大多数软件质量工作都交给了测试人员。结果，他们没有准备好承认和奖励那些对建立卓越性能文化至关重要的工作。结果如何？你猜对了：完全缺乏性能文化，但更微妙的是，最终还是出现了“领导漏洞”。直到最近，几乎没有高级工程师致力于任务关键的工程系统，并使整个团队更高效和有能力。谁想通过分配给承包商的那些繁琐的工作来创造自己的事业，而不能得到管理层的重视？再一次地，你获得了应得的回报。</p><!-- There's a catch-22 with early prototyping where you don't know if the code is going to survive at all, and so thetemptation is to spend zero time on performance.  If you're hacking away towards a minimum viable product (MVP), andyou're a startup burning cash, it's understandable.  But I strongly advise against this.  Architecture matters, and somepoorly made architectural decisions made at the outset can lay the foundation for an entire skyscraper ofill-performing code atop.  It's better to make performance part of the feasibility study and early exploration. --><p>有一个关于项目早期原型的“22条军规”，它是这么说的：如果你不知道项目是否能够存活下来，那么不要在性能上花费任何时间。如果你正在实现最低可行产品（MVP），并且处于一个非常烧钱的创业公司中，那这种做法是可以理解的。但我强烈反对这一点，因为架构真的很重要，一开始就做出的一些不当的架构决策可能为整个低质量代码的摩天大楼奠定了基础。因此，最好将性能作为可行性研究和早期探索的一部分。</p><!-- Finally, to tie up everything above, as a manager of large teams, I think it's important to get together regularly-- every other sprint or two -- to review performance progress with the management team.  This is in addition to themore fine-grained engineer, lead, and architect level pow-wows that happen continuously.  There's a bit of a "stick"aspect of such a review, but it's more about celebrating the team's self-driven accomplishments, and keeping it onmanagement's radar.  Such reviews should be driven from the lab and manually generated numbers should be outlawed. --><p>最后，作为大型团队的经理，为了统一上述所有内容，我认为定期，比如每隔一两个迭代周期，召开会议来审核管理团队的性能进展是非常重要的。除此之外，还有更细粒度的工程师，leader和架构师级别的碰头要不断的进行。这种审查有一点“坚持”的味道，但它更多的是关于庆祝团队的自我驱动成就，并将其保持在管理层的雷达上。这些审查应该由实验室所驱动，而手动生成的数字是不合规的。</p><!-- Which brings me to ... --><p>这为我带来了……</p><!-- # Process and Infrastructure --><h1 id="流程和基础设施"><a href="#流程和基础设施" class="headerlink" title="流程和基础设施"></a>流程和基础设施</h1><!-- "Process and infrastructure" -- how boring! --><p>“流程和基础设施”——多么无聊的事情！</p><!-- Good infrastructure is a must.  A team lacking the above cultural traits won't even stop to invest in infrastructure;they will simply live with what should be an infuriating lack of rigor.  And good process must ensure effective useof this infrastructure.  Here is the bare minimum in my book: --><p>良好的基础设施是必须的。 缺乏上述文化特征的团队甚至不会停下来对基础设施进行投资，所以他们只会处在一个令人恼怒并缺乏严谨的环境中，而良好的流程能确保有效使用此基础设施。下面是我在本书中的最低要求：</p><!-- * All commits must pass a set of gated performance tests beforehand.* Any commits that slip past this and regress performance are reverted without question.  I call this the zero  tolerance rule.* Continuous performance telemetry is reported from the lab, flighting, and live environments.* This implies that performance tests and infrastructure have a few important characteristics:    - They aren't noisy.    - They measure the "right" thing.    - They can be run in a "reasonable" amount of time. --><ul><li>所有的代码提交必须先通过一组门控性能测试；</li><li>任何惊险通过测试并且性能退化的提交都将毫无疑问地被撤回，我称其为零容忍规则；</li><li>连续性能遥测通过实验室，飞行和在线环境中上报；</li><li>这意味着性能测试和基础架构具有一些重要特征：<ul><li>它们不会让人觉得吵闹；</li><li>能测量“正确”的东西；</li><li>可以在“合理”的时间内运行完成。</li></ul></li></ul><!-- I have this saying: "If it's not automated, it's dead to me." --><p>有这么一句话：“如果它不是自动化的，那么对我来说它已经死了”。</p><!-- This highlights the importance of good infrastructure and avoids the dreaded "it worked fine on my computer" thateverybody, I'm sure, has encountered: a test run on some random machine -- under who knows what circumstances -- isquoted to declare success on a benchmark... only to find out some time later that the results didn't hold.  Why is this? --><p>这凸显了良好基础设施的重要性，并避免了可怕的“它明明在我的机器上运行的很好”的情况出现，我可以保证每个人都遇到过：在某个谁也不知道环境配置的随机机器上运行通过测试，然后便宣称在某个基准测试上获得了成功，而在一段时间后发现并没有这种预期的结果。那么为什么会遇到这种情况？</p><!-- There are countless possibilities.  Perhaps a noisy process interfered, like AntiVirus, search indexing, or theapplication of operating system updates.  Maybe the developer accidentally left music playing in the background ontheir multimedia player.  Maybe the BIOS wasn't properly adjusted to disable dynamic clock scaling.  Perhaps it wasdue to an innocent data-entry error when copy-and-pasting the numbers into a spreadhseet.  Or maybe the numbers for twocomparison benchmarks came from two, incomparable machine configurations.  I've seen all of these happen in practice. --><p>这里存在无数种可能性：也许是一个嘈杂进程的干扰，比如AntiVirus或搜索索引进程，或操作系统的更新程序，或者是开发者不小心把音乐留在他们的多媒体播放器的后台进行播放，也可能是BIOS没有正确调整到禁用动态时钟缩放，或由于在将数字复制并粘贴到电子表格时出现了数据输入错误，更或者是两个进行比较的基准数字来自两个根本无法比较的机器配置上。我见证过以上所有这些都在现实世界中发生过。</p><!-- In any manual human activity, mistakes happen.  These days, I literally refuse to look at or trust any number thatdidn't come from the lab.  The solution is to automate everything and focus your energy on making the automationinfrastructure as damned good as possible.  Pay some of your best people to make this rock solid for the rest of theteam.  Encourage everybody on the team to fix broken windows, and take a proactive approach to improving theinfrastructure.  And reward it heartily.  You might have to go a little slower, but it'll be worth it, trust me. --><p>在任何有人参与的活动中，都会发生错误。现在这些天，我真的拒绝查看或信任任何不是来自实验室的数字，而解决方案是实现所有一切的自动化，并将精力集中在尽可能使自动化基础设施变得更好上面，通过部署最优秀的人才在这上面，为团队的其他成员打造坚实的基础。鼓励团队中的每个人修复“破损的窗户”，并采取积极主动的方法来改善基础设施，以及衷心地奖励这些做法。这样做可能会稍微前进的慢一点，但请相信我，这样做是值得的。</p><!-- ## Test Rings --><h2 id="测试环"><a href="#测试环" class="headerlink" title="测试环"></a>测试环</h2><!-- I glossed over a fair bit above when I said "all commits must pass a set of performance tests," and then went on totalk about how a checkin might "slip past" said tests.  How is this possible? --><p>当我说“所有代码提交都必须通过一系列性能测试”时，我是很认真的，然后我又继续说到代码提交可能会“侥幸通过”所述测试。那么为什么会这样？</p><!-- The reality is that it's usually not possible to run all tests and find all problems before a commit goes in, at leastnot within a reasonable amount of time.  A good performance engineering system should balance the productivity of speedycodeflow with the productivity and assurance of regression prevention. --><p>实际的情况是，通常不可能在提交之前运行所有测试并找到所有问题，至少在合理的时间内不会达到。一个好的性能工程系统应该要在快速代码流的生产力与回归预防的保证之间做出平衡。</p><!-- A decent approach for this is to organize tests into so-called "rings": --><p>一种合适的方法是将测试组织成所谓的“环（ring）”：</p><!-- * An inner ring containing tests that all developers on the team measure before each commit.* An inner ring containing tests that developers on your particular sub-team measure before each commit.* An inner ring containing tests that developers run at their discretion before each commit.* Any number of successive rings outside of this:    - Gates for each code-flow point between branches.    - Post-commit testing -- nightly, weekly, etc. -- based on time/resource constraints.    - Pre-release verification.    - Post-release telemetry and monitoring. --><ul><li>一个内环，包含团队中所有开发者在每次提交之前要测量的测试；</li><li>一个内环，包含特定子团队的开发者在每次提交之前要测量的测试；</li><li>一个内环，包含开发者在每次提交之前自行决定要运行的测试；</li><li>除此之外的任意数量的连续环，这包括：<ul><li>分支之间的每个代码流门控测试；</li><li>每晚（nightly）每周（weekly）或基于时间/资源的限制等条件的，提交后（post-commit）的测试；</li><li>发布前（pre-release）的验证；</li><li>发布后（post-release）的遥测和监测。</li></ul></li></ul><!-- As you can see, there's a bit of flexibility in how this gets structured in practice.  I wish I could lie and say thatit's a science, however it's an art that requires intelligently trading off many factors.  This is a constant source ofdebate and one that the management team should be actively involved in. --><p>正如你所看到的，在实践中如何构建它有一定的灵活性。我多么希望我能谎称这是一门科学，但这确实是一门需要智慧地折衷许多因素的艺术。这是一个不断争论的焦点，也是管理团队应该积极参与辩论的话题。</p><!-- A small team might settle on one standard set of benchmarks across the whole team.  A larger team might need to splitinner ring tests along branching lines.  And no matter the size, we would expect the master/main branch to enforce themost important performance metrics for the whole team, ensuring no code ever flows in that damages a core scenario. --><p>一个小型团队可以在整个团队中建立一套标准的基准，而一个更大的团队可能需要沿着分支分割内环测试。无论规模大小，我们都希望主干/主分支能够为整个团队强制执行最重要的性能指标，确保没有任何损害核心场景的代码进入。</p><!-- In some cases, we might leave running certain pre-commit tests to the developer's discretion.  (Note, this does not meanrunning pre-commit tests altogether is optional -- only a particular set of them!)  This might be the case if, forexample, the test covered a lesser-used component and we know the nightly tests would catch any post-commit regression.In general, when you have a strong performance culture, it's okay to trust judgement calls sometimes.  Trust but verify. --><p>在某些情况下，我们可能会将某些提交前（pre-commit）测试留给开发者来自行决定（注意，这并不意味着运行提交前测试完全是可选的，而只是有一组是特定于此的），例如，如果测试覆盖了较少使用的组件并且我们知道每夜测试会捕获任何提交后回归。一般来说，当你拥有强大的性能文化时，有时候可以信任这种判断。信任但要验证。</p><!-- Let's take a few concrete examples.  Performance tests often range from micro to macro in size.  These typically rangefrom easier to harder to pinpoint the source of a regression, respectively.  (Micro measures just one thing, and sofluctuations tend to be easier to understand, whereas macro measures an entire system, where fluctuations tend to take abit of elbow grease to track down.)  A web server team might include a range of micro and macro tests in the innermostpre-commit suite of tests: number of bytes allocated per requests (micro), request response time (micro), ... perhapsa half dozen other micro-to-midpoint benchmarks ..., and [TechEmpower](https://www.techempower.com/benchmarks/) (macro),let's say.  Thanks to lab resources, test parallelism, and the awesomeness of GitHub webhooks, let's say these allcomplete in 15 minutes, nicely integrated into your pull request and code review processes.  Not too bad.  But thisclearly isn't perfect coverage.  Maybe every night, TechEmpower is run for 4 hours, to measure performance over a longerperiod of time to identify leaks.  It's possible a developer could pass the pre-commit tests, and then fail this longertest, of course.  Hence, the team lets developers run this test on-demand, so a good doobie can avoid getting egg on hisor her face.  But alas, mistakes happen, and again there isn't a culture of blame or witchhunting; it is what it is. --><p>让我们来看几个具体的例子。性能测试的范围通常从微观遍布宏观，因此从精确定位回归来源的角度看，他们通常也从更容易遍布到更难上 （微测试只测量一种，所以波动往往更容易理解，而宏测试则需测量整个系统，通过波动找到目标往往要费点力气）。网络服务器团队可能在最里面的预提交测试套件中，包括一系列的微测试和宏测试，比如说：每个请求分配的字节数（微），请求响应时间（微）……可能还有其他六个微到中间规模的基准测试……以及<a href="https://www.techempower.com/benchmarks/" target="_blank" rel="noopener">TechEmpower</a>（宏）。感谢实验室的资源，测试并行以及强大的GitHub webhooks，使得这些测试都在15分钟内完成，并能很好地集成到你的pull request和代码审查流程中，这一点是相当不错的。但这显然不能完美的覆盖，也许每天晚上，TechEmpower需要运行4个小时，以便能在更长的时间内测量性能并识别泄漏，开发者可能通过提交前测试，但却在这个更长的测试中失败。因此，团队允许按需运行此测试，所以好的开发者可以避免丢脸。但即使错误发生了，却再也不是一种抱怨或迫害的文化，是怎样那就是怎样。</p><!-- This leads me to back to the zero tolerance rule. --><p>这导致我回退到了零容忍规则上。</p><!-- Barring exceptional circumstances, regressions should be backed out immediately.  In teams where I've seen this succeed,there were no questions asked, and no IOUs.  As soon as you relax this stance, the culture begins to break down.  Layersof regressions pile on top of one another and you end up ceding ground permanently, no matter the best intentions of theteam.  The commit should be undone, the developer should identify the root cause, remedy it, ideally write a new test ifappropriate, and then go through all the hoops again to submit the checkin, this time ensuring good performance. --><p>除非出现特殊情况，否则出现回归则应立即撤销。在我看到这个成功的团队中，这一点是毫无疑问的，也没有任何借口，一旦你放松这种立场，文化就会开始崩溃。无论团队的最佳意图是什么，如果性能回归都堆积在一起，那么你最终再也回不到最初的状态。对于回归的提交应该撤消，开发者需确定根本原因，对其进行补救。理想情况下，如果合适的话，编写一个新的测试，然后再次通过所有测试环以提交签入代码，并在这次确保良好的性能。</p><!-- ## Measurement, Metrics, and Statistics --><h2 id="测量，指标和统计"><a href="#测量，指标和统计" class="headerlink" title="测量，指标和统计"></a>测量，指标和统计</h2><!-- Decent engineers intuit.  Good engineers measure.  Great engineers do both. --><p>体面的工程师靠直觉，优秀的工程师靠测量，伟大的工程师两种都做到。</p><!-- Measure what, though? --><p>那到底测量什么？</p><!-- I put metrics into two distinct categories: --><p>我将度量分为两个不同的类别：</p><!-- * *Consumption metrics.*  These directly measure the resources consumed by running a test.* *Observational metrics.*  These measure the outcome of running a test, *observationally*, using metrics "outside" of  the system. --><ul><li><em>消耗指标</em>：它们直接测量运行测试所消耗的资源；</li><li><em>观察指标</em>：它们使用系统“外部”的度量标准来衡量运行测试的结果。</li></ul><!-- Examples of consumption metrics are hardware performance counters, such as instructions retired, data cache misses,instruction cache misses, TLB misses, and/or context switches.  Software performance counters are also good candidates,like number of I/Os, memory allocated (and collected), interrupts, and/or number of syscalls.  Examples of observationalmetrics include elapsed time and cost of running the test as billed by your cloud provider.  Both are clearly importantfor different reasons. --><p>一个消耗指标的例子是硬件性能计数器，比如说完成指令（instruction retired），未命中的数据缓存，未命中的指令缓存，未命中的TLB和/或上下文切换等。软件性能计数器也是很好的例子，例如I/O数量，分配（和收集）的内存，中断和/或系统调用的数量等。观察度量的示例包括运行时间和来自云提供商的运行测试成本开销。处于多种原因，两者显然都很重要。</p><!-- Seeing a team measure time and time alone literally brings me to tears.  It's a good measure of what an end-user willsee -- and therefore makes a good high-level test -- however it is seriously lacking in the insights it can give you.And if there's no visibility into variance, it can be borderline useless. --><p>逐字观察一整个团队的测量时间本身就非常耗费精力，虽然它可以很好地衡量最终用户将会看到什么，并因此进行良好的高级测试， 但它可以为你提供的洞察力严重缺乏，如果没有对变化的可见性，那么可能是无用的。</p><!-- Consumption metrics are obviously much more helpful to an engineer trying to understand why something changed.  In ourabove web server team example, imagine request response time regressed by 30%.  All the test report tells us is thetime.  It's true, a developer can then try to reproduce the scenario locally, and manually narrow down the cause,however can be tedious, takes time, and is likely imperfect due to differences in lab versus local hardware.  What if,instead, both instructions retired and memory allocated were reported alongside the regression in time?  From this, itcould be easy to see that suddenly 256KB of memory was being allocated per request that wasn't there before.  Beingaware of recent commits, this could make it easy for an engineer to quickly pinpoint and back out the culprit in atimely manner before additional commits pile on top, further obscuring the problem.  It's like printf debugging. --><p>对于试图理解为什么会发生变化的工程师而言，消耗指标显然更有帮助。在我们上面的Web服务器团队的例子中，假设请求响应时间增加了30%，所有测试报告告诉我们的都是时间。开发者确实可以尝试在本地重现场景，并手动缩小原因范围，但是由于实验室与本地硬件的差异，这样做可能会很繁琐并耗费时间，而且可能并不完美。相反，如果是完成指令和分配内存的报告与时间回归报告一起呈现呢？从它们那里，可以很容易地看出，每个请求突然分配了256KB的内存，这在之前是没有出现的。结合考虑最近的提交，这可以使工程师在增加提交堆积在最顶层并进一步使问题难以理解之前，很容易及时地快速查明并找到罪魁祸首。而这就好像printf调试所提供的一样。</p><!-- Speaking of printf debugging, telemetry is essential for long-running tests.  Even low-tech approaches like printfingthe current set of metrics every so often (e.g., every 15 seconds), can help track down where something went into theweeds simply by inspecting a database or logfile.  Imagine trying to figure out where the 4-hour web server test wentoff the rails at around the 3 1/2 hour mark.  This can can be utterly maddening without continuous telemetry!  Ofcourse, it's also a good idea to go beyond this.  The product should have a built-in way to collect this telemtry outin the wild, and correlate it back to key metrics.  [StatsD](https://github.com/etsy/statsd) is a fantastic option. --><p>说到printf调试，遥测对于长时间运行的测试至关重要。即使是低技术含量的方法，例如每隔一段时间（例如每15秒）打印一组当前的度量值，也可以简单地通过检查数据库或日志文件来追踪到陷入困境的位置。想象一下，试图找出4小时网络服务器测试在3个半小时左右的轨道上脱轨的地方，如果没有连续的遥测，这可能真是令人头疼的一件事！当然，在这上面走的更远也是一个好主意，产品应具有内置的方式在收集此遥测，并将其与关键指标相关联。 这方面，<a href="https://github.com/etsy/statsd" target="_blank" rel="noopener">StatsD</a>是一个很棒的选择。</p><!-- Finally, it's important to measure these metrics as scientifically as possible.  That includes tracking [standarddeviation](https://en.wikipedia.org/wiki/Standard_deviation), [coefficient of variation (CV)](https://en.wikipedia.org/wiki/Coefficient_of_variation), and [geomean](https://en.wikipedia.org/wiki/Geometric_mean),and using these to ensure tests don't vary wildly from one run to the next.  (Hint: commits that tank CV should beblocked, just as those that tank the core metric itself.)  Having a statistics wonk on your team is also a good idea! --><p>最后，尽可能科学地衡量这些指标非常重要。这包括跟踪<a href="https://en.wikipedia.org/wiki/Standard_deviation" target="_blank" rel="noopener">标准差</a>，<a href="https://en.wikipedia.org/wiki/Coefficient_of_variation" target="_blank" rel="noopener">变异系数（CV）</a>和<a href="https://en.wikipedia.org/wiki/Geometric_mean" target="_blank" rel="noopener">几何平均值</a>，并使用这些参数来确保两次测试结果不会相差太远。（提示：大幅度改变CV的提交应该被阻止，就像那些大幅度改变核心指标本身一样）。对你的团队进行统计数据研究也是一个不错主意！</p><!-- ## Goals and Baselines --><h2 id="目标和基线"><a href="#目标和基线" class="headerlink" title="目标和基线"></a>目标和基线</h2><!-- Little of the above matters if you lack goals and baselines.  For each benchmark/metric pair, I recommend recognizingfour distinct concepts in your infrastructure and processes: --><p>如果你缺乏目标和基线，那么上述的方法所产生的作用很小。对于每个基准/指标对，我建议你在基础架构和流程中识别四个不同的概念：</p><!-- * *Current*: the current performance (which can span multiple metrics).* *Baseline*: the threshold the product must stay above/below, otherwise tests fail.* *Sprint Goal*: where the team must get to before the current sprint ends.* *Ship Goal*: where the team must get to in order to ship a competitive feature/scenario. --><ul><li><em>当前</em>：当前性能（可以跨越多个指标）；</li><li><em>基线</em>：产品必须保持高于/低于的阈值，否则测试失败；</li><li><em>迭代目标</em>：在当前迭代周期结束之前团队必须到达的位置；</li><li><em>交付目标</em>：团队必须到达此位置，才能发布有竞争力的功能/方案。</li></ul><!-- Assume a metric where higher is better (like throughput); then it's usually the case that Ship Goal &gt;= Sprint Goal&gt;= Current &gt;= Baseline.  As wins and losses happen, continual adjustments should be made. --><p>对于一个越高越好的指标（如吞吐量）来讲，那么通常情况下的情况是，交付目标 &gt;= 迭代目标 &gt;= 当前 &gt;= 基线。随着输赢比较的不断发生，应该不断进行调整。</p><!-- For example, a "baseline ratcheting" process is necessary to lock in improvements.  A reasonable approach is to ratchetthe baseline automatically to within some percentage of the current performance, ideally based on standard deviationand/or CV.  Another approach is to require that developers do it manually, so that all ratcheting is intentional andaccounted for.  And interestingly, you may find it helpful to ratchet in the other direction too.  That is, blockcommits that *improve* performance dramatically and yet do not ratchet the baseline.  This forces engineers to stop andthink about whether performance changes were intentional -- even the good ones!  A.k.a., "confirm your kill." --><p>例如，“基线棘轮效应”的过程应该在改进的过程中被锁定。一种合理的方法是将基线自动调整到当前性能的某个百分比内，理想情况是基于标准偏差和/或变异系数的。另一种方法是要求开发者手动执行此操作，以便所有棘轮操作都是有意且深思熟虑后做出的决定。有趣的是，你可能会发现在另一个方向上棘轮也是有帮助的；也就是说，阻止那些显著<em>提高</em>性能却没有提高基线的提交。这便迫使工程师停下来思考性能变化是否是有意的，即使是好的变化也应如此！这也被称为“确认杀戮”。</p><!-- It's of course common that sprint goals remain stable from one sprint to the next.  All numbers can't be improving allthe time.  But this system also helps to ensure that the team doesn't backslide on prior achievements. --><p>从一个迭代周期到下一个迭代周期，目标保持稳定当然是很常见的；所有数字都无法永远处于增长的状态，但是这个系统也有助于确保团队不会退步到以前所取得的结果上。</p><!-- I've found it useful to organize sprint goals behind themes.  Make this sprint about "server performance."  Or "shakeout excessive allocations."  Or something else that gives the team a sense of cohesion, shared purpose, and adds alittle fun into the mix.  As managers, we often forget how important fun is.  It turns out performance can be thegreatest fun of all; it's hugely measurable -- which engineers love -- and, speaking for myself, it's a hell of a timeto pull out the scythe and start whacking away!  It can even be a time to learn as a team, and to even try out some fun,new algorithmic techniques, like [bloom filters](https://en.wikipedia.org/wiki/Bloom_filter). --><p>我发现组织主题背后的迭代目标很有用。制定名为“服务器性能”或者“摆脱过多的分配”的这个迭代周期，能让团队有一种凝聚力和共同目标，并混合添加一点其他有趣的东西。作为经理，我们经常忘记有趣的重要性。事实证明，性能可以是所有人的最大乐趣，并且它也是非常容易测量，这一点上工程师都很喜欢。而且，对我自己来说，这是一个可以暂时放下工作和休息的时刻！这甚至可以作为一整个团队学习的时间，以尝试一些有趣的新算法技术，比如说<a href="https://en.wikipedia.org/wiki/Bloom_filter" target="_blank" rel="noopener">布隆过滤器</a>。</p><!-- Not every performance test needs this level of rigor.  Any that are important enough to automatically run pre-commitmost certainly demand it.  And probably those that are run daily or monthly.  But managing all these goals and baselinesand whatnot can get really cumbersome when there are too many of them.  This is a real risk especially if you'retracking multiple metrics for each of your benchmarks. --><p>并非每项性能测试都需要这种严格程度，但任何足够重要的测试在自动运行提交前肯定都需要它，也许那些每天或每月运行的测试也会如此。但管理所有这些目标和基线，以及当它们太多时，可能会变得非常麻烦。这是会成为一个真正的风险，特别是如果你要跟踪每个基准测试的多个指标。</p><!-- This is where the idea of ["key performance indicators" (KPIs)](https://en.wikipedia.org/wiki/Performance_indicator)becomes very important.  These are the performance metrics important enough to track at a management level, to the wholeteam how healthy the overall product is at any given time.  In my past team who built an operating system and itscomponents, this included things like process startup time, web server throughput, browser performance on standardindustry benchmarks, and number of frames dropped in our realtime audio/video client, including multiple metrics apieceplus the abovementioned statistics metrics.  These were of course in the regularly running pre- and post-commit testsuites, but rolling them up in one place, and tracking against the goals, was a hugely focusing exercise. --><p>这就是<a href="https://en.wikipedia.org/wiki/Performance_indicator" target="_blank" rel="noopener">“关键绩效指标”（KPI）</a>的理念变得非常重要的原因。这些性能指标非常重要，足以使得管理层跟踪整个团队在任何给定时间内整体产品的健康度。在我过去构建操作系统及其组件的团队中，这些指标包括进程启动时间，Web服务器吞吐量，浏览器在标准业界基准测试上的性能以及实时音频/视频客户端中丢帧数等，这包括多个指标以及上述统计指标。这些当然是包含在定期运行的提交前和提交后的测试套件中，但将它们放在同一个地方并对目标进行跟踪，是一个非常聚焦的工作。</p><!-- # In Summary --><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><!-- This post just scratches the surface of how to do good performance engineering, but I hope you walk away with at leastone thing: doing performance well is all about having a great performance culture. --><p>这篇文章只是简单介绍了如何进行良好的性能工程，但我希望你能理解到一件事：在性能做到出色就需要一个良好的性能文化。</p><!-- This culture needs to be infused throughout the entire organization, from management down to engineers, and everybody inbetween.  It needs to be transparent, respectful, aggressive, data-driven, self-critical, and relentlessly ambitious.Great infrastructure and supporting processes are a must, and management needs to appreciate and reward these, just asthey would feature work (and frequently even more).  Only then will the self-reinforcing flywheel get going. --><p>这种文化需要贯穿整个组织，从管理层到工程师，以及介于两者之间的每个人。它需要透明，尊重，积极，数据驱动，自我批评和勃勃雄心。伟大的基础设施和支持流程也是必须的，管理层需要能对这些有所欣赏并进行奖励，就像他们对待功能一样（并需要经常做的更好）。只有这样，自增长的飞轮才会开始运转。</p><!-- Setting goals, communicating regularly, and obsessively tracking goals and customer-facing metrics is paramount. --><p>设定目标，定期沟通，痴迷地追踪目标以及面向客户的指标也是至关重要的。</p><!-- It's not easy to do everything I've written in this article.  It's honestly very difficult to remember to slow down andbe disciplined in these areas, and it's easy to trick yourself into thinking running as fast as possible and worryingabout performance later is the right call.  Well, I'm sorry to tell you, sometimes it is.  You've got to use yourintuition and your gut, however, in my experience, we tend to undervalue performance considerably compared to features. --><p>要达到我在本文中所写的所有内容并不容易。老实说，记得在这些领域放慢速度并坚持这些原则是很难的一件事情，而忽悠自己尽快完成功能并把关于性能的担心留到以后却很容易。好吧，我很遗憾地告诉你，有时它就是这样的。你必须依靠你的直觉和勇气，但是，根据我的经验，与功能相比，我们往往倾向于低估性能。</p><!-- If you're a manager, your team will thank you for instilling a culture like this, and you'll be rewarded by shippingbetter performing software on schedule.  If you're an engineer, I guarantee you'll spend far less time scrambling, moretime being proactive, and more time having fun, in a team obsessed over customer performance.  I'd love to hear what youthink and your own experiences establishing a performance culture. --><p>如果你是一名经理，你的团队会感谢你灌输这样的文化，并且将以按计划交付性能更佳的软件作为回报。如果你是一名工程师，并处于一个痴迷于客户性能的团队中，我保证你处于仓促之中的时间会更少，而处于积极主动状态的时间更多，以及获得更多快乐的时间。同时，我很想知道你的想法以及你自己建立性能文化的经历。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- 
In this essay, I&#39;ll talk about &quot;performance culture.&quot;  Performance is one of the key pillars of software engineering,
and is something
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="操作系统" scheme="https://blog.zhangpf.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="Midori" scheme="https://blog.zhangpf.com/tags/Midori/"/>
    
      <category term="翻译" scheme="https://blog.zhangpf.com/tags/%E7%BF%BB%E8%AF%91/"/>
    
      <category term="性能" scheme="https://blog.zhangpf.com/tags/%E6%80%A7%E8%83%BD/"/>
    
      <category term="软件工程" scheme="https://blog.zhangpf.com/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Midori博客系列翻译（5）——错误模型</title>
    <link href="https://blog.zhangpf.com/2019/03/09/midori/5-the-error-model/"/>
    <id>https://blog.zhangpf.com/2019/03/09/midori/5-the-error-model/</id>
    <published>2019-03-09T13:07:00.000Z</published>
    <updated>2020-01-29T02:19:38.939Z</updated>
    
    <content type="html"><![CDATA[<!-- [Midori](http://joeduffyblog.com/2015/11/03/blogging-about-midori/) was written in [an ahead-of-time compiled, type-safelanguage based on C#](http://joeduffyblog.com/2015/12/19/safe-native-code/).  Aside from our microkernel, the wholesystem was written in it, including drivers, the domain kernel, and all user code.  I've hinted at a few things alongthe way and now it's time to address them head-on.  The entire language is a huge space to cover and will take a seriesof posts.  First up?  The Error Model.  The way errors are communicated and dealt with is fundamental to any language,especially one used to write a reliable operating system.  Like many other things we did in Midori, a "whole system"approach was necessary to getting it right, taking several iterations over several years.  I regularly hear from oldteammates, however, that this is the thing they miss most about programming in Midori.  It's right up there for me too.So, without further ado, let's start.--><p><a href="/2018/10/20/midori/0-blogging-about-midori/">Midori</a>是由<a href="/2019/02/17/midori/4-safe-native-code/">基于C#，通过AOT编译的且类型安全的语言</a>编写而成的操作系统。除了其微内核部分之外，整个系统都由该语言而成，包括驱动程序、域内核和所有的用户态代码。我在前面的文章中已经提及该语言设计的一些方面，现在是时候正面介绍的时候了。整个语言需要巨大的空间来覆盖，也需要一系列的文章来分析。那从什么地方开始呢？错误模型。传递和处理错误的方式是任何一门语言的基础，尤其是用于编写可靠操作系统的语言。像我们在Midori上所做的许多其他事情一样，需要几年的几次迭代的“整个系统（wholesystem）”方法，在错误模型上也是必要的。然而，我经常听到以前团队成员们说，这是他们关于Midori最怀念的地方，而对我来说也是如此。因此，不用多说了，让我们现在开始吧。</p><!-- # Introduction --><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><!-- The basic question an Error Model seeks to answer is: how do "errors" get communicated to programmers and users of thesystem?  Pretty simple, no?  So it seems. --><p>错误模型试图回答的基本问题是：“错误”是如何传递给程序员和系统用户的。很简单，不是吗？至少它看起来是这样的。</p><!-- One of the biggest challenges in answering this question turns out to be defining what an error actually *is*.  Mostlanguages lump bugs and recoverable errors into the same category, and use the same facilities to deal with them.  A`null` dereference or out-of-bounds array access is treated the same way as a network connectivity problem or parsingerror.  This consistency may seem nice at first glance, but it has deep-rooted issues.  In particular, it is misleadingand frequently leads to unreliable code. --><p>回答这个问题的最大挑战之一是，定义错误<em>究竟是什么</em>。大多数语言将程序bug（缺陷）和可恢复的错误归为同一类，并使用相同的工具来处理它们，使得<code>null</code>指针的解引用或数组越界访问，与网络连接问题或解析错误的处理方式相同。这样的一致性乍一眼看起来似乎很不错，但它有根深蒂固的问题，特别是这种方式具有误导性并经常导致代码的不可靠。</p><!-- Our overall solution was to offer a two-pronged error model.  On one hand, you had fail-fast -- we called it*abandonment* -- for programming bugs.  And on the other hand, you had statically checked exceptions for recoverableerrors.  The two were very different, both in programming model and the mechanics behind them.  Abandonmentunapologetically tore down the entire process in an instant, refusing to run any user code while doing so.  (Remember,a typical Midori program had many small, lightweight processes.)  Exceptions, of course, facilitated recovery, but haddeep type system support to aid checking and verification. --><p>我们的整体解决方案是提供双管齐下的错误模型。一方面，对于编程bug来讲，语言有快速失败机制，我们称之为<em>Abandonment（放弃）</em>，另一方面，对于可恢复的错误，语言有静态检查性异常机制。这两者在编程模型和它们背后的机制方面都大相径庭，Abandonment意味着无条件地在瞬间终止整个进程，因此不再运行任何用户代码（还记得么，一个典型的Midori程序由许多小型轻量级的进程组成），而异常当然有助于程序的恢复，同时它也有深层次的类型支持以帮助检查和验证。</p><!-- This journey was long and winding.  To tell the tale, I've broken this post into six major areas: --><p>这段旅程漫长而曲折，为了便于讲述，我将这篇文章分为六个主要部分：</p><!-- * [Ambitions and Learnings](#ambitions-and-learnings)* [Bugs Aren't Recoverable Errors!](#bugs-arent-recoverable-errors)* [Reliability, Fault-Tolerance, and Isolation](#reliability-fault-tolerance-and-isolation)* [Bugs: Abandonment, Assertions, and Contracts](#bugs-abandonment-assertions-and-contracts)* [Recoverable Errors: Type-Directed Exceptions](#recoverable-errors-type-directed-exceptions)* [Retrospective and Conclusions](#retrospective-and-conclusions) --><ul><li><a href="#雄心和学习">野心和学习</a></li><li><a href="#Bug不是可恢复的错误！">Bug不是可恢复的错误！</a></li><li><a href="#可靠性，容错和隔离">可靠性，容错和隔离</a></li><li><a href="#Bug：Abandonment，断言和合约">Bug：Abandonment，断言和合约</a></li><li><a href="#可恢复错误：类型导向的异常">可恢复错误：类型导向的异常</a></li><li><a href="#回顾与总结">回顾与总结</a></li></ul><!-- In hindsight, certain outcomes seem obvious.  Especially given modern systems languages like Go and Rust.  But someoutcomes surprised us.  I'll cut to the chase wherever I can but I'll give ample back-story along the way.  We tried outplenty of things that didn't work, and I suspect that's even more interesting than where we ended up when the dustsettled. --><p>事后看来，某些结果似乎很明显，特别是对于现代系统语言，如Go和Rust来说，但另外一些结果也出乎我们的意料。无论如何我都会尽力讲述这一切，并且会在此过程中提供充足的背景故事。我们尝试了许多了不起作用的东西，因此我觉得这些尝试比最终已知结果时更加有趣。</p><!-- # Ambitions and Learnings --><h1 id="雄心和学习"><a href="#雄心和学习" class="headerlink" title="雄心和学习"></a>雄心和学习</h1><!-- Let's start by examining our architectural principles, requirements, and learnings from existing systems. --><p>让我们首先从检视我们的架构原则和要求，以及从现有的系统中学习开始。</p><!-- ## Principles --><h2 id="原则"><a href="#原则" class="headerlink" title="原则"></a>原则</h2><!-- As we set out on this journey, we called out several requirements of a good Error Model: --><p>在开始这段旅程时，我们提出了，对于一个错误模型来讲的其优异体现的几个要求：</p><!-- * **Usable.**  It must be easy for developers to do the "right" thing in the face of error, almost as if by accident.  A  friend and colleague famously called this falling into the [The Pit of Success](  http://blogs.msdn.com/b/brada/archive/2003/10/02/50420.aspx).  The model should not impose excessive ceremony in order  to write idiomatic code.  Ideally it is cognitively familiar to our target audience.* **Reliable.**  The Error Model is the foundation of the entire system's reliability.  We were building an operating  system, after all, so reliability was paramount.  You might even have accused us as obsessively pursuing extreme  levels of it.  Our mantra guiding much of the programming model development was "*correct by construction*."* **Performant.**  The common case needs to be extremely fast.  That means as close to zero overhead as possible for  success paths.  Any added costs for failure paths must be entirely "pay-for-play."  And unlike many modern systems  that are willing to overly penalize error paths, we had several performance-critical components for which this wasn't  acceptable, so errors had to be reasonably fast too.* **Concurrent.**  Our entire system was distributed and highly concurrent.  This raises concerns that are usually  afterthoughts in other Error Models.  They needed to be front-and-center in ours.* **Diagnosable.**  Debugging failures, either interactively or after-the-fact, needs to be productive and easy.* **Composable.**  At the core, the Error Model is a programming language feature, sitting at the center of a  developer's expression of code.  As such, it had to provide familiar orthogonality and composability with other  features of the system.  Integrating separately authored components had to be natural, reliable, and predictable. --><ul><li><p><strong>可用性</strong>：开发者面对错误时必须很容易做出“正确”的事情，几乎就像下意识一样。一位朋友和同事很准确地将其称为“<a href="http://blogs.msdn.com/b/brada/archive/2003/10/02/50420.aspx" target="_blank" rel="noopener">成功之坑（The Pit of Success）</a>”。为了写出符合习惯的代码，模型不应该施加过多的限制，并且在理想情况下，对于目标受众来说，它应该在认知上是熟悉的。</p></li><li><p><strong>可靠性</strong>：错误模型是整个系统可靠性的基础，毕竟，我们正在构建的是一个操作系统，因此可靠性至关重要，你甚至可能会指责我们痴迷于在此方面追求到极致。我们对编程模型开发的大量指导原则就是“通过构造来纠正错误”。</p></li><li><p><strong>高性能</strong>：在正常情况，系统需要高效运行，这意味着成功路径的开销要尽可能地接近于零，并且失败路径的任何增加的开销必须完全是“按使用付费（pay-for-play）”。与许多愿意为失败路径接受过度开销的现代系统不同，我们有几个性能关键的组件，错误的过度开销对于它们来说是不可接受的，因此错误处理也必须要足够高效。</p></li><li><p><strong>并发</strong>：我们的整个系统是分布式且高度并发，所以这会引起在其他错误模型中通常也会遇到的问题，因此这部分需成为我们工作的最前沿和重心。</p></li><li><p><strong>可诊断</strong>：无论是交互式的还是事后进行的调试，都需要高效且简单。</p></li><li><p><strong>可组合的</strong>：错误模型是编程语言的核心功能，位于开发者代码描述的中心，因此，错误模型必须提供常见的正交性和与系统其他功能的可组合性，与单独编写的组件的集成也必须是自然、可靠和可预测的。</p></li></ul><!-- It's a bold claim, however I do think what we ended up with succeeded across all dimensions. --><p>这是一个勇敢的要求，但我确实认为我们最终在所有方面上取得了成功。</p><!-- ## Learnings --><h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><!-- Existing Error Models didn't meet the above requirements for us.  At least not fully.  If one did well on a dimension,it'd do poorly at another.  For instance, error codes can have good reliability, but many programmers find them errorprone to use; further, it's easy to do the wrong thing -- like forget to check one -- which clearly violates the"pit of success" requirement. --><p>现有的错误模型并不能满足我们的上述要求，至少不完全满足，某些系统在一个维度上做得很好，但在另一个维度上表现不佳。例如，错误码返回的方式具有良好的可靠性，但许多程序员发现容易错误地使用它们，进而很容易出错，比如说忘记检查某些返回码，这显然违反了“成功之坑”的要求。</p><!-- Given the extreme level of reliability we sought, it's little surprise we were dissatisfied with most models.--><p>鉴于对可靠性极高的追求，我们对大多数模型并不满意这一点上并不令人惊讶。</p><!-- If you're optimizing for ease-of-use over reliability, as you might in a scripting language, your conclusions willdiffer significantly.  Languages like Java and C# struggle because they are right at the crossroads of scenarios --sometimes being used for systems, sometimes being used for applications -- but overall their Error Models were veryunsuitable for our needs. --><p>如果你像使用脚本语言那样，对易用性的优化胜过可靠性，那么得到的结论将会非常不同。而像Java和C#这样的语言很难再优化，是因为它们处于不同使用场景的十字路口上：有时用于系统，有时又用于应用程序，但总体而言，它们的错误模型非常不适合我们的要求。</p><!-- Finally, also recall that this story began in the mid-2000s timeframe, before Go, Rust, and Swift were available for ourconsideration.  These three languages have done some great things with Error Models since then. --><p>最后要提醒的是，本篇文章在时间轴上始于2000年代中期，在那之后，Go、Rust和Swift语言在错误模型上做了一些很不错的工作，但在当时，却没有这样的语言可供我们选择。</p><!-- ### Error Codes --><h3 id="错误码"><a href="#错误码" class="headerlink" title="错误码"></a>错误码</h3><!-- Error codes are arguably the simplest Error Model possible.  The idea is very basic and doesn't even require languageor runtime support.  A function just returns a value, usually an integer, to indicate success or failure: --><p>错误码可以说是最简单的错误模型。该想法非常基础，甚至不需要语言或运行时的支持，只需要函数返回一个值，这个值通常是一个整数，表示执行成功或失败：</p><!--     int foo() {        // <try something here>        if (failed) {            return 1;        }        return 0;    } --><pre><code>int foo() {    // &lt;在这里尝试完成某项任务&gt;    if (failed) {        return 1;    }    return 0;} </code></pre><!-- This is the typical pattern, where a return of `0` means success and non-zero means failure.  A caller must check it: --><p>以上是典型的模式，返回“0”表示成功，非零则表示失败，而调用者必须对其进行检查：</p><!--     int err = foo();    if (err) {        // Error!  Deal with it.    } --><pre><code>int err = foo();if (err) {    // 发生错误！需对其进行处理 } </code></pre><!-- Most systems offer constants representing the set of error codes rather than magic numbers.  There may or may not befunctions you can use to get extra information about the most recent error (like `errno` in standard C and`GetLastError` in Win32).  A return code really isn't anything special in the language -- it's just a return value. --><p>大多数系统提供表示错误码的常量集合，而不仅仅是幻数（magic number），你可以使用或不使用相关函数来获取最近一次错误发生的额外信息（例如标准C语言中的<code>errno</code>和Win32中的<code>GetLastError</code>）。返回码在编程语言中并不特别，它从本质上讲仅仅只是一个返回值而已。</p><!-- C has long used error codes.  As a result, most C-based ecosystems do.  More low-level systems code has been writtenusing the return code discipline than any other.  [Linux does](http://linux.die.net/man/3/errno), as do countlessmission-critical and realtime systems.  So it's fair to say they have an impressive track record going for them! --><p>C语言长期使用错误码的方式，因此，大多数基于C的生态系统也都如此。使用返回码规则编写的低层系统代码比任何其他类型的都要多，<a href="http://linux.die.net/man/3/errno" target="_blank" rel="noopener">Linux是这么做的</a>，无数任务关键和实时系统也是如此。因此可以说，它有令人印象深刻的历史记录！</p><!-- On Windows, `HRESULT`s are equivalent.  An `HRESULT` is just an integer "handle" and there are a bunch of constants andmacros in `winerror.h` like `S_OK`, `E_FAULT`, and `SUCCEEDED()`, that are used to create and check values.  The mostimportant code in Windows is written using a return code discipline.  No exceptions are to be found in the kernel.  Atleast not intentionally. --><p>在Windows中，<code>HRESULT</code>与错误码是等效的。<code>HRESULT</code>仅仅是一个整数的“句柄”，在<code>winerror.h</code>中有一堆常量和宏，如<code>S_OK</code>，<code>E_FAULT</code>和<code>SUCCEEDED()</code>等，用于创建和检查这些返回码。Windows中最重要的代码是使用返回码规则编写，例如在内核中是没有异常的，至少不是故意这么做的。</p><!-- In environments with manual memory management, deallocating memory on error is uniquely difficult.  Return codes canmake this (more) tolerable.  C++ has more automatic ways of doing this using [RAII](https://en.wikipedia.org/wiki/Resource_Acquisition_Is_Initialization), but unless you buy into the C++ model whole hog-- which a fair number of systems programmers don't -- then there's no good way to incrementally use RAII in your Cprograms. --><p>在手动管理内存的环境中，出错时释放内存是非常困难的一件事，而返回码可以使这种情况（更容易）被容忍。C++通过使用<a href="https://en.wikipedia.org/wiki/Resource_Acquisition_Is_Initialization" target="_blank" rel="noopener">RAII</a>的方式来自动管理内存，但除非你是C++模型（不是C语言那部分）的彻底贯彻者（事实上相当多的系统程序员并非如此），否则并没有好的方法在你的C程序中增量式地使用RAII。</p><!-- More recently, Go has chosen error codes.  Although Go's approach is similar to C's, it has been modernized with muchnicer syntax and libraries. --><p>再后来，Go选择了错误码方式，虽然Go的方法与C类似，但它更现代化，并且具有更好的语法和库支持。</p><!-- Many functional languages use return codes disguised in monads and named things like `Option<T>`, `Maybe<T>`, or`Error<T>`, which, when coupled with a dataflow-style of programming and pattern matching, feel far more natural.  Thisapproach removes several major drawbacks to return codes that we're about to discuss, especially compared to C.  Rusthas largely adopted this model but has dome some exciting things with it for systems programmers. --><p>许多函数式语言使用monad来封装返回码，并命名为<code>Option&lt;T&gt;</code>、<code>Maybe&lt;T&gt;</code>或<code>Error&lt;T&gt;</code>，使得它们与数据流编程和模式匹配相结合时感觉更自然，特别是与C相比，这种方法消除了我们即将讨论的几个返回码的主要缺点。Rust已经在很大程度上采用了这个模型，它为系统程序员提供了一些令人兴奋的特性。</p><!-- Despite their simplicity, return codes do come with some baggage; in summary: --><p>虽然它很简单，返回码也确实存在一些缺陷，总的说来包括以下几点：</p><!-- * Performance can suffer.* Programming model usability can be poor.* The biggie: You can accidentally forget to check for errors. --><ul><li>性能可能会受到影响；</li><li>编程模型的可用性会变差；</li><li>最重要的一点是，可能会意外地忘记对错误进行检查。</li></ul><!-- Let's discuss each one, in order, with examples from the languages cited above. --><p>让我们依次用上面提及的语言中的例子来讨论每一个缺点。</p><!-- #### Performance --><h4 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h4><!-- Error codes fail the test of "zero overhead for common cases; pay for play for uncommon cases": --><p>错误码未满足所谓“正常情况下零开销，非正常情况下才付费（zero overhead for common cases， pay for play for uncommon cases）”的要求：</p><!-- 1. There is calling convention impact.  You now have *two* values to return (for non-`void` returning functions): the   actual return value and the possible error.  This burns more registers and/or stack space, making calls less   efficient.  Inlining can of course help to recover this for the subset of calls that can be inlined. --><ol><li>对调用约定会会造成影响。函数现在需要返回<em>两个</em>值（对于非void函数来说）：实际的返回值和可能的错误码，从而消耗掉更多寄存器和/或栈空间，导致调用效率降低。当然，内联化调用子集可以弥补这种开销。</li></ol><!-- 1. There are branches injected into callsites anywhere a callee can fail.  I call costs like this "peanut butter,"   because the checks are smeared across the code, making it difficult to measure the impact directly.  In Midori we   were able to experiment and measure, and confirm that yes, indeed, the cost here is nontrivial.  There is also a   secondary effect which is, because functions contain more branches, there is more risk of confusing the optimizer. --><ol start="2"><li>在被调用点可能出现失败的任何地方需要注入分支。我将其称之为“花生酱（peanut butter）”开销，因为检查像花生酱一样被“涂抹”在代码中的各个地方，使得难以直接衡量其影响。在Midori中，我们能够通过试验和测量并确认这种影响——是的，这里的开销是不可忽略的。还有一个次要的影响是，因为函数包含更多的分支，所以更有可能混淆优化器。</li></ol><!-- This might be surprising to some people, since undoubtedly everyone has heard that "exceptions are slow."  It turns outthat they don't have to be.  And, when done right, they get error handling code and data off hot paths which increasesI-cache and TLB performance, compared to the overheads above, which obviously decreases them. --><p>这样的事实对某些人来说可能会吃惊，因为每个人毫无疑问都听说过“异常处理很耗时”的说法。事实证明，异常处理并不低效，当正确地执行时，代码会从热点路径上剔除错误处理的代码和数据，与上面提到的返回码开销相比，这种情况增加了I-cache和TLB的性能。</p><!-- Many high performance systems have been built using return codes, so you might think I'm nitpicking.  As with manythings we did, an easy criticism is that we took too extreme an approach.  But the baggage gets worse. --><p>许多高性能系统都是使用返回码构建的，所以你可能会认为我在挑剔，正如我们所做的许多其他事情一样，一种简单的批评方式是我们采取了极端的方法，但事实是性能变得更糟糕。</p><!-- #### Forgetting to Check Them --><h4 id="忘记对它们进行检查"><a href="#忘记对它们进行检查" class="headerlink" title="忘记对它们进行检查"></a>忘记对它们进行检查</h4><!-- It's easy to forget to check a return code.  For example, consider a function: --><p>忘记检查返回码的情况是很容易发生的。比如说，考虑如下函数：</p><pre><code>int foo() { ... } </code></pre><!-- Now at the callsite, what if we silently ignore the returned value entirely, and just keep going?--><p>在调用点上，如果现在我们悄悄地忽略掉返回码，然后继续执行，会出现什么情况呢？</p><!--     foo();    // Keep going -- but foo might have failed! --><pre><code>foo();// 继续执行，但foo可能执行失败了！ </code></pre><!-- At this point, you've masked a potentially critical error in your program.  This is easily the most vexing anddamaging problem with error codes.  As I will show later, option types help to address this for functional languages.But in C-based languages, and even Go with its modern syntax, this is a real issue. --><p>此时，你已经掩盖了程序中可能存在的严重错误，这很容易成为错误码最棘手和最具破坏性的问题。正如我稍后将要介绍的那样，选项类型（option）有助于在函数式语言中解决这个问题，但是在基于C的语言，甚至是Go的现代语法中，这是一个真实存在的问题。</p><!-- This problem isn't theoretical.  I've encountered numerous bugs caused by ignoring return codes and I'm sure you havetoo.  Indeed, in the development of this very Error Model, my team encountered some fascinating ones.  For example, whenwe ported Microsoft's Speech Server to Midori, we found that 80% of Taiwan Chinese (`zh-tw`) requests were failing.  Notfailing in a way the developers immediately saw, however; instead, clients would get a gibberish response.  At first, wethought it was our fault.  But then we discovered a silently swallowed `HRESULT` in the original code.  Once we got itover to Midori, the bug was thrown into our faces, found, and fixed immediately after porting.  This experiencecertainly informed our opinion about error codes. --><p>这个问题不仅仅是在理论上存在，实际上我遇到了无数忽略返回码导致的错误，我相信你也遇到过。正是在这个错误模型的开发中，我的团队遇到了一些令人大开眼界的问题。例如，当我们将微软的语音服务器移植到Midori时，我们发现80%的繁体中文（<code>zh-tw</code>）请求都失败了，但开发者并不会立即看到失败并对其进行处理，相反地，客户端会得到莫名其妙的回应。起初，我们认为这是Miodri的问题，但后来我们在原始代码中发现了一个悄悄忽略掉的<code>HRESULT</code>。一旦我们把它移植到Midori上，这个bug就立刻出现在我们的眼帘，被找到并在移植后立即得到了修复。毫无疑问地，这样的经验告诉了我们对错误码的上述看法。</p><!-- It's surprising to me that Go made unused `import`s an error, and yet missed this far more critical one.  So close! --><p>令我惊讶的是，Go将未使用的<code>import</code>当成错误对待，但却错过了这个更为关键性的错误，而它们仅仅一步之遥！</p><!-- It's true you can add a static analysis checker, or maybe an "unused return value" warning as most commercial C++compilers do.  But once you've missed the opportunity to add it to the core of the language, as a requirement, none ofthose techniques will reach critical mass due to complaints about noisy analysis. --><p>没错，你可以增加静态分析检查器，或者像大多数商业C++编译器那样增加“未使用的返回值”等警告方式。 但是，一旦你错过了将其添加到语言核心并作为一项要求的机会，开发者处于对烦人的分析和警告的抱怨，这些技术都不会起到关键的作用。</p><!-- For what it's worth, forgetting to use return values in our language was a compile time error.  You had to explicitlyignore them; early on we used an API for this, but eventually devised language syntax -- the equivalent of `>/dev/null`: --> <p>不管值不值得，在我们的语言中忘记使用返回值会造成编译时错误，因此你必须显式地忽略它们。在早先时候，我们用API实现这个功能，但最终将其设计成语言的语法部分，其功能相当于<code>&gt;/dev/null</code>：</p><pre><code>ignore foo(); </code></pre><!-- We didn't use error codes, however the inability to accidentally ignore a return value was important for the overallreliability of the system.  How many times have you debugged a problem only to find that the root cause was a returnvalue you forgot to use?  There have even been security exploits where this was the root cause.  Letting developers say`ignore` wasn't bulletproof, of course, as they could still do the wrong thing.  But it was at least explicit andauditable. --><p>虽然我们未选择使用错误码进行错误处理，但无法意外地忽略返回值对于系统的整体可靠性来讲非常重要。试想你多少次在调试一个问题时，才发现其根本原因是忘记使用返回值，更有甚者因为此问题造成了安全漏洞。当然，因为程序员依然可以做坏事，所以让他们使用<code>ignore</code>并不是完全防弹的方法，但它至少是显式的和可审计的。</p><!-- #### Programming Model Usability --><h4 id="编程模型的可用性"><a href="#编程模型的可用性" class="headerlink" title="编程模型的可用性"></a>编程模型的可用性</h4><!-- In C-based languages with error codes, you end up writing lots of hand-crafted `if` checks everywhere after functioncalls.  This can be especially tedious if many of your functions fail which, in C programs where allocation failures arealso communicated with return codes, is frequently the case.  It's also clumsy to return multiple values. --><p>在具有错误码方式的基于C的语言中，如果在任何的函数调用位置进行检查，那么会导致大量手动的<code>if</code>语句的检查。尤其是如果大量函数调用失败是很常见的情况时，这可能会特别繁琐，因为在C语言的程序中，分配失败也需要通过返回码来交互。另外，多值返回也是一件很笨拙的事情。</p><!-- A warning: this complaint is subjective.  In many ways, the usability of return codes is actually elegant.  You reusevery simple primitives -- integers, returns, and `if` branches -- that are used in myriad other situations.  In myhumble opinion, errors are an important enough aspect of programming that the language should be helping you out. --><p>警告：这样的抱怨其实是主观的，在许多方面，返回码的可用性实际上是优雅的。你可以重用非常简单的原语——整型，return和<code>if</code>分支，并在无数其他情况下使用。在我看来，错误也是编程的足够重要的一个方面，语言应该需要在这方面对你有所帮助。</p><!-- Go has a nice syntactic shortcut to make the standard return code checking *slightly* more pleasant: --><p>Go有很好的语法快捷方式，能够<em>稍微</em>更愉快地对标准的返回码进行检查：</p><!--     if err := foo(); err != nil {        // Deal with the error.    } --><pre><code>if err := foo(); err != nil {    // 处理错误 } </code></pre><!-- Notice that we've invoked `foo` and checked whether the error is non-`nil` in one line.  Pretty neat. --><p>注意，我们在一行中调用了<code>foo</code>并在检查错误是否为非<code>nil</code>，这种方式相当简约。</p><!-- The usability problems don't stop there, however. --><p>然而，可用性的问题并不止于此。</p><!-- It's common that many errors in a given function should share some recovery or remediation logic.  Many C programmersuse labels and `goto`s to structure such code.  For example: --><p>通常，对于给定函数中的许多错误应该共享一些恢复或补救的逻辑代码，许多C程序员使用标签和<code>goto</code>语句来构建这样的代码，比如说：</p><!--     int error;    // ...    error = step1();    if (error) {        goto Error;    }    // ...    error = step2();    if (error) {        goto Error;    }    // ...    // Normal function exit.    return 0;    // ...    Error:    // Do something about `error`.    return error; --><pre><code>int error;// ...error = step1();if (error) {    goto Error;}// ...error = step2();if (error) {    goto Error;}// ...// 函数正常退出 return 0;// ...Error:// 错误相关的处理 return error; </code></pre><!-- Needless to say, this is the kind of code only a mother could love. --><p>不用说，这是只有妈妈才会爱的代码。</p><!-- In languages like D, C#, and Java, you have `finally` blocks to encode this "before scope exits" pattern more directly.Similarly, Microsoft's proprietary extensions to C++ offer `__finally`, even if you're not fully buying into RAII andexceptions.  And D provides `scope` and Go offers `defer`.  All of these help to eradicate the `goto Error` pattern. --><p>在D、C#和Java等语言中，可使用<code>finally</code>块对这种“在作用域退出之前”的模式进行更直接地编码，同样地，即使没有完全采用RAII和异常的方式，微软也对C++提供了专有扩展<code>__finally</code>。D提供<code>scope</code>，Go提供了<code>defer</code>，所有这些都有助于彻底消除<code>goto Error</code>的模式。</p><!-- Next, imagine my function wants to return a real value *and* the possibility of an error?  We've burned the return slotalready so there are two obvious possibilities: --><p>接下来，想象一下函数是如何返回真实的值<em>和</em>可能的错误码的？由于我们已经破坏了返回槽，所以有两种明显的可能做法：</p><!-- 1. We can use the return slot for one of the two values (commonly the error), and another slot -- like a pointer   parameter -- for the other of the two (commonly the real value).  This is the common approach in C.2. We can return a data structure that carries the possibility of both in its very structure.  As we will see, this is   common in functional languages.  But in a language like C, or Go even, that lacks parametric polymorphism, you lose   typing information about the returned value, so this is less common to see.  C++ of course adds templates, so in   principle it could do this, however because it adds exceptions, the ecosystem around return codes is lacking. --><ol><li>可以将返回槽用于两个值中的一个（通常是错误码），而另一个槽，比如指针参数的形式，用于两者中的另一个（通常是实际值），这也是C中的常用做法；</li><li>可以返回一个同时在结构中具有两者的数据结构。在后文中我们将会看到，这种做法在函数式语言中相当常见，但是对于C或Go语言，由于缺少参数多态，会丢失返回值的类型信息，所以这种方式不太常看到。当然由于C++添加模板，因此原则上它也可以做到，但另一方面C++也同时增加了异常机制，所以返回码在生态系统上是匮乏的。</li></ol><!-- In support of the performance claims above, imagine what both of these do to your program's resulting assembly code. --><p>为了支持上面的性能要求，想象一下这两种方法对程序生成的汇编代码有何影响。</p><!-- ##### Returning Values "On The Side" --><h5 id="从“侧面”返回值"><a href="#从“侧面”返回值" class="headerlink" title="从“侧面”返回值"></a>从“侧面”返回值</h5><!-- An example of the first approach in C looks like this: --><p>第一种方法在C中的例子如下所示：</p><!--     int foo(int* out) {        // <try something here>        if (failed) {            return 1;        }        *out = 42;        return 0;    } --><pre><code>int foo(int* out) {    // &lt;在这里尝试操作&gt;    if (failed) {        return 1;    }    *out = 42;    return 0;} </code></pre><!-- The real value has to be returned "on the side," making callsites clumsy: --><p>真正的返回值必须“在侧面”（在本例中通过指针参数）返回，使得调用变得十分笨拙：</p><!--     int value;    int ret = foo(&value);    if (ret) {        // Error!  Deal with it.    }    else {        // Use value...    } --><pre><code>int value;int ret = foo(&amp;value);if (ret) {    // 出现错误，对其进行处理 }else {    // 使用返回值...} </code></pre><!-- In addition to being clumsy, this pattern perturbs your compiler's [definite assignment analysis](https://en.wikipedia.org/wiki/Definite_assignment_analysis) which impairs your ability to get good warnings aboutthings like using uninitialized values.--><p>除了变得笨拙之外，这种模式还会扰乱编译器的<a href="https://en.wikipedia.org/wiki/Definite_assignment_analysis" target="_blank" rel="noopener">定义赋值分析</a>，削弱编译器对使用未初始化值等情况发出良好警告的能力。</p><!-- Go also takes aim at this problem with nicer syntax, thanks to multi-valued returns: --><p>归功于多值返回，Go还可以通过更好的语法来解决这个问题：</p><pre><code>func foo() (int, error) {    if failed {        return 0, errors.New(&quot;Bad things happened&quot;)    }    return 42, nil} </code></pre><!-- And callsites are much cleaner as a result.  Combined with the earlier feature of single-line `if` checking for errors-- a subtle twist, since at first glance the `value` return wouldn't be in scope, but it is -- this gets a touch nicer: --><p>这使得调用点变得更加整洁，结合先前提到的单行<code>if</code>检查错误（一个微小的改变，因为乍一看返回的<code>value</code>不在作用域内）的特性，它确实成为更好的处理方式：</p><!--     if value, err := foo(); err != nil {        // Error!  Deal with it.    } else {        // Use value ...    } --><pre><code>if value, err := foo(); err != nil {    // 发生错误并处理它 } else {    // 使用值 ...} </code></pre><!-- Notice that this also helps to remind you to check the error.  It's not bulletproof, however, because functions canreturn an error and nothing else, at which point forgetting to check it is just as easy as it is in C. --><p>注意这也有助于提醒你对错误进行检查。然而，它也不是完全没有问题，因为函数可以仅仅返回错误码，此时忘记对其进行检查就像在C中一样的容易。</p><!-- As I mentioned above, some would argue against me on the usability point.  Especially Go's designers, I suspect.  A bigappeal to Go using error codes is as a rebellion against the overly complex languages in today's landscape.  We havelost a lot of what makes C so elegant -- that you can usually look at any line of code and guess what machine code ittranslates into.  I won't argue against these points.  In fact, I vastly prefer Go's model over both uncheckedexceptions and Java's incarnation of checked exceptions.  Even as I write this post, having written lots of Go lately,I look at Go's simplicity and wonder, did we go too far with all the `try` and `requires` and so on that you'll seeshortly?  I'm not sure.  Go's error model tends to be one of the most divisive aspect of the language; it's probablylargely because you can't be sloppy with errors as in most languages, however programmers really did enjoy writing codein Midori's.  In the end, it's hard to compare them.  I'm convinced both can be used to write reliable code. --><p>正如我上面提到的，有些人会在可用性上反驳我，我想对于Go的设计师来说，更是如此。因为Go使用错误码的一个巨大吸引力是，这是当今对于过于复杂的语言的一种反叛，我们已经失去了很多C语言的优雅特性——通常情况下，你可以查看任何一行C语言代码并猜测它编译后的机器代码。我不反对这种观点，事实上，我喜欢Go的模型胜过非检查性异常或Java中的检查性异常的化身，并且即使是在我最近写这篇文章的时候，也写了很多Go代码。我看着Go的简单性并问自己，我们是否在所有的<code>try</code>和<code>requires</code>等你将很快会看到的内容上的走得太远？我不确定。Go的错误模型往往是该语言最具分裂性的方面之一，这很大程度上是因为你不能像大多数语言那样草率地对待错误，但程序员确实喜欢在Midori中编写代码。最后，很难对它们进行比较，但我确信两者都可以用来编写可靠的代码。</p><!-- ##### Return Values in Data Structures --><h5 id="从数据结构中返回"><a href="#从数据结构中返回" class="headerlink" title="从数据结构中返回"></a>从数据结构中返回</h5><!-- Functional languages address many of the usability challenges by packaging up the possibility of *either* a value*or* an error, into a single data structure.  Because you're forced to pick apart the error from the value if you wantto do anything useful with the value at the callsite -- which, thanks to a dataflow style of programming, you probablywill -- it's easy to avoid the killer problem of forgetting to check for errors. --><p>函数语言通过将值或错误的可能性打包到单个数据结构的方式解决许多可用性上挑战，因为如果你想对调用点上的返回值做任何有意义的事情，就不得不从返回值中挑出错误。这要归功于数据流风格的编程，使得很容易避免遗忘检查错误这种杀手级的问题。</p><!-- For an example of a modern take on this, check out [Scala's `Option` type](http://danielwestheide.com/blog/2012/12/19/the-neophytes-guide-to-scala-part-5-the-option-type.html).  The unfortunatenews is that some languages, like those in the ML family and even Scala (thanks to its JVM heritage), mix this elegantmodel with the world of unchecked exceptions.  This taints the elegance of the monadic data structure approach. --><p>有关此方法的现代语言示例，请查看<a href="http://danielwestheide.com/blog/2012/12/19/the-neophytes-guide-to-scala-part-5-the-option-type.html" target="_blank" rel="noopener">Scala种的<code>Option</code>类型</a>。但不幸消息是，有些语言，例如ML系列中的语言，甚至是Scala（由于其JVM的血统），将这种优雅的模型与非检查性异常方法混合在一起，从而玷污了monad数据结构方法的优雅性。</p><!-- Haskell does something even cooler and [gives the illusion of exception handling while still using error values andlocal control flow](https://wiki.haskell.org/Exception): --><p>Haskell做的事情更酷，<a href="https://wiki.haskell.org/Exception" target="_blank" rel="noopener">在仍然使用错误值和局部控制流的前提下，提供了类似异常处理的错觉</a>：</p><!-- > There is an old dispute between C++ programmers on whether exceptions or error return codes are the right way.  Niklas> Wirth considered exceptions to be the reincarnation of GOTO and thus omitted them in his languages.  Haskell solves> this problem a diplomatic way: Functions return error codes, but the handling of error codes does not uglify the code. --><blockquote><p>C++程序员之间就异常还是错误返回码是正确的方式上存在长期的争议。Niklas Wirth认为异常是goto方式的转世，因此在他的语言中消除了这种方式，Haskell以更圆滑的方式解决了这个问题：函数返回错误码，但错误码的处理不会让代码变丑。</p></blockquote><!-- The trick here is to support all the familiar `throw` and `catch` patterns, but using monads rather than control flow. --><p>这里的技巧是使用monad而不是控制流，以支持所有熟悉的<code>throw</code>和<code>catch</code>模式。</p><!-- Although [Rust also uses error codes](https://doc.rust-lang.org/book/error-handling.html) it is also in the style ofthe functional error types.  For example, imagine we are writing a function named `bar` in Go: we'd like to call `foo`,and then simply propagate the error to our caller if it fails: --><p>虽然<a href="https://doc.rust-lang.org/book/error-handling.html" target="_blank" rel="noopener">Rust也使用错误码方式</a>，但它也采用函数式错误类型的风格。例如，假设我们在Go中编写了一个名为<code>bar</code>的函数，它调用<code>foo</code>，并只是在调用出错时才将错误传播给调用者：</p><pre><code>func bar() error {    if value, err := foo(); err != nil {        return err    } else {        // 使用值...    }} </code></pre><!-- The longhand version in Rust isn't any more concise.  It might, however, send C programmers reeling with its foreignpattern matching syntax (a real concern but not a dealbreaker).  Anyone comfortable with functional programming,however, probably won't even blink, and this approach certainly serves as a reminder to deal with your errors: --><p>而Rust中的速记版本就不那么简洁。虽然这种外来的模式匹配语法（一个关注点而不是问题的死结）可能会让C程序员发昏，然而，任何对函数编程感到满意的人可能甚至都不会眨眼，并且这种方法毫无疑问地会对错误处理进行提醒：</p><!--     fn bar() -> Result<(), Error> {        match foo() {            Ok(value) => /* Use value ... */,            Err(err) => return Err(err)        }    } --><pre><code>fn bar() -&gt; Result&lt;(), Error&gt; {    match foo() {        Ok(value) =&gt; /* 使用值... */,        Err(err) =&gt; return Err(err)    }} </code></pre><!-- But it gets better.  Rust has a [`try!` macro](https://doc.rust-lang.org/std/macro.try!.html) that reduces boilerplatelike the most recent example to a single expression: --><p>但这种方式还可以变得更好，Rust具有<a href="https://doc.rust-lang.org/std/macro.try!.html" target="_blank" rel="noopener"><code>try!</code>宏</a>，可以将上述写法减少到单行的表达式：</p><!--     fn bar() -> Result<(), Error> {        let value = try!(foo);        // Use value ...    } --><pre><code>fn bar() -&gt; Result&lt;(), Error&gt; {    let value = try!(foo);    // 使用值...} </code></pre><!-- This leads us to a beautiful sweet spot.  It does suffer from the performance problems I mentioned earlier, but doesvery well on all other dimensions.  It alone is an incomplete picture -- for that, we need to cover fail-fast (a.k.a.abandonment) -- but as we will see, it's far better than any other exception-based model in widespread use today. --><p>这种方式提供了在平衡各种因素下的最佳位置。虽然它确实会遭遇到我之前提到的性能问题，但在所有其他方面都已经做得很好，但仅性能这一点，它便不是完整的解决方案。所以为此我们需要包含对快速失败（也就是Abandonment）的处理，正如我们将会看到的，它远远优于任何当今广泛使用的其他基于异常的模型。</p><!-- ### Exceptions --><h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><!-- The history of exceptions is fascinating.  During this journey I spent countless hours retracing the industry's steps.That includes reading some of the original papers -- like Goodenough's 1975 classic, [Exception Handling: Issuesand a Proposed Notation](https://www.cs.virginia.edu/~weimer/2006-615/reading/goodenough-exceptions.pdf) -- in additionto looking at the approaches of several languages: Ada, Eiffel, Modula-2 and 3, ML, and, [most inspirationally, CLU](http://csg.csail.mit.edu/pubs/memos/Memo-155/Memo-155-3.pdf).  Many papers do a better job than I can summarizing thelong and arduous journey, so I won't do that here.  Instead, I'll focus on what works and what doesn't work for buildingreliable systems. --><p>异常的历史是令人着迷的。在错误模型的研发过程中，我花了无数时间来回顾行业在这个领域的发展，其中包括阅读的一些原始论文，如Goodenough在1975年经典文章，<a href="https://www.cs.virginia.edu/~weimer/2006-615/reading/goodenough-exceptions.pdf" target="_blank" rel="noopener">Exception Handling: Issues and a Proposed Notation</a>。除此之外，还研究了几种语言的方法：Ada，Eiffel，Modula-2和3，ML以及<a href="http://csg.csail.mit.edu/pubs/memos/Memo-155/Memo-155-3.pdf" target="_blank" rel="noopener">给人以灵感的CLU</a>。许多论文比我能总结的漫长而艰辛之旅做得更好，所以我不将不在此赘述，相反，我会专注于对于构建可靠系统来讲，哪些工作是有效以及哪些是无效的。</p><!-- Reliability is the most important of our requirements above when developing the Error Model.  If you can't reactappropriately to failures, your system, by definition, won't be very reliable.  Operating systems generally speaking needto be reliable.  Sadly, the most commonplace model -- unchecked exceptions -- is the worst you can do in this dimension. --><p>在开发错误模型时，可靠性在我们上述的要求中是最重要的，如果无法对故障做出适当的反应，那么根据定义，系统将不会非常可靠。而通常来说，操作系统需要就是可靠性，但可悲的是，最常见的模型——非检查性异常（unchecked exception），在这个维度上是做的最差的。</p><!-- For these reasons, most reliable systems use return codes instead of exceptions.  They make it possible to locallyreason about and decide how best to react to error conditions.  But I'm getting ahead of myself.  Let's dig in. --><p>出于以上原因，大多数可靠系统使用返回码而不是异常模型，因为错误码使得，在局部进行分析并决定如何最好地对错误条件进行处理成为可能。但是我说过头了，让我们继续深入下去。</p><!-- ### Unchecked Exceptions --><h3 id="非检查性异常"><a href="#非检查性异常" class="headerlink" title="非检查性异常"></a>非检查性异常</h3><!-- A quick recap.  In an unchecked exceptions model, you `throw` and `catch` exceptions, without it being part of the typesystem or a function's signature.  For example: --><p>让我们来快速回顾一下，在非检查性异常模型中，代码<code>throw</code>并<code>catch</code>异常，但异常不是类型系统或函数签名的一部分。例如：</p><!--     // Foo throws an unhandled exception:    void Foo() {        throw new Exception(...);    }    // Bar calls Foo, and handles that exception:    void Bar() {        try {            Foo();        }        catch (Exception e) {            // Handle the error.        }    }    // Baz also calls Foo, but does not handle that exception:    void Baz() {        Foo(); // Let the error escape to our callers.    } --><pre><code>// Foo抛出一个未处理的异常： void Foo() {    throw new Exception(...);}// Bar调用Foo，并处理该异常： void Bar() {    try {        Foo();    }    catch (Exception e) {        // 处理错误     }}// Baz也调用Foo，但不处理该异常： void Baz() {    Foo(); // 让错误逃逸到Baz的调用者 } </code></pre><!-- In this model, any function call -- and sometimes any *statement* -- can throw an exception, transferring controlnon-locally somewhere else.  Where?  Who knows.  There are no annotations or type system artifacts to guide youranalysis.  As a result, it's difficult for anyone to reason about a program's state at the time of the throw, the statechanges that occur while that exception is propagated up the call stack -- and possibly across threads in a concurrentprogram -- and the resulting state by the time it gets caught or goes unhandled. --><p>在这个模型中，任何函数调用（有时是任何<em>语句</em>）都可以抛出异常，并将控制权转移到其他非局部的位置。那么到底转移到何处？没有人知道。因为没有注解或类型系统的制品能辅助你的分析，因此任何人都很难推断程序在抛出时的状态，和异常被捕获或保持未处理时的最终状态。因为当异常在调用栈中向上传播，甚至在并发程序中以跨线程的方式传播时，程序的状态都可能发生更改。</p><!-- It's of course possible to try.  Doing so requires reading API documentation, doing manual audits of the code, leaningheavily on code reviews, and a healthy dose of luck.  The language isn't helping you out one bit here.  Because failuresare rare, this tends not to be as utterly disastrous as it sounds.  My conclusion is that's why many people in the industrythink unchecked exceptions are "good enough."  They stay out of your way for the common success paths and, because mostpeople don't write robust error handling code in non-systems programs, throwing an exception *usually* gets you out of apickle fast.  Catching and then proceeding often works too.  No harm, no foul.  Statistically speaking, programs "work." --><p>当然这也是可以尝试的，为此需要阅读API文档，对代码进行手动审计，并严重依赖于代码审查以及良好的运气，但语言对此没有提供任何帮助。 因为程序中的故障总是少数，所以这并不像听起来那样完全是灾难性的。我的结论是，这就是为什么业内很多人认为非检查异常已经“足够好”，他们会为了通常的成功路径而避免引入其它的干扰，因为大多数人都不会在非系统程序中编写强大的错误处理代码，而抛出异常<em>通常</em>会让你快速逃离错误引起的困境。捕捉错误然后继续运行通常也是有效的，没有造成伤害，没有不合规定。因此从统计上讲，程序“能够工作”。</p><!-- Maybe statistical correctness is okay for scripting languages, but for the lowest levels of an operating system, or anymission critical application or service, this isn't an appropriate solution.  I hope this isn't controversial. --><p>也许这种统计的正确性对于脚本语言是可行的，但对于操作系统的最低层次或任何任务关键的应用程序和服务而言，这不是一个合适的解决方案。我希望对此是没有争议的。</p><!-- .NET makes a bad situation even worse due to *asynchronous exceptions*.  C++ has so-called "asynchronous exceptions"too: these are failures that are triggered by hardware faults, like access violations.  It gets really nasty in .NET,however.  An arbitrary thread can inject a failure at nearly any point in your code.  Even between the RHS and LHS of anassignment!  As a result, things that look atomic in source code aren't.  [I wrote about this 10 years ago](http://joeduffyblog.com/2005/03/18/atomicity-and-asynchronous-exception-failures/) and the challenges still exist,although the risk has lessened as .NET generally learned that thread aborts are problematic.  The new CoreCLR evenlacks AppDomains, and the new ASP.NET Core 1.0 stack certainly doesn't use thread aborts like it used to.  But the [APIs arestill there](https://github.com/dotnet/coreclr/blob/99e7f7c741a847454ab0ace1febd911378dcb464/src/mscorlib/src/System/Threading/Thread.cs#L518). --><p>在.NET中，由于<em>异步异常</em>的存在，使得情况更加糟糕。C++也有所谓的“异步异常”：由硬件故障触发的故障，例如访问违例。然而，它在.NET中，它却显得非常令人讨厌，因为任意线程几乎可以在代码中的任何一处注入失败，即使在赋值语句的RHS和LHS之间也是如此！ 因此，源代码中看起来像原子的操作，在实际中却并不如此。我在<a href="http://joeduffyblog.com/2005/03/18/atomicity-and-asynchronous-exception-failures/" target="_blank" rel="noopener">10年前写过关于此的文章</a>，尽管现在已普遍意识到.NET的线程中止是有问题的并使得这种风险减弱，但挑战实际上仍然是存在的。新的CoreCLR甚至缺少AppDomains，并且在ASP.NET Core 1.0中，栈肯定不会像过去那样使用线程中止，但<a href="https://github.com/dotnet/coreclr/blob/99e7f7c741a847454ab0ace1febd911378dcb464/src/mscorlib/src/System/Threading/Thread.cs#L518" target="_blank" rel="noopener">它们的API仍然存在</a>。</p><!-- There's a famous interview with Anders Hejlsberg, C#'s chief designer, called [The Trouble with Checked Exceptions](http://www.artima.com/intv/handcuffs.html).  From a systems programmer's perspective, much of it leaves you scratchingyour head.  No statement affirms that the target customer for C# was the rapid application developer more than this: --><p>有一个关于C#的首席设计师Anders Hejlsberg的著名采访，名为<a href="http://www.artima.com/intv/handcuffs.html" target="_blank" rel="noopener">The Trouble with Checked Exceptions</a>。从系统程序员的角度来看，大部分的内容都会让你感到困惑，但有一点却是可以理解的，没有声明能够肯定C#的目标客户是超过如下内容的快速应用程序开发者：</p><!-- > *Bill Venners*: But aren't you breaking their code in that case anyway, even in a language without checked exceptions?> If the new version of foo is going to throw a new exception that clients should think about handling, isn't their> code broken just by the fact that they didn't expect that exception when they wrote the code?> > *Anders Hejlsberg* : No, because in a lot of cases, people don't care. They're not going to handle any of these> exceptions. There's a bottom level exception handler around their message loop. That handler is just going to bring> up a dialog that says what went wrong and continue. The programmers protect their code by writing try finally's> everywhere, so they'll back out correctly if an exception occurs, but they're not actually interested in handling> the exceptions. --><blockquote><p><em>Bill Venners</em>：但是你不是在这种情况下破坏他们的代码，即使是在缺乏检查性异常的语言中也是如此么？ 如果foo的新版本将抛出一个应该处理的新异常类型，那么他们的程序是不是仅仅因为在编写代码时没有预料该异常而导致崩溃呢？</p></blockquote><blockquote><p><em>Anders Hejlsberg</em>：不会，因为在很多情况下，人们并不关心，他们不会处理任何这些异常情况。在他们的消息循环中有一个底层的异常处理程序，该处理程序只是打开一个对话框，说明出了什么问题然后继续执行。程序员通过在任何地方编写<code>try</code>和<code>finally</code>来保护他们的代码，因此如果发生异常他们将正确退出，但他们实际上对异常的处理并不感兴趣。</p></blockquote><!-- This reminds me of `On Error Resume Next` in Visual Basic, and the way Windows Forms automatically caught and swallowederrors thrown by the application, and attempted to proceed.  I'm not blaming Anders for his viewpoint here; heck, forC#'s wild popularity, I'm convinced it was the right call given the climate at the time.   But this sure isn't the wayto write operating system code. --><p>这让我想起了Visual Basic中的<code>On Error Resume Next</code>，以及Windows Forms自动捕获并透明处理应用程序抛出的错误，然后尝试继续执行的方式。 我在这里并不责怪Anders的观点，由于C#的受欢迎程度，我确信这是当时大背景下的正确选择，但这肯定不是编写操作系统代码的正确方法。</p><!-- C++ at least *tried* to offer something better than unchecked exceptions with its [`throw` exception specifications](http://en.cppreference.com/w/cpp/language/except_spec).  Unfortunately, the feature relied on dynamic enforcement whichsounded its death knell instantaneously. --><p>C++至少<em>尝试过</em>使用它的<a href="http://en.cppreference.com/w/cpp/language/except_spec" target="_blank" rel="noopener"><code>throw</code>异常规范</a>提供比非检查性异常更好的方式。 但不幸的是，这个功能依赖于动态增强（dynamic enhancement），凭这一点便宣告了它的失败。</p><!-- If I write a function `void f() throw(SomeError)`, the body of `f` is still free to invoke functions that throw thingsother than `SomeError`.  Similarly, if I state that `f` throws no exceptions, using `void f() throw()`, it's stillpossible to invoke things that throw.  To implement the stated contract, therefore, the compiler and runtime must ensurethat, should this happen, `std::unexpected` is called to rip the process down in response.--><p>如果我写一个函数<code>void f() throw(SomeError)</code>，<code>f</code>的函数体仍然可以自由地调用抛出除SomeError之外的异常的函数；类似地，如果我声明<code>f</code>不会抛出异常，但使用<code>void f() throw()</code>仍然可以调用抛出异常的函数。因此，为了实现所述的合约，编译器和运行时必须确保，如果发生这两种情况，则需调用<code>std::unexpected</code>杀死进程以作为响应。</p><!-- I'm not the only person to recognize this design was a mistake.  Indeed, `throw` is now deprecated.  A detailed WG21paper, [Deprecating Exception Specifications](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2010/n3051.html),describes how C++ ended up here, and has this to offer in its opening statement: --><p>我不是唯一认识到这种设计是错误的人，事实上，<code>throw</code>已经遭到弃用。一篇详细的WG21论文，<a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2010/n3051.html" target="_blank" rel="noopener">Deprecating Exception Specifications</a>，阐述了C++是如何沦落于此的，在这里我提供了此文的开场白：</p><!-- > Exception specifications have proven close to worthless in practice, while adding a measurable overhead to programs. --><blockquote><p>事实证明，异常规范在实践中几乎毫无价值，反而还为程序增加了可见的开销。</p></blockquote><!-- The authors list three reasons for deprecating `throw`.  Two of the three reasons were a result of the dynamic choice:runtime checking (and its associated opaque failure mode) and runtime performance overheads.  The third reason, lackof composition in generic code, could have been dealt with using a proper type system (admittedly at an expense). --><p>作者列举了弃用<code>throw</code>的三个原因。这三个原因中的两个是动态选择的结果：运行时检查（及其关联的opaque故障模式）和运行时性能开销。第三个原因是，虽然泛型代码中缺乏合成，但可以使用适当的类型系统来解决（当然这也是有开销的）。</p><!-- But the worst part is that the cure relies on yet another dynamically enforced construct -- the [`noexcept` specifier](http://en.cppreference.com/w/cpp/language/noexcept_spec) -- which, in my opinion, is just as bad as the disease. --><p>但最糟糕的是，这种方法依赖于另一个动态强制构造——<a href="http://en.cppreference.com/w/cpp/language/noexcept_spec" target="_blank" rel="noopener"><code>noexcept</code>分类符</a>，在我看来，这种解决方案与问题本身一样糟糕。</p><!-- ["Exception safety"](https://en.wikipedia.org/wiki/Exception_safety) is a commonly discussed practice in the C++community.  This approach neatly classifies how functions are intended to behave from a caller's perspective withrespect to failure, state transitions, and memory management.  A function falls into one of four kinds: *no-throw* meansforward progress is guaranteed and no exceptions will emerge; *strong safety* means that state transitions happenatomically and a failure will not leave behind partially committed state or broken invariants; *basic safety* meansthat, though a function might partially commit state changes, invariants will not be broken and leaks are prevented; andfinally, *no safety* means anything's possible.  This taxonomy is quite helpful and I encourage anyone to be intentionaland rigorous about error behavior, either using this approach or something similar.  Even if you're using error codes.The problem is, it's essentially impossible to follow these guidelines in a system using unchecked exceptions, exceptfor leaf node data structures that call a small and easily auditable set of other functions.  Just think about it: toguarantee strong safety everywhere, you would need to consider the possibility of *all function calls throwing*, andsafeguard the surrounding code accordingly.  That either means programming defensively, trusting another function'sdocumented English prose (that isn't being checked by a computer), getting lucky and only calling `noexcept` functions,or just hoping for the best.  Thanks to RAII, the leak-freedom aspect of basic safety is easier to attain -- and prettycommon these days thanks to smart pointers -- but even broken invariants are tricky to prevent.  The article[Exception Handling: A False Sense of Security](http://ptgmedia.pearsoncmg.com/images/020163371x/supplements/Exception_Handling_Article.html) sums this up well. --><p><a href="https://en.wikipedia.org/wiki/Exception_safety" target="_blank" rel="noopener">“异常安全性（Exception safety）”</a>是C++社区中经常讨论的一种实践方法。这种方法巧妙地从调用者关于故障，状态转换和内存管理的视角对函数的意图进行分类。函数被分类为四种类型之一：<em>“no-throw”</em>意味着向前执行的程度得到保证且不会出现异常；<em>“strong safety”</em>意味着状态转换以原子的方式发生，而故障不会导致部分提交的状态更改或不变量被破坏；<em>“basic safety”</em>意味着，虽然函数可能发生部分提交的状态更改，但不会破坏不变量并能防止内存泄漏；最后，<em>“no safety”</em>意味着一切皆有可能，无任何保障。这种分类法非常有用，我鼓励任何人对错误行为采取有意和严谨的态度，无论是使用这种方法还是其它类似的方法，即使是你使用的是错误码方式。</p><p>但问题是，除了叶节点数据结构调用一组小且易于审计的其他函数之外，在使用非检查性异常的系统中遵循这些指南基本上是不可能的。试想一下：为了保证各处的安全性，你需要考虑所有函数调用的可能性，并相应地对周围的代码进行保护。这意味者要么进行防御性编程（Defensive programming），要么信任所调用函数的（无法被计算机检查的）文档，要么只调用<code>noexcept</code>函数而走运，要么只是希望最好的情况出现。多亏有了RAII，免于泄露的基本安全性变得更容易实现（由于智能指针的出现，现在这些安全也变得很常见），但破坏不变量这一点却也很难避免。<a href="http://ptgmedia.pearsoncmg.com/images/020163371x/supplements/Exception_Handling_Article.html" target="_blank" rel="noopener">“Exception Handling: A False Sense of Security”</a>一文对此进行了很好地总结。</p><!-- For C++, the real solution is easy to predict, and rather straightforward: for robust systems programs, don't useexceptions.  That's the approach [Embedded C++](https://en.wikipedia.org/wiki/Embedded_C%2B%2B) takes, in addition tonumerous realtime and mission critical guidelines for C++, including NASA's Jet Propulsion Laboratory's.[C++ on Mars sure ain't using exceptions anytime soon](https://www.youtube.com/watch?v=3SdSKZFoUa8). --><p>对于C++而言，真正的解决方案很容易猜到，而且非常简单，那便是对于健壮的系统程序，不要使用异常处理。这是<a href="https://en.wikipedia.org/wiki/Embedded_C%2B%2B" target="_blank" rel="noopener">嵌入式C++</a>的常用做法，除此之外还有包括NASA喷气推进器实验室在内的C++的大量实时和任务关键指南也建议这么做。因此，<a href="https://www.youtube.com/watch?v=3SdSKZFoUa8" target="_blank" rel="noopener">火星上的C++代码肯定不会很快地使用上异常</a>。</p><!-- So if you can safely avoid exceptions and stick to C-like return codes in C++, what's the beef? --><p>所以如果你可以安全地避免使用异常并坚持使用C++中的类C的返回码方式，那么问题又出在何处呢？</p><!-- The entire C++ ecosystem uses exceptions.  To obey the above guidance, you must avoid significant parts of the languageand, it turns out, significant chunks of the library ecosystem.  Want to use the Standard Template Library?  Too bad, ituses exceptions.  Want to use Boost?  Too bad, it uses exceptions.  Your allocator likely throws `bad_alloc`.  And soon.  This even causes insanity like people creating forks of existing libraries that eradicates exceptions.  The Windowskernel, for instance, has its own fork of the STL that doesn't use exceptions.  This bifurcation of the ecosystem isneither pleasant nor practical to sustain. --><p>问题出在，整个C++生态系统都在使用异常。为了遵守上述指导原则，你必须避免使用该语言的相当一部分特性。但事实证明，这些重要特性是库生态系统的重要组成部分。想使用标准模板库？太糟糕了，它使用了异常，想使用Boost？太糟糕了，它也使用异常。你的内存分配器也可能会抛出<code>bad_alloc</code>异常等等。甚至会导致创建现有库无异常处理分支一样的神经错乱做法，例如，Windows内核有自己的STL分支，但它不使用异常处理，但这种生态系统的分叉既不愉快也不实用。</p><!-- This mess puts us in a bad spot.  Especially because many languages use unchecked exceptions.  It's clear that they areill-suited for writing low-level, reliable systems code.  (I'm sure I will make a few C++ enemies by saying this sobluntly.)  After writing code in Midori for years, it brings me tears to go back and write code that uses uncheckedexceptions; even simply code reviewing is torture.  But "thankfully" we have checked exceptions from Java to learn andborrow from ... Right? --><p>这种混乱让我们陷入了困境，特别是因为许多语言使用非检查性异常。很明显，它们不适合编写低层次的可靠的系统代码（我如此会直截了说出来，肯定会招惹几个来自C++的反对者）。在为Midori编写多年代码后，让我回去编写使用非检查性异常的代码，会让我欲哭无泪，即使仅仅对代码进行审查也是一种折磨。但“幸运的是”，我们已经有了来自Java的检查性异常用于学习和借鉴……不是吗？</p><!-- ### Checked Exceptions --><h3 id="检查性异常"><a href="#检查性异常" class="headerlink" title="检查性异常"></a>检查性异常</h3><!-- Ah, checked exceptions.  The rag doll that nearly every Java programmer, and every person who's observed Java from anarm's length distance, likes to beat on.  Unfairly so, in my opinion, when you compare it to the unchecked exceptionsmess. --><p>检查性异常就像，几乎每个Java程序员以及近距离观察过Java的人都喜欢拍打的布娃娃。在我看来，将它与非检查性异常的混乱进行比较是不公平的。</p><!-- In Java, you know *mostly* what a method might throw, because a method must say so: --><p>在Java中，因为方法必须进行如下申明，所以你知道<em>大多数</em>方法可能会抛出什么类型异常：</p><pre><code>void foo() throws FooException, BarException {    ...} </code></pre><!-- Now a caller knows that invoking `foo` could result in either `FooException` or `BarException` being thrown.  Atcallsites a programmer must now decide: 1) propagate thrown exceptions as-is, 2) catch and deal with them, or 3) somehowtransform the type of exception being thrown (possibly even "forgetting" the type altogether).  For instance: --><p>那么，现在调用者便知道调用<code>foo</code>可能导致抛出<code>FooException</code>或<code>BarException</code>类型的异常。因此在调用点中，程序员必须如下决定：1）按原样传播抛出的异常；2）捕获并处理它们；或者3）以某种方式转换抛出的异常类型（甚至可能是“完全遗忘掉”异常类型）。例如：</p><!--     // 1) Propagate exceptions as-is:    void bar() throws FooException, BarException {        foo();    }    // 2) Catch and deal with them:    void bar() {        try {            foo();        }        catch (FooException e) {            // Deal with the FooException error conditions.        }        catch (BarException e) {            // Deal with the BarException error conditions.        }    }    // 3) Transform the exception types being thrown:    void bar() throws Exception {        foo();    } --><pre><code>// 1) 按原样传播异常： void bar() throws FooException, BarException {    foo();}// 2) 捕捉并处理它们： void bar() {    try {        foo();    }    catch (FooException e) {        // 处理FooException的错误情况     }    catch (BarException e) {        // 处理BarException的错误情况     }}// 3) 转换抛出的异常类型： void bar() throws Exception {    foo();} </code></pre><!-- This is getting much closer to something we can use.  But it fails on a few accounts: --><p>这与我们可以使用的东西越来越接近，但它在某些情况下也会失效：</p><!-- 1. Exceptions are used to communicate unrecoverable bugs, like null dereferences, divide-by-zero, etc.2. You don't actually know *everything* that might be thrown, thanks to our little friend `RuntimeException`.  Because   Java uses exceptions for all error conditions -- even bugs, per above -- the designers realized people would go mad   with all those exception specifications.  And so they introduced a kind of exception that is unchecked.  That is, a   method can throw it without declaring it, and so callers can invoke it seamlessly.3. Although signatures declare exception types, there is no indication at callsites what calls might throw.4. People hate them. --><ol><li>异常用于传递不可恢复的错误，如null解引用，除零等；</li><li>由于<code>RuntimeException</code>的存在，实际上并不知道可能抛出的<em>所有</em>内容，因为Java对所有错误条件使用异常，甚至对程序中的bug也是如此。设计师也意识到人们会对所有这些异常规范感到厌烦，因此他们引入了一种非检查性异常。也就是说，一个方法可以在不声明的情况下抛出这种异常，使得调用者可以无缝地调用它。</li><li>虽然签名声明了异常类型，但在调用点没有迹象表明调用可能会抛出什么类型的异常；</li><li>人们讨厌这种方式。</li></ol><!-- That last one is interesting, and I shall return to it later when describing the approach Midori took.  In summary,peoples' distaste for checked exceptions in Java is largely derived from, or at least significantly reinforced by, theother three bullets above.  The resulting model seems to be the worst of both worlds.  It doesn't help you to writebulletproof code and it's hard to use.  You end up writing down a lot of gibberish in your code for little perceivedbenefit.  And versioning your interfaces is a pain in the ass.  As we'll see later, we can do better. --><p>最后一项是很有趣的，稍后我将在描述Midori所采用的方法时再回头来看看。总而言之，人们对Java中检查性异常的厌恶主要源于上面其他三项，或者至少在其他三项上得到了增强。由此产生的模型似乎是两种方式之间最糟糕的：它无法帮助你编写完全免于错误的代码，同时也难以实用。最终，你在代码中写下了很多莫名其妙并且几乎没有什么好处的语句。另外，对接口进行版本控制也是一件很痛苦的事。正如我们稍后将会看到的，我们本来可以做得更好。</p><!-- That versioning point is worth a ponder.  If you stick to a single kind of `throw`, then the versioning problem is noworse than error codes.  Either a function fails or it doesn't.  It's true that if you design version 1 of your API tohave no failure mode, and then want to add failures in version 2, you're screwed.  As you should be, in my opinion.  AnAPI's failure mode is a critical part of its design and contract with callers.  Just as you wouldn't change the returntype of an API silently without callers needing to know, you shouldn't change its failure mode in a semanticallymeaningful way.  More on this controversial point later on. --><p>该版本控制点是值得深思的，如果你坚持使用单一类型的<code>throw</code>，那么其版本控制问题并不比错误码更糟糕，函数要么失败或成功。确实，如果你将API的第一版设计为无故障的模式，然后想要在第二版中添加故障的抛出代码，那么事情就被你搞砸了，在我看来，这种情况可能就会发生。API的故障模式是其设计和与调用者之间的合约的关键部分，正如你不会在调用者未知的情况下静默更改API的返回类型一样，你也不应该以语义上有意义的方式更改其故障模式。稍后将对这个有争议的问题进行讨论。</p><!-- CLU has an interesting approach, as described in this crooked and wobbly scan of a 1979 paper by Barbara Liskov,[Exception Handling in CLU](http://csg.csail.mit.edu/pubs/memos/Memo-155/Memo-155-3.pdf).  Notice that they focus a loton "linguistics"; in other words, they wanted a language that people would love.  The need to check and repropagate allerrors at callsites felt a lot more like return values, yet the programming model had that richer and slightlydeclarative feel of what we now know as exceptions.  And most importantly, `signal`s (their name for `throw`) werechecked.  There were also convenient ways to terminate the program should an unexpected `signal` occur. --><p>在CLU中有一种有趣的方法，正如Barbara Liskov在1979年的<a href="http://csg.csail.mit.edu/pubs/memos/Memo-155/Memo-155-3.pdf" target="_blank" rel="noopener">“Exception Handling in CLU”</a>一文中所描述的那样。我注意到他们非常关注于“语言学”，换句话说，他们想要一种人们喜欢的编程语言：在callites上检查和重新传播所有错误的感觉更像是返回值，但编程模型对我们现在所知的异常有更丰富和略微声明式的感觉。最重要的是，<code>signal</code>（它们现在的名字式<code>throw</code>）是检查性的，并且如果发生意外<code>signal</code>，还有方便的方法来终止整个程序。</p><!-- ### Universal Problems with Exceptions --><h3 id="异常的中普遍存在的问题"><a href="#异常的中普遍存在的问题" class="headerlink" title="异常的中普遍存在的问题"></a>异常的中普遍存在的问题</h3><!-- Most exception systems get a few major things wrong, regardless of whether they are checked or unchecked. --><p>大多数的异常系统，无论是检查性的还是非检查性的，都会出现一些普遍的问题。</p><!-- First, throwing an exception is usually ridiculously expensive.  This is almost always due to the gathering of a stacktrace.  In managed systems, gathering a stack trace also requires groveling metadata, to create strings of functionsymbol names.  If the error is caught and handled, however, you don't even need that information at runtime!Diagnostics are better implemented in the logging and diagnostics infrastructure, not the exception system itself.  Theconcerns are orthogonal.  Although, to really nail the diagnostics requirement above, *something* needs to be able torecover stack traces; never underestimate the power of `printf` debugging and how important stack traces are to it. --><p>首先，抛出异常开销通常非常大，这基本上总是由收集堆栈跟踪（stack trace）所引起的。在托管系统中，收集堆栈跟踪还需要对元数据进行搜索，以创建函数符号名称的字符串，但是，如果错误得以捕获并处理，你甚至不需要在运行时获取这些信息；诊断能够在日志记录和诊断基础设施中更好地被实现，而不是在异常系统本身中，而上述这些关心点又是正交的。但为了真正取得上述的诊断要求，<em>某些系统</em>需要能够恢复堆栈跟踪；永远不要低估<code>printf</code>调试的强大能力以及堆栈跟踪对此的重要性。</p><!-- Next, exceptions can significantly impair code quality.  I touched on this topic [in my last post](http://joeduffyblog.com/2015/12/19/safe-native-code/), and there are [good papers on the topic in the context of C++](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.116.8337&rep=rep1&type=pdf).  Not having static type systeminformation makes it hard to model control flow in the compiler, which leads to overly conservative optimizers. --><p>其次，异常会严重影响代码质量。我在<a href="/2019/02/17/midori/4-safe-native-code/">上一篇文章</a>中谈到了这个主题，并且有<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.116.8337&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">C++环境中关于该主题很好的论文</a>。静态类型系统信息的缺乏使得很难在编译器中对控制流进行建模，从而导致优化器变得过于保守。</p><!-- Another thing most exception systems get wrong is encouraging too coarse a granularity of handling errors.  Proponentsof return codes love that error handling is localized to a specific function call.  (I do too!)  In exception handlingsystems, it's all too easy to slap a coarse-grained `try`/`catch` block around some huge hunk of code, without carefullyreacting to individual failures.  That produces brittle code that's almost certainly wrong; if not today, then after theinevitable refactoring that will occur down the road.  A lot of this has to do with having the right syntaxes. --><p>大多数异常系统出问题的另一个方面是鼓励对错误进行粗粒度的处理。返回码的支持者喜欢将错误处理本地化为特定的函数调用（，而我也是这样做的）。但在异常处理系统中，很容易在一些巨大的代码块周围加上粗粒度的<code>try</code>/<code>catch</code>块，而不会仔细地对单个故障做出反应。这种方式产生的脆弱代码几乎肯定是会出错的。如果不是今天，那么沿着这条路走，不可避免的重构必将会发生，因此，这与拥有正确的语法有很大程度的关系。</p><!-- Finally, control flow for `throw`s is usually invisible.  Even with Java, where you annotate method signatures, it's notpossible to audit a body of code and see precisely where exceptions come from.  Silent control flow is just as bad as`goto`, or `setjmp`/`longjmp`, and makes writing reliable code very difficult. --><p>最后，<code>throw</code>的控制流通常是不可见的，即使是使用Java的注解方法签名的方式，也无法对代码体进行审计并准确查看异常的来源。静默的控制流与<code>goto</code>或<code>setjmp</code>/<code>longjmp</code>一样糟糕，并且使编写可靠代码变得非常困难。</p><!-- ## Where Are We? --><h2 id="我们身在何处？"><a href="#我们身在何处？" class="headerlink" title="我们身在何处？"></a>我们身在何处？</h2><!-- Before moving on, let's recap where we are: --><p>在继续前行之前，让我们回顾一下我们所处的位置：</p><!-- ![Good-Bad-Ugly](/assets/img/2016-02-07-the-error-model-1.png) --><img src="/2019/03/09/midori/5-the-error-model/2016-02-07-the-error-model-1.png" title="“好家伙，坏家伙，丑家伙”"><!-- Wouldn't it be great if we could take all of The Goods and leave out The Bads and The Uglies? --><p>如果我们可以带走所有的“Goods（好家伙）”，并舍弃“Bads（坏家伙）”和“Uglies（丑家伙）”，那不是一件很好的事吗？</p><!-- This alone would be a great step forward.  But it's insufficient.  This leads me to our first big "ah-hah" moment thatshaped everything to come.  For a significant class of error, *none* of these approaches are appropriate! --><p>仅次一点就是向前迈出的一大步，但这还远远不够，这让我想起了我们首个影响未来一切的“欢呼”时刻。对于一大类的错误，表中的所有这些方法都<em>不</em>合适！</p><!-- # Bugs Aren't Recoverable Errors! --><h1 id="Bug不是可恢复的错误！"><a href="#Bug不是可恢复的错误！" class="headerlink" title="Bug不是可恢复的错误！"></a>Bug不是可恢复的错误！</h1><!-- A critical distinction we made early on is the difference between recoverable errors and bugs: --><p>我们早期做出的一个重要区分，是可恢复错误和bug之间的区别：</p><!-- * A *recoverable error* is usually the result of programmatic data validation.  Some code has examined the state of the  world and deemed the situation unacceptable for progress.  Maybe it's some markup text being parsed, user input from a  website, or a transient network connection failure.  In these cases, programs are expected to recover.  The developer  who wrote this code must think about what to do in the event of failure because it will happen in well-constructed  programs no matter what you do.  The response might be to communicate the situation to an end-user, retry, or abandon  the operation entirely, however it is a *predictable* and, frequently, *planned* situation, despite being called an  "error." --><ul><li><em>可恢复的错误</em>通常是程序化的数据验证的结果，一些代码对上下文的状态进行了检查，并认为这种状态对于继续执行是不可接受的。也许是正在解析的标记文本、来自网站的用户输入或易失网络的连接故障等，在这些情况下，程序期望能够正常恢复。编写此代码的开发者必须考虑在发生故障时该怎么做，因为无论你怎么编写代码，这些情况都会在构造良好的程序中发生。响应的方式可能是将情况传达给终端用户、重试或完全放弃的操作，但这是一种<em>可预测的</em>，经常是<em>计划内的</em>情况，尽管它被称为“错误”。</li></ul><!-- * A *bug* is a kind of error the programmer didn't expect.  Inputs weren't validated correctly, logic was written wrong,  or any host of problems have arisen.  Such problems often aren't even detected promptly; it takes a while until  "secondary effects" are observed indirectly, at which point significant damage to the program's state might have  occurred.  Because the developer didn't expect this to happen, all bets are off.  All data structures reachable by  this code are now suspect.  And because these problems aren't necessarily detected promptly, in fact, a whole lot  more is suspect.  Depending on the isolation guarantees of your language, perhaps the entire process is tainted. --><ul><li><em>bug</em>是程序员没有意料到的一种错误，这包括输入未正确验证、逻辑的编码错误或出现任何问题。这些问题通常甚至不能被及时发现，它的“次要影响”需要一段时间才能间接地观察到，而此时可能会对程序的状态造成重大破坏。因为开发者没料到会发生这种情况，所以一切都结束了。此代码可访问的所有数据结构都是被怀疑的对象，而且因为这些问题不一定能及时被发现，事实上，更多的部分将是不可信的。根据语言的隔离保证，可能使得整个进程都受到了污染。</li></ul><!-- This distinction is paramount.  Surprisingly, most systems don't make one, at least not in a principled way!  As we sawabove, Java, C#, and dynamic languages just use exceptions for everything; and C and Go use return codes.  C++ uses amixture depending on the audience, but the usual story is a project picks a single one and uses it everywhere.  Youusually don't hear of languages suggesting *two* different techniques for error handling, however. --><p>这种区分至关重要。但令人惊讶的是，大多数系统都没有做到这一点，至少不是以一种原则性的方式进行区分！如上所述，Java，C#和动态语言仅使用异常，而C和Go仅使用返回码。 C++根据受众的不用使用两者混合的方式，但通常的情况是项目选择其中之一，并在代码的任何地方都这样使用。但你通常不会听到某种语言建议使用<em>这两种</em>不同的错误处理技术。</p><!-- Given that bugs are inherently not recoverable, we made no attempt to try.  All bugs detected at runtime causedsomething called *abandonment*, which was Midori's term for something otherwise known as ["fail-fast"](https://en.wikipedia.org/wiki/Fail-fast). --><p>鉴于bug本质上是不可恢复的，我们没有试图对其进行<code>try</code>捕捉。在运行时检测到的所有bug所导致的后果，在Midori的术语中被称为<em>Abandonment（放弃）</em>，也就是所谓的<a href="https://en.wikipedia.org/wiki/Fail-fast" target="_blank" rel="noopener">“快速失败”</a>。</p><!-- Each of the above systems offers abandonment-like mechanisms.  C# has `Environment.FailFast`; C++ has `std::terminate`;Go has `panic`; Rust has `panic!`; and so on.  Each rips down the surrounding context abruptly and promptly.  The scopeof this context depends on the system -- for example, C# and C++ terminate the process, Go the current Goroutine, andRust the current thread, optionally with a panic handler attached to salvage the process.--><p>上述每种语言都提供类似Abandonment的机制：C#有<code>Environment.FailFast</code>，C++有<code>std::terminate</code>，Go有<code>panic</code>，以及Rust有<code>panic!</code>等等。每一方式都突然且迅速地销毁掉上下文环境，而此上下文的范围则取决于系统，例如，对于C#和C++来说是终止进程，对于Go来说是Goroutine，而对于Rust来说则是当前线程，并可选择地附加一个panic处理程序来对进程进行抢救。</p><!--Although we did use abandonment in a more disciplined and ubiquitous way than is common, we certainly weren't the firstto recognize this pattern.  This [Haskell essay](https://wiki.haskell.org/Error_vs._Exception), articulates thisdistinction quite well: --><p>虽然我们确实以比一般的更有纪律和无处不在的方式使用Abandonment，但我们当然不是第一个认识到这种模式的团队。这篇<a href="https://wiki.haskell.org/Error_vs._Exception" target="_blank" rel="noopener">Haskell文章</a>非常清楚地表达了这种区别：</p><!-- > I was involved in the development of a library that was written in C++.  One of the developers told me that the> developers are divided into the ones who like exceptions and the other ones who prefer return codes. As it seem to me,> the friends of return codes won.  However, I got the impression that **they debated the wrong point: Exceptions and> return codes are equally expressive**, they should however not be used to describe errors. Actually the return codes> contained definitions like `ARRAY_INDEX_OUT_OF_RANGE`.  But I wondered: How shall my function react, when it gets this> return code from a subroutine?  Shall it send a mail to its programmer?  It could return this code to its caller in> turn, but it will also not know how to cope with it.  Even worse, since I cannot make assumptions about the> implementation of a function, I have to expect an `ARRAY_INDEX_OUT_OF_RANGE` from every subroutine. My conclusion is> that `ARRAY_INDEX_OUT_OF_RANGE` is a (programming) error.  **It cannot be handled or fixed at runtime, it can only be> fixed by its developer. Thus there should be no according return code, but instead there should be asserts.** --><blockquote><p>我参与了用C++编写的库的开发。其中一位开发者告诉我，开发者可以分为喜欢异常和喜欢返回码两种类型。在我看来，使用返回码的朋友赢了。但是，我得到的印象是<strong>他们论点是错误的：异常和返回代同有同样的表达能力</strong>，但这并不适用于描述bug。实际上，返回码包含<code>ARRAY_INDEX_OUT_OF_RANGE</code>等定义，但我想知道：当程序从子程序获得该返回码时，它将如何对其作出反应？它应该向程序员发送邮件告知吗？它可以依次将此错误码返回给其调用者，但它们其实也都不知道如何对其进行处理。更糟糕的是，由于我不能对函数的实现做出假设，所以不得不认为每个子程序都有可能返回<code>ARRAY_INDEX_OUT_OF_RANGE</code>。我的结论是<code>ARRAY_INDEX_OUT_OF_RANGE</code>是一个（编程性）的错误，<strong>无法在运行时对其进行处理或修复，只能由其开发者来修复。因此，这不应该有这样的返回码存在，而是应该采用断言的方式</strong>。</p></blockquote><!-- Abandoning fine grained mutable shared memory scopes is suspect -- like Goroutines or threads or whatever -- unless yoursystem somehow makes guarantees about the scope of the potential damage done.  However, it's great that these mechanismsare there for us to use!  It means using an abandonment discipline in these languages is indeed possible. --><p>放弃细粒度的可变共享内存作用域是不可信的，比如Goroutine、线程或其他方式，除非你的系统以某种方式保证潜在的内存破坏范围。但是，很不错的是这些机制是存在的并可以被我们所利用！这也就意味着在这些语言中使用Abandonment机制确实是有可能的。</p><!-- There are architectural elements necessary for this approach to succeed at scale, however.  I'm sure you're thinking"If I tossed the entire process each time I had a null dereference in my C# program, I'd have some pretty pissed offcustomers"; and, similarly, "That wouldn't be reliable at all!"  Reliability, it turns out, might not be what you think. --><p>然而，它还必须具有大规模成功所必需的架构元素。我相信你在想“如果我每次在我的C#程序中进行null解引用时都会抛出整个进程，那么我的一些客户会非常的生气”，和“这根本不可靠！”等相似的想法。事实证明，可靠性可能与你的想法有所不同。</p><!-- # Reliability, Fault-Tolerance, and Isolation --><h1 id="可靠性，容错和隔离"><a href="#可靠性，容错和隔离" class="headerlink" title="可靠性，容错和隔离"></a>可靠性，容错和隔离</h1><!-- Before we get any further, we need to state a central belief: <s>Shi</s> Failure Happens. --><p>在我们进一步讨论之前，我们需要先表达中心信念：故障会发生。</p><!-- ## To Build a Reliable System --><h2 id="构建可靠的系统"><a href="#构建可靠的系统" class="headerlink" title="构建可靠的系统"></a>构建可靠的系统</h2><!-- Common wisdom is that you build a reliable system by systematically guaranteeing that failure can never happen.Intuitively, that makes a lot of sense.  There's one problem: in the limit, it's impossible.  If you can spend millionsof dollars on this property alone -- like many mission critical, realtime systems do -- then you can make a significantdent.  And perhaps use a language like [SPARK](https://en.wikipedia.org/wiki/SPARK_(programming_language)) (a set ofcontract-based extensions to Ada) to formally prove the correctness of each line written.  However, [experience shows](https://en.wikipedia.org/wiki/List_of_software_bugs) that even this approach is not foolproof. --><p>这里的常识是通过系统地保证故障永不发生的方式，来构建一个可靠的系统。直观上来说，这是很有道理的，但有一个问题：在极限的情况下，这是不可能的。如果你可以单独花费数百万美元来获得这样的可靠性，就像许多任务关键的实时系统一样，那么会给你留下深刻的印象。也许使用像<a href="http://t.cn/EMGjc0l" target="_blank" rel="noopener">SPARK</a>这样的语言（一组基于合约的Ada扩展）来形式化证明每行代码的正确性。然而，<a href="https://en.wikipedia.org/wiki/List_of_software_bugs" target="_blank" rel="noopener">经验表明</a>即使这种方法也不是万无一失的。</p><!-- Rather than fighting this fact of life, we embraced it.  Obviously you try to eliminate failures where possible.  Theerror model must make them transparent and easy to deal with.  But more importantly, you architect your system so thatthe whole remains functional even when individual pieces fail, and then teach your system to recover those failingpieces gracefully.  This is well known in distributed systems.  So why is it novel? --><p>我们接受而不是反对这一事实。但如果显然试图在所有可能的情况下消除失败，那么错误模型必须使它们透明且易于处理。更重要的是，系统被构建为即使单个部件出现故障，整个系统仍然可以正常运行，然后再指导系统优雅地恢复那些故障部分。这种原则在分布式系统中是众所周知的，那为什么它又是新颖的呢？</p><!-- At the center of it all, an operating system is just a distributed network of cooperating processes, much like adistributed cluster of microservices or the Internet itself.  The main differences include things like latency; whatlevels of trust you can establish and how easily; and various assumptions about locations, identity, etc.  But failurein [highly asynchronous, distributed, and I/O intensive systems](http://joeduffyblog.com/2015/11/19/asynchronous-everything/) is just bound to happen.  My impression is that, largelybecause of the continued success of monolithic kernels, the world at large hasn't yet made the leap to "operating systemas a distributed system" insight.  Once you do, however, a lot of design principles become apparent. --><p>处于这一切的中心的操作系统，只是协作进程们的分布式网络，就像微服务的分布式集群或互联网本身，它与这些系统的主要区别在于延迟，和可以建立什么样的信任程度以及达到的难度如何，以及关于位置，身份的各种假设等。但<a href="/2018/11/25/midori/3-asynchronous-everything/">高度异步，分布式和I/O密集型系统</a>中的故障必然会发生。对此，我的看法是，很大程度上是因为宏内核的持续成功，使得整个世界还没有跃升到“作为分布式系统的操作系统”的洞察力。但是，一旦你这么做了，很多的设计原则就会变得明显起来。</p><!-- As with most distributed systems, our architecture assumed process failure was inevitable.  We went to greatlength to defend against cascading failures, journal regularly, and to enable restartability of programs and services. --><p>与大多数分布式系统一样，我们的架构假设进程失败是不可避免的，尽管我们花了很长时间来防止级联故障发生，定期日志记录，以及实现程序和服务的可重启性。</p><!-- You build things differently when you go in assuming this. --><p>当你如此假设时，便会以不同的方式来构建整个系统。</p><!-- In particular, isolation is critical.  Midori's process model encouraged lightweight fine-grained isolation.  As aresult, programs and what would ordinarily be "threads" in modern operating systems were independent isolated entities.Safeguarding against failure of one such connection is far easier than when sharing mutable state in an address space. --><p>特别地，隔离至关重要，Midori的进程模型有助于轻量级细粒度的隔离，因此，程序和现代操作系统中通常称为“线程”的构造是独立的孤立实体。对一个这样的系统中的网络连接失败进行保护比在地址空间中的共享可变状态下要容易得多。</p><!-- Isolation also encourages simplicity.  Butler Lampson's classic [Hints on Computer System Design](http://research.microsoft.com/pubs/68221/acrobat.pdf) explores this topic.  And I always loved this quote from Hoare: --><p>隔离同样也有助于简单性，Butler Lampson的经典文章<a href="http://research.microsoft.com/pubs/68221/acrobat.pdf" target="_blank" rel="noopener">“Hints on Computer System Design”</a>探索了这个主题。我一直很喜欢Hoare的这句话：</p><!-- > The unavoidable price of reliability is simplicity. (C. Hoare). --><blockquote><p>可靠性的无法避免的代价是简单性（C. Hoare）。</p></blockquote><!-- By keeping programs broken into smaller pieces, each of which can fail or succeed on its own, the state machines withinthem stay simpler.  As a result, recovering from failure is easier.  In our language, the points of possible failurewere explicit, further helping to keep those internal state machines correct, and pointing out those connections withthe messier outside world.  In this world, the price of individual failure is not nearly as dire.  I can'tover-emphasize this point.  None of the language features I describe later would have worked so well without thisarchitectural foundation of cheap and ever-present isolation. --><p>通过将程序分解成更小的部分，每个部分都可以自行失败或成功，使得其中的每个状态机都能保持为更简单的形式，因此使得从故障中恢复变得更加容易。在我们的语言中，可能的失败点是明确的，从而有利于进一步保持这些内部状态机的正确性，并指明了与混乱的外部世界的接口。在这个世界上，单个个体失败的代价并不是那么可怕，所以我不能过分强调这一点。如果没有廉价和永远在线的隔离的架构基础，我后面描述的语言功能都不会很好的工作。</p><!-- Erlang has been very successful at building this property into the language in a fundamental way.  It, like Midori,leverages lightweight processes connected by message passing, and encourages fault-tolerant architectures.  A commonpattern is the "supervisor," where some processes are responsible for watching and, in the event of failure, restartingother processes.  [This article](http://ferd.ca/the-zen-of-erlang.html) does a terrific job articulating this philosophy-- "let it crash" -- and recommended techniques for architecting reliable Erlang programs in practice. --><p>Erlang非常成功地以一种基础的方式将这样的属性构建到语言中。它与Midori一样，利用通过消息传递连接的轻量级进程并鼓励容错架构等措施来实现。它采取的一种常见的模式是“监视器”模式：其中一些进程负责监测环境，并在其他进程发生故障时重启这些进程。<a href="http://ferd.ca/the-zen-of-erlang.html" target="_blank" rel="noopener">这篇文章</a>在明确“让它崩溃（let it crash）”的哲学理念，和关于在实践中构建可靠的Erlang程序的推荐技术上，做了很不错的工作。</p><!-- The key thing, then, is not preventing failure per se, but rather knowing how and when to deal with it. --><p>因此，关键不在于要防止失败本身，而是要知道如何以及何时处理它们。</p><!-- Once you've established this architecture, you beat the hell out of it to make sure it works.  For us, this meantweek-long stress runs, where processes would come and go, some due to failures, to ensure the system as a whole keptmaking good forward progress.  This reminds me of systems like Netflix's [Chaos Monkey](https://github.com/Netflix/SimianArmy/wiki/Chaos-Monkey) which just randomly kills entire machines in your cluster toensure the service as a whole stays healthy. --><p>一旦你构建了这种架构，你就会战胜它们并确保系统的运行。对我们来说，这意味着为期一周的压力运行：进程不断启动和中止，有些是由故障造成的，并同时确保整个系统良好地前进。这让我想起了像Netflix的<a href="https://github.com/Netflix/SimianArmy/wiki/Chaos-Monkey" target="_blank" rel="noopener">Chaos Monkey</a>这样的系统，它会随机终止集群中的某些机器的运行，以确保整个服务保持健康状态。</p><!-- I expect more of the world to adopt this philosophy as the shift to more distributed computing happens.  In a cluster ofmicroservices, for example, the failure of a single container is often handled seamlessly by the enclosing clustermanagement software (Kubernetes, Amazon EC2 Container Service, Docker Swarm, etc).  As a result, what I describe in thispost is possibly helpful for writing more reliable Java, Node.js/JavaScript, Python, and even Ruby services.  Theunfortunate news is you're likely going to be fighting your languages to get there.  A lot of code in your process isgoing to work real damn hard to keep limping along when something goes awry. --><p>随着更多的向分布式计算的转变不断发生，我期望更多的系统采用这种理念。例如，在微服务集群中，单个容器的故障通常由封闭的集群管理软件（如Kubernetes，Amazon EC2 Container Service和Docker Swarm等）无缝地处理。因此，我在本文中描述的内容可能有助于编写更可靠的Java，Node.js/JavaScript，Python甚至是Ruby服务。不幸的是，你很可能会与所使用的语言不断抗争来实现这样的目标，因为很多进程的代码在出现问题时也会万分努力地艰难执行。</p><h2 id="Abandonment"><a href="#Abandonment" class="headerlink" title="Abandonment"></a>Abandonment</h2><!-- Even when processes are cheap and isolated and easy to recreate, it's still reasonable to think that abandoning anentire process in the face of a bug is an overreaction.  Let me try to convince you otherwise. --><p>即使进程是轻量级、隔离且易于重新创建的，你仍然有理由认为在面对错误时放弃整个进程是一种过度的反应。那么下面让我试着来说服你。</p><!-- Proceeding in the face of a bug is dangerous when you're trying to build a robust system.  If a programmer didn't expecta given situation that's arisen, who knows whether the code will do the right thing anymore.  Critical data structuresmay have been left behind in an incorrect state.  As an extreme (and possibly slightly silly) example, a routine that ismeant to round your numbers *down* for banking purposes might start rounding them *up*. --><p>当你尝试构建一个健壮的系统时，在运行过程中遇到bug是危险的。如果程序员没有预料到会出现特定的情况，没人知道代码下次是否还会再做正确的事情。关键的数据结构可能在不正确的状态下被丢弃，一个极端（也可能是稍微有点愚蠢）的例子是，一个用于银行业务的例程，其本来目的是将你的存款数字向<em>下</em>舍入现在则可能变成了向<em>上</em>舍入。</p><!-- And you might be tempted to whittle down the granularity of abandonment to something smaller than a process.  But that'stricky.  To take an example, imagine a thread in your process encounters a bug, and fails.  This bug might have beentriggered by some state stored in a static variable.  Even though some other thread might *appear* to have beenunaffected by the conditions leading to failure, you cannot make this conclusion.  Unless some property of your system-- isolation in your language, isolation of the object root-sets exposed to independent threads, or something else --it's safest to assume that anything other than tossing the entire address space out the window is risky and unreliable. --><p>你可能会试图将Abandonment的粒度减少到比进程更小的范围里，但这种方式是很棘手的。举一个例子来说，假设你的进程中的一个线程遇到了一个bug，并且执行失败，而且这个bug可能是由存储在静态变量中的某些状态所触发的。即使其他线程似乎<em>看起来</em>不受导致故障的条件的影响，但你也无法对此确定。除非你的系统有一些属性，被你的语言隔离，隔离暴露给独立线程或其他地方的对象根集合，那么最安全的做法是，假设除了把整个地址空间全部销毁之外的任何操作都是有风险和不可靠的。</p><!-- Thanks to the lightweight nature of Midori processes, abandoning a process was more like abandoning a single thread in aclassical system than a whole process.  But our isolation model let us do this reliably. --><p>由于Midori进程的轻量级特性，放弃进程更像是在经典操作系统中放弃单个线程而不是整个进程。但我们的隔离模型让我们可靠地做到了这一点。</p><!-- I'll admit the scoping topic is a slippery slope.  Maybe all the data in the world has become corrupt, so how do youknow that tossing the process is even enough?!  There is an important distinction here.  Process state is transient bydesign.  In a well designed system it can be thrown away and recreated on a whim.  It's true that a bug can corruptpersistent state, but then you have a bigger problem on your hands -- a problem that must be dealt with differently. --><p>我承认作用域主题是个滑坡谬误：也许环境中所有的数据都已经被破坏了，那么你怎么知道中止掉这个进程就足够了？！这里有一个重要的区别，也就是进程的状态在设计上是瞬态的。在一个设计良好的系统中，它可以被丢弃并随意地被重建。没错，一个bug会对持久性状态造成破坏，但是你手头上有一个更大的麻烦，这个麻烦必须以不同的方式进行处理。</p><!-- For some background, we can look to fault-tolerant systems design.  Abandonment (fail-fast) is already a commontechnique in that realm, and we can apply much of what we know about these systems to ordinary programs and processes.Perhaps the most important technique is regularly journaling and checkpointing precious persistent state.  Jim Gray's1985 paper, [Why Do Computers Stop and What Can Be Done About It?](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.110.9127&rep=rep1&type=pdf), describes this concept nicely.As programs continue moving to the cloud, and become aggressively decomposed into smaller independent services, thisclear separation of transient and persistent state is even more important.  As a result of these shifts in how softwareis written, abandonment is far more achievable in modern architectures than it once was.  Indeed, abandonment can helpyou avoid data corruption, because bugs detected before the next checkpoint prevent bad state from ever escaping. --><p>对于某些背景，我们可以考虑容错的系统设计。Abandonment（快速失败）已经是该领域的常用技术，我们可以将我们对这些系统的大部分知识应用于普通程序和进程，也许最重要的技术是定期记录和对宝贵的持久性状态做快照。Jim Gray在1985年的论文，<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.110.9127&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">“Why Do Computers Stop and What Can Be Done About It?”</a>，很好地描述了这个概念。随着程序持续地向云平台迁移，并且激进地分解为更小的独立服务的趋势，瞬态和持久状态的这种明确区分甚至变得更为重要。由于这种软件编写方式的转变，在现代架构中Abandonment比以前更容易实现。实际上，放弃可以帮助你避免数据被损坏，因为在下一个快照之前检测到的bug可以防止错误状态的逸出。</p><!-- Bugs in Midori's kernel were handled differently.  A bug in the microkernel, for instance, is an entirely differentbeast than a bug in a user-mode process.  The scope of possible damage was greater, and the safest response was toabandon an entire "domain" (address space).  Thankfully, most of what you'd think of being classic "kernel"functionality -- the scheduler, memory manager, filesystem, networking stack, and even device drivers -- was runinstead in isolated processes in user-mode where failures could be contained in the usual ways described above. --><p>Midori内核中的bug处理方式也有所不同，因为微内核中的bug与用户进程中的bug就完全不同。它可能造成的损害范围更大，因此最安全的反应是放弃整个“域”（地址空间）。值得庆幸的是，大多数你认为经典的“内核”功能——调度器、内存管理器、文件系统、网络堆栈甚至是设备驱动程序，都是在用户模式以隔离进程的方式运行，所以故障以如上所述的通常方式被限定在隔离的进程中。</p><!-- # Bugs: Abandonment, Assertions, and Contracts --><h1 id="Bug：Abandonment，断言和合约"><a href="#Bug：Abandonment，断言和合约" class="headerlink" title="Bug：Abandonment，断言和合约"></a>Bug：Abandonment，断言和合约</h1><!-- A number of kinds of bugs in Midori might trigger abandonment: --><p>Midori中如下的一些bug可能会导致Abandonment：</p><!-- * An incorrect cast.* An attempt to dereference a `null` pointer.* An attempt to access an array outside of its bounds.* Divide-by-zero.* An unintended mathematical over/underflow.* Out-of-memory.* Stack overflow.* Explicit abandonment.* Contract failures.* Assertion failures. --><ul><li>不正确的强制类型转换；</li><li>试图解引用<code>null</code>指针；</li><li>试图越界访问数组；</li><li>除零；</li><li>意外的数学上/下溢；</li><li>内存不足；</li><li>栈溢出；</li><li>显式的放弃；</li><li>合约失败；</li><li>断言失败。</li></ul><!-- Our fundamental belief was that each is a condition the program cannot recover from.  Let's discuss each one. --><p>我们的基本理念是：以上每种都是程序无法自动恢复的条件，让我们来依次讨论其中每一个。</p><!-- ## Plain Old Bugs --><h2 id="普通的旧Bug类型"><a href="#普通的旧Bug类型" class="headerlink" title="普通的旧Bug类型"></a>普通的旧Bug类型</h2><!-- Some of these situations are unquestionably indicative of a program bug. --><p>一些情形毫无疑问表明程序存在bug。</p><!-- An incorrect cast, attempt to dereference `null`, array out-of-bounds access, or divide-by-zero are clearly problemswith the program's logic, in that it attempted an undeniably illegal operation.  As we will see later, there are waysout (e.g., perhaps you want NaN-style propagation for DbZ).  But by default we assume it's a bug. --><p>不正确的强制转换，试图对<code>null</code>指针解引用，数组越界访问或除零显然是程序逻辑的问题，因为它试图进行无法否认的非法操作。正如我们稍后将看到的，有一些是可以解决的（例如对于除零操作而言，你可能想使用NaN风格的传播），但默认情况下我们认为这是一个bug。</p><!-- Most programmers were willing to accept this without question.   And dealing with them as bugs this way broughtabandonment to the inner development loop where bugs during development could be found and fixed fast.  Abandonmentreally did help to make people more productive at writing code.  This was a surprise to me at first, but it makes sense. --><p>大多数的程序员都毫无疑问地愿意接受这一点，并且将它们以这种方式作为bug处理可将Abandonment带到内部开发循环中，从而有助于快速找到并修复开发过程中的bug。Abandonment确实有助于提高人们编写代码的效率，起初这对我来说是一个意外惊喜，但它确实是有道理的。</p><!-- Some of these situations, on the other hand, are subjective.  We had to make a decision about the default behavior,often with controversy, and sometimes offer programmatic control. --><p>另一方面，其他的一些情况则是比较主观的。我们必须对这些情况的默认行为做出决定，这通常会引起争议，所以有时还需提供对程序的控制。</p><!-- ### Arithmetic Over/Underflow --><h3 id="算术上-下溢出"><a href="#算术上-下溢出" class="headerlink" title="算术上/下溢出"></a>算术上/下溢出</h3><!-- Saying an unintended arithmetic over/underflow represents a bug is certainly a contentious stance.  In an unsafe system,however, such things frequently lead to security vulnerabilities.  I encourage you to review the National VulnerabilityDatabase to see [the sheer number of these](https://web.nvd.nist.gov/view/vuln/search-results?query=%22integer+overflow%22&search_type=all&cves=on). --><p>如果说意外的算术上/下溢出是一种bug，这肯定是有争议的说法。然而，在不安全的系统中，这种情况经常导致安全漏洞，对此，我建议你打开国家漏洞数据库来看看<a href="https://web.nvd.nist.gov/view/vuln/search-results?query=%22integer+overflow%22&amp;search_type=all&amp;cves=on" target="_blank" rel="noopener">这种类型漏洞的绝对数量</a>。</p><!-- In fact, the Windows TrueType Font parser, which we ported to Midori (with *gains* in performance), has suffered over adozen of them in the past few years alone.  (Parsers tend to be farms for security holes like this.) --><p>事实上，我们移植到Midori（并获得性能<em>提升</em>）的Windows TrueType字体解析器，仅在过去几年就遭受了十几次的上/下溢出（解析器往往是像这样的安全漏洞的发生地）。</p><!-- This has given rise to packages like [SafeInt](https://safeint.codeplex.com/), whichessentially moves you away from your native language's arithmetic operations, in favor of checked library ones. --><p>这就产生了像<a href="https://safeint.codeplex.com/" target="_blank" rel="noopener">SafeInt</a>这样的软件包，它基本上使你远离了原生的算术运算，转而使用检查性的库来实现。</p><!-- Most of these exploits are of course also coupled with an access to unsafe memory.  You could reasonably argue thereforethat overflows are innocuous in a safe language and therefore should be permitted.  It's pretty clear, however, based onthe security experience, that a program often does the wrong thing in the face of an unintended over/underflow.  Simplyput, developers frequently overlook the possibility, and the program proceeds to do unplanned things.  That's thedefinition of a bug which is precisely what abandonment is meant to catch.  The final nail in the coffin on this one isthat philisophically, when there was any question about correctness, we tended to err on the side of explicit intent.--><p>这些漏洞中的大多数当然还伴随着对不安全内存的访问，因此，你可以有理由的认为，溢出在安全语言中是无害的，所以应该允许出现。然而，基于安全上的经验，很明显的是，程序在面临意外的上/下溢时经常会做错事。简单地说，开发者经常忽略溢出的可能性，使得程序继续执行计划之外的事。而这恰好是Abandonment试图捕捉的bug所定义的内容。关于这一点的最致命一击是哲学上的，当有任何关于正确性上的问题时，我们倾向于在明确的意图方面犯错误。</p><!-- Hence, all unannotated over/underflows were considered bugs and led to abandonment.  This was similar to compilingC# with [the `/checked` switch](https://msdn.microsoft.com/en-us/library/h25wtyxf.aspx), except that our compileraggressively optimized redundant checks away.  (Since few people ever think to throw this switch in C#, thecode-generators don't do nearly as aggressive a job in removing the inserted checks.)  Thanks to this language andcompiler co-development, the result was far better than what most C++ compilers will produce in the face of SafeIntarithmetic.  Also as with C#, [the `unchecked` scoping construct](https://msdn.microsoft.com/en-us/library/khy08726.aspx) could be used where over/underflow was intended. --><p>因此，所有未注解的上/下溢出都被视为bug并会导致Abandonment。除了我们的编译器会激进地优化掉冗余检查之外，这基本上与使用<a href="https://msdn.microsoft.com/en-us/library/h25wtyxf.aspx" target="_blank" rel="noopener"><code>/checked</code>选项</a>来编译C#程序相类似（因为很少有人考虑在C#中使用这个选项，所欲代码生成器在删除冗余的插入检查时几乎没有那么积极地处理）。多亏了这种语言和编译器共同开发的方式，其结果远远好于大多数C++编译器面对SafeInt算法时所生成的代码。与C#一样，<a href="https://msdn.microsoft.com/en-us/library/khy08726.aspx" target="_blank" rel="noopener"><code>unchecked</code>修饰的范围构造</a>也可用于故意设定的上/下溢出的情况。</p><!-- Although the initial reactions from most C# and C++ developers I've spoken to about this idea are negative about it, ourexperience was that 9 times out of 10, this approach helped to avoid a bug in the program.  That remaining 1 time wasusually an abandonment sometime late in one of our 72 hour stress runs -- in which we battered the entire system withbrowsers and multimedia players and anything else we could do to torture the system -- when some harmless counteroverflowed.  I always found it amusing that we spent time fixing these instead of the classical way products maturethrough the stress program, which is to say deadlocks and race conditions.  Between you and me, I'll take the overflowabandonments! --><p>虽然大多数C#和C++开发者对我所说的这个想法最初反应都是负面的，但我们的经验是：10次中有9次，这种方法都有助于避免程序中的bug，而剩下的那一次通常是在我们72小时的压力运行（我们用浏览器和多媒体播放器，以及我们认为可以给系统加压的任何应用来测试系统）的后来某个时间点上，因为一些无害的计数器溢出时发生的Abandonment。我们花时间修复这些而不是采用压力程序提升产品成熟度的经典方式——也就是所谓的死锁和竞争条件，这一点上我总是觉得是非常有趣的。在你我之间，我会将溢出采用Abandonment！</p><!-- ### Out-of-Memory and Stack Overflow --><h3 id="内存不足和栈溢出"><a href="#内存不足和栈溢出" class="headerlink" title="内存不足和栈溢出"></a>内存不足和栈溢出</h3><!-- Out-of-memory (OOM) is complicated.  It always is.  And our stance here was certainly contentious also. --><p>内存不足（OOM）的情况总是很复杂，所以我们在这里的立场当然也是有争议的。</p><!-- In environments where memory is manually managed, error code-style of checking is the most common approach: --><p>在手动管理内存的环境中，错误码风格的检查方式是最常用的方法：</p><!--     X* x = (X*)malloc(...);    if (!x) {        // Handle allocation failure.    } --><pre><code>X* x = (X*)malloc(...);if (!x) {    // 处理内存分配失败 } </code></pre><!-- This has one subtle benefit: allocations are painful, require thought, and therefore programs that use this techniqueare often more frugal and deliberate with the way they use memory.  But it has a huge downside: it's error prone andleads to huge amounts of frequently untested code-paths.  And when code-paths are untested, they usually don't work. --><p>这里的一个微妙的好处是：分配是痛苦的，并且需要思考，因此使用这种技术的程序在使用内存的方式上通常更加节俭和慎重。但它也有一个巨大的缺点：容易出错，并导致大量通常是未经测试的代码路径。当代码路径未经测试时，它们通常都会出错。</p><!-- Developers in general do a terrible job making their software work properly right at the edge of resource exhaustion.In my experience with Windows and the .NET Framework, this is where egregious mistakes get made.  And it leads toridiculously complex programming models, like .NET's so-called [Constrained Execution Regions](https://blogs.msdn.microsoft.com/bclteam/2005/06/13/constrained-execution-regions-and-other-errata-brian-grunkemeyer/).A program limping along, unable to allocate even tiny amounts of memory, can quickly become the enemy of reliability.[Chris Brumme's wondrous Reliability post](http://blogs.msdn.com/b/cbrumme/archive/2003/06/23/51482.aspx) describes thisand related challenges in all its gory glory. --><p>一般而言，开发者在系统处于资源枯竭的边缘时，非常努力地试图使他们的软件正常工作，但根据我使用Windows和.NET Framework的经验来看，这是一个令人震惊的错误。它会导致非常复杂的编程模型，比如.NET的所谓的<a href="https://blogs.msdn.microsoft.com/bclteam/2005/06/13/constrained-execution-regions-and-other-errata-brian-grunkemeyer/" target="_blank" rel="noopener">约束执行区域（Constrained Execution Region）</a>。如果一个程序艰难地运行着，即使是少量的内存也无法分配，那么这种情况很快就会成为可靠性的天敌，<a href="http://blogs.msdn.com/b/cbrumme/archive/2003/06/23/51482.aspx" target="_blank" rel="noopener">Chris Brumme奇妙的关于可靠性文章</a>中描述了这种情形以及相关的挑战。</p><!-- Parts of our system were of course "hardened" in a sense, like the lowest levels of the kernel, where abandonment'sscope would be necessarily wider than a single process.  But we kept this to as little code as possible. --><p>在某种意义上，我们系统的某些部分当然是“硬化的”，就像内核的最低层次一样，对这部分采用Abandonment其影响范围必然比单个进程要更宽，但我们也尽量保持这部分代码尽量的小。</p><!-- For the rest?  Yes, you guessed it: abandonment.  Nice and simple. --><p>对于系统其余部分呢？是的你猜对了——Abandonment，这很不错也很简单。</p><!-- It was surprising how much of this we got away with.  I attribute most of this to the isolation model.  In fact, wecould *intentionally* let a process suffer OOM, and ensuing abandonment, as a result of resource management policy, andstill remain confident that stability and recovery were built in to the overall architecture.--><p>令人惊讶的是我们侥幸逃脱了多少这种方式，对此我将大部分原因都归结于隔离模型。实际上，由于资源管理的策略，我们可以<em>故意</em>让一个进程遭受OOM，然后对其中止的策略，并且仍然对建立在整体架构上稳定性和恢复保持信心。</p><!-- It was possible to opt-in to recoverable failure for individual allocations if you really wanted.  This was not commonin the slightest, however the mechanisms to support it were there.  Perhaps the best motivating example is this: imagineyour program wants to allocate a buffer of 1MB in size.  This situation is different than your ordinary run-of-the-millsub-1KB object allocation.  A developer may very well be prepared to think and explicitly deal with the fact that acontiguous block of 1MB in size might not be available, and deal with it accordingly.  For example: --><p>如果你真的需要的话，可以选择对单个分配采取可恢复故障的方式，虽然这并不常见，但支持的机制是存在的。也许最激发积极性的例子是：假设你的程序想要分配1MB大小的缓冲区，这种情况与普通的1KB对象的分配是有所不同的。开发者可能已经思考并准备好应对，那些可能无法获得1MB大小的连续块内存的情况，并相应地对其进行处理。例如：</p><!--     var bb = try new byte[1024*1024] else catch;    if (bb.Failed) {        // Handle allocation failure.    } --><pre><code>var bb = try new byte[1024*1024] else catch;if (bb.Failed) {    // 处理分配失败 } </code></pre><!-- Stack overflow is a simple extension of this same philosophy.  Stack is just a memory-backed resource.  In fact, thanksto our asynchronous linked stacks model, running out of stack was physically identical to running out of heap memory,so the consistency in how it was dealt with was hardly surprising to developers.  Many systems treat stack overflow thisway these days. --><p>栈溢出是这一理念的简单扩展，因为栈只是内存所支持下的一种资源。实际上，由于我们的异步链接栈模型，栈内存的溢出与堆内存的溢出在物理上是完全相同的，因此开发者对其处理方式的一致性并不令人惊讶。如今，许多系统也都以这种方式来处理栈溢出。</p><!-- ## Assertions --><h2 id="断言"><a href="#断言" class="headerlink" title="断言"></a>断言</h2><!-- An assertion was a manual check in the code that some condition held true, triggering abandonment if it did not.  Aswith most systems, we had both debug-only and release code assertions, however unlike most other systems, we had morerelease ones than debug.  In fact, our code was peppered liberally with assertions.  Most methods had multiple. --><p>断言是代码中的手动检查某些条件是否成立，如果不成立则触发Abandonment的机制。与大多数系统一样，我们同时具有调试（debug-only）版本和发布（release）版本的代码断言，但与大多数其他系统不同的是，我们在发布版本中的断言数量多于调试版本。事实上，我们的代码充满了断言，而大多数方法中存在多个断言。</p><!-- This kept with the philosophy that it's better to find a bug at runtime than to proceed in the face of one.  And, ofcourse, our backend compiler was taught how to optimize them aggressively as with everything else.  This level ofassertion density is similar to what guidelines for highly reliable systems suggest.  For example, from NASA's paper,[The Power of Ten -Rules for Developing Safety Critical Code](http://pixelscommander.com/wp-content/uploads/2014/12/P10.pdf): --><p>这样做的理念是，在运行时找到bug比在遇到错误时继续运行更好，当然，我们的后端编译器也被实现为像其他方面一样激进地对断言进行优化。这种断言的密度水平类似于高可靠性系统的指导原则所建议的一样，例如，来自美国宇航局的论文，<a href="http://pixelscommander.com/wp-content/uploads/2014/12/P10.pdf" target="_blank" rel="noopener">“The Power of Ten -Rules for Developing Safety Critical Code”</a>是这样描述的：</p><!-- > Rule: The assertion density of the code should average to a minimum of two assertions per function.  Assertions are> used to check for anomalous conditions that should never happen in real-life executions.  Assertions must always be> side-effect free and should be defined as Boolean tests.> > Rationale: Statistics for industrial coding efforts indicate that unit tests often find at least one defect per 10 to> 100 lines of code written.  The odds of intercepting defects increase with assertion density.  Use of assertions is> often also recommended as part of strong defensive coding strategy. --><blockquote><p>规则：代码的断言密度应为平均每个函数最少两个断言。断言用于检查在现实生活中不应发生的异常情况。断言必须始终是无副作用的，并且应该定义为布尔测试。</p></blockquote><blockquote><p>理由：工业编码工作的统计数据表明，通常每写入10到100行代码，单元测试就会发现至少一个缺陷。而拦截缺陷的几率随着断言密度的增加而增加，断言的使用通常也被推荐为强防御性编码策略的一部分。</p></blockquote><!-- To indicate an assertion, you simply called `Debug.Assert` or `Release.Assert`: --><p>要表示断言，只需调用<code>Debug.Assert</code>或<code>Release.Assert</code>即可：</p><!--     void Foo() {        Debug.Assert(something); // Debug-only assert.        Release.Assert(something); // Always-checked assert.    } --><pre><code>void Foo() {    Debug.Assert(something); // 仅对调试版本的断言     Release.Assert(something); // 始终检查性的断言 } </code></pre><!-- We also implemented functionality akin to `__FILE__` and `__LINE__` macros like in C++, in addition to `__EXPR__` forthe text of the predicate expression, so that abandonments due to failed assertions contained useful information. --><p>我们还实现了类似于C++中的<code>__FILE__</code>和<code>__LINE__</code>宏的功能，以及谓词表达式文本的<code>__EXPR__</code>，因此由于断言失败而导致的Abandonment会包含有用的调试信息。</p><!-- In the early days, we used different "levels" of assertions than these.  We had three levels, `Contract.Strong.Assert`,`Contract.Assert`, and `Contract.Weak.Assert`.  The strong level meant "always checked," the middle one meant "it's upto the compiler," and the weak one meant "only checked in debug mode."  I made the controversial decision to move awayfrom this model.  In fact, I'm pretty sure 49.99% of the team absolutely hated my choice of terminology (`Debug.Assert`and `Release.Assert`), but I always liked them because it's pretty unambiguous what they do.  The problem with the oldtaxonomy was that nobody ever knew exactly when the assertions would be checked; confusion in this area is simply notacceptable, in my opinion, given how important good assertion discipline is to the reliability of one's program. --><p>在早期，我们使用不同“级别”断言的方式。断言有三个级别，分别是<code>Contract.Strong.Assert</code>，<code>Contract.Assert</code>和<code>Contract.Weak.Assert</code>。最强的<code>Contract.Strong.Assert</code>级别意味着“始终检查”，中间的<code>Contract.Assert</code>级别意味这“是否检查取决于编译器”，最弱的<code>Contract.Weak.Assert</code>级别意味着“只在调试模式下检查”。我做出了有争议的决定，以放弃这种这种分类方式。事实上，我非常确定团队49.99%的成员绝对讨厌我所选择的术语（<code>Debug.Assert</code>和<code>Release.Assert</code>），但我总是喜欢这种方式，因为它们表示的意义非常明确。旧的分类方法的问题在于，没有人确切知道何时会检查断言，而在我看来，这个领域内的混乱根本是不可接受的，因为好的断言规则对程序的可靠性非常重要。</p><!-- As we moved contracts to the language (more on that soon), we tried making `assert` a keyword too.  However, weeventually switched back to using APIs.  The primary reason was that assertions were *not* part of an API's signaturelike contracts are; and given that assertions could easily be implemented as a library, it wasn't clear what we gainedfrom having them in the language.  Furthermore, policies like "checked in debug" versus "checked in release" simplydidn't feel like they belonged in a programming language.  I'll admit, years later, I'm still on the fence about this. --><p>当我们将合约添加到语言中（很快会有更多的合约）时，我们也尝试将<code>assert</code>变成关键字。但是，我们最终转而使用API的方式，其主要原因是断言不像合约那样，它<em>不是</em>API签名的一部分；并鉴于断言可以很容易地作为一个库来实现，我们也不清楚加入到语言中能获得什么。此外，像“checked in debug”和“checked in release”之类的策略根本不像是编程语言特性，我承认，多年以后，我仍然对此持怀疑态度。</p><!-- ## Contracts --><h2 id="合约"><a href="#合约" class="headerlink" title="合约"></a>合约</h2><!-- Contracts were *the* central mechanism for catching bugs in Midori.  Despite us beginning with [Singularity](https://en.wikipedia.org/wiki/Singularity_(operating_system)), which used Sing#, a variant of [Spec#](http://research.microsoft.com/en-us/projects/specsharp/), we quickly moved away to vanilla C# and had to rediscover whatwe wanted.  We ultimately ended up in a very different place after living with the model for years. --><p>在Midori中，合约是捕获bug的<em>核心</em>机制。尽管我们以使用了<a href="http://research.microsoft.com/en-us/projects/specsharp/" target="_blank" rel="noopener">Spec#</a>变体Sing#的<a href="http://t.cn/EMtXJiW" target="_blank" rel="noopener">Singularity</a>作为开始，但我们很快就转移到了普通C#并且不得不重新发现我们想要的东西。在与此模型打交道多年以后，我们最终以和开始时非常不同的模样作为结束。</p><!-- All contracts and assertions were proven side-effect free thanks to our language's understanding of immutability andside-effects.  This was perhaps the biggest area of language innovation, so I'll be sure to write a post about it soon. --><p>由于我们的语言对不变性和副作用的理解方式，所有的合约和断言都被证明是无副作用的，这可能是语言创新的最大领域，所以我一定会尽快写一篇关于此的文章。</p><!-- As with other areas, we were inspired and influenced by many other systems.  Spec# is the obvious one.  [Eiffel washugely influential](https://files.ifi.uzh.ch/rerg/amadeus/teaching/courses/ase_fs10/Meyer1992.pdf) especially as thereare many published case studies to learn from.  Research efforts like Ada-based [SPARK](https://en.wikipedia.org/wiki/SPARK_(programming_language)) and proposals for realtime and embedded systems too.  Goingdeeper into the theoretical rabbit's hole, programming logic like [Hoare's axiomatic semantics](http://www.spatial.maine.edu/~worboys/processes/hoare%20axiomatic.pdf) provide the foundation for all of it.  For me,however, the most philosophical inspiration came from CLU's, and later [Argus](https://pdos.csail.mit.edu/archive/6.824-2009/papers/argus88.pdf)'s, overall approach to error handling. --><p>与其他地方一样，关于合约，我们也受到许多其他系统的启发和影响。 Spec#显然是其中之一，<a href="https://files.ifi.uzh.ch/rerg/amadeus/teaching/courses/ase_fs10/Meyer1992.pdf" target="_blank" rel="noopener">Effiel对我们也有很大的影响力</a>，特别是因为他有许多已发表的案例研究可以学习，另外基于Ada的<a href="http://t.cn/EMGjc0l" target="_blank" rel="noopener">SPARK</a>的相关研究工作以及实时和嵌入式系统的建议也是如此。像<a href="http://www.spatial.maine.edu/~worboys/processes/hoare%20axiomatic.pdf" target="_blank" rel="noopener">Hoare的公理语义</a>这样的编程逻辑，深入研究理论上的未知领域，为所有这些打下了基础。然而，对我来说，最有哲学意义上的灵感来自CLU以及后来的<a href="https://pdos.csail.mit.edu/archive/6.824-2009/papers/argus88.pdf" target="_blank" rel="noopener">Argus</a>的整体错误处理方法。</p><!-- ### Preconditions and Postconditions --><h3 id="前置条件和后置条件"><a href="#前置条件和后置条件" class="headerlink" title="前置条件和后置条件"></a>前置条件和后置条件</h3><!-- The most basic form of contract is a method precondition.  This states what conditions must hold for the method to bedispatched.  This is most often used to validate arguments.  Sometimes it's used to validate the state of the targetobject, however this was generally frowned upon, since modality is a tough thing for programmers to reason about.A precondition is essentially a guarantee the caller provides to the callee. --><p>最基本的合约形式是方法的前置条件，其申明了要指派的方法必须具备的条件，并通常用于验证参数。它有时也用于验证目标对象的状态，但这通常是不受欢迎的，因为对于程序员来说，形态是很难推算的。前置条件基本上是调用者向被调用者提供的一种保证。</p><!-- In our final model, a precondition was stated using the `requires` keyword: --><p>在我们的最终模型中，使用<code>requires</code>关键字声明前置条件：</p><!--     void Register(string name)        requires !string.IsEmpty(name) {        // Proceed, knowing the string isn't empty.    } --><pre><code>void Register(string name)    requires !string.IsEmpty(name) {    // 字符串不为空，并继续处理 } </code></pre><!-- A slightly less common form of contract is a method postcondition.  This states what conditions hold *after* the methodhas been dispatched.  This is a guarantee the callee provides to the caller. --><p>一种稍微不太常见的合约形式是方法的后置条件，它表明在指派完方法<em>之后</em>保持何种状态，这是被调用者向调用者提供的一种保证。</p><!-- In our final model, a postcondition was stated using the `ensures` keyword: --><p>在我们的最终模型中，使用<code>ensure</code>关键字声明后置条件：</p><!--     void Clear()        ensures Count == 0 {        // Proceed; the caller can be guaranteed the Count is 0 when we return.    } --><pre><code>void Clear()    ensures Count == 0 {    // 继续处理，并当函数返回时，调用者可以保证Count值是0 } </code></pre><!-- It was also possible to mention the return value in the postcondition, through the special name `return`.  Old values --such as necessary for mentioning an input in a post-condition -- could be captured through `old(..)`; for example: --><p>也可以通过特殊名称<code>return</code>来声明后置条件中的返回值，而旧的参数值，例如在后置条件中引用输入所必需的值，可以通过<code>old(..)</code>来捕获。比如说：</p><pre><code>int AddOne(int value)    ensures return == old(value)+1 {    ...} </code></pre><!-- Of course, pre- and postconditions could be mixed.  For example, from our ring buffer in the Midori kernel: --><p>当然，前置和后置条件也可能是混合出现的，比如说下面是来自Midori内核中环形缓冲区的代码：</p><pre><code>public bool PublishPosition()    requires RemainingSize == 0    ensures UnpublishedSize == 0 {    ...} </code></pre><!-- This method could safely execute its body knowing that `RemainingSize` is `0` and callers could safely execute afterthe return knowing that `UnpublishedSize` is also `0`. --><p>此方法在知道<code>RemainingSize</code>的值为0时，可以安全地执行其函数体；而调用者知道<code>UnpublishedSize</code>也为0后，可以在被调用函数返回后安全地执行。</p><!-- If any of these contracts are found to be false at runtime, abandonment occurs. --><p>如果在运行时发现这些合约中任何一个是错误的，则会执行Abandonment操作。</p><!-- This is an area where we differ from other efforts.  Contracts have recently became popular as an expression of programlogics used in advanced proof techniques.  Such tools prove truths or falsities about stated contracts, often usingglobal analysis.  We took a simpler approach.  By default, contracts are checked at runtime.  If a compiler could provetruth or falsehood at compile-time, it was free to elide runtime checks or issue a compile-time error, respectively. --><p>该领域是我们与其他工作所不同之处。合约作为高级证明技术中使用的程序逻辑表达最近变得流行起来，这些工具通常使用全局分析来证明所述合约的真实性或虚假性。我们采取了一种更简单的方法：在默认情况下，合约在运行被时检查，但如果编译器可以在编译时证明其真或假，则可以自由地分别进行运行时检查或发出编译时错误。</p><!-- Modern compilers have constraint-based analyses that do a good job at this, like the [range analysis](https://en.wikipedia.org/wiki/Value_range_analysis) I mentioned in my last post.  These propagate facts and use them tooptimize code already.  This includes eliminating redundant checks: either explicitly encoded in contracts, or innormal program logic.  And they are trained to perform these analyses in reasonable amounts of time, lest programmersswitch to a different, faster compiler.  The theorem proving techniques simply did not scale for our needs; our coresystem module took over a day to analyze using the best in breed theorem proving analysis framework! --><p>现代编译器具有基于约束的分析，并在这方面已经做得很好，就像我在上一篇文章中提到的<a href="https://en.wikipedia.org/wiki/Value_range_analysis" target="_blank" rel="noopener">范围分析</a>一样。分析器传播事实并使用它们来优化代码，优化包括消除冗余检查，在合约或正常程序逻辑中显式地编码。并且它们被训练地可以在合理的时间内执行这些分析，避免程序员因等待时间过长而切换到其他更快的编译器上。而定理证明技术根本无法满足在我们的规模上的需求，我们的核心系统模块使用最优秀定理证明分析框架，也花了一天的时间来对其进行分析！</p><!-- Furthermore, the contracts a method declared were part of its signature.  This meant they would automatically show up indocumentation, IDE tooltips, and more.  A contract was as important as a method's return and argument types.  Contractsreally were just an extension of the type system, using arbitrary logic in the language to control the shape of exchangetypes.  As a result, all the usual subtyping requirements applied to them.  And, of course, this facilitated modularlocal analysis which could be done in seconds using standard optimizing compiler techniques. --><p>此外，方法声明的合约也是其签名的一部分，这意味着它们会自动显示在文档和IDE的工具提示中，所以合约与方法的返回值和参数类型一样重要。合约实际上只是类型系统的扩展，使用语言中的任意逻辑来控制交换类型的形状，因此，所有通常的子类型要求对于它们都是适用的。当然，这有助于使用标准优化编译器技术在几秒钟内完成对局部分析的模块化。</p><!-- 90-something% of the typical uses of exceptions in .NET and Java became preconditions.  All of the`ArgumentNullException`, `ArgumentOutOfRangeException`, and related types and, more importantly, the manual checks and`throw`s were gone.  Methods are often peppered with these checks in C# today; there are thousands of these in .NET'sCoreFX repo alone.  For example, here is `System.IO.TextReader`'s `Read` method: --><p>.NET和Java中90%的典型异常用法都变成了前置条件。所有的<code>ArgumentNullException</code>，<code>ArgumentOutOfRangeException</code>和相关类型，以及更重要的的人工手动检查和<code>throw</code>都消失了。如今，C#中的方法经常被这些检查所覆盖，仅在.NET的CoreFX代码仓库中就有数千个这样的检查。例如，下面是<code>System.IO.TextReader</code>中的<code>Read</code>方法：</p><!--     /// <summary>    /// ...    /// </summary>    /// <exception cref="ArgumentNullException">Thrown if buffer is null.</exception>    /// <exception cref="ArgumentOutOfRangeException">Thrown if index is less than zero.</exception>    /// <exception cref="ArgumentOutOfRangeException">Thrown if count is less than zero.</exception>    /// <exception cref="ArgumentException">Thrown if index and count are outside of buffer's bounds.</exception>    public virtual int Read(char[] buffer, int index, int count) {        if (buffer == null) {            throw new ArgumentNullException("buffer");        }        if (index < 0) {            throw new ArgumentOutOfRangeException("index");        }        if (count < 0) {            throw new ArgumentOutOfRangeException("count");        }        if (buffer.Length - index < count) {            throw new ArgumentException();        }        ...    } --><pre><code>/// &lt;summary&gt;/// .../// &lt;/summary&gt;/// &lt;exception cref=&quot;ArgumentNullException&quot;&gt;如果buffer为null，则抛出该异常&lt;/exception&gt; /// &lt;exception cref=&quot;ArgumentOutOfRangeException&quot;&gt;如果index小于零，则抛出该异常&lt;/exception&gt;/// &lt;exception cref=&quot;ArgumentOutOfRangeException&quot;&gt;如果count小于零，则抛出该异常&lt;/exception&gt;/// &lt;exception cref=&quot;ArgumentException&quot;&gt;如果index和cout超出缓冲区的范围，则抛出该异常&lt;/exception&gt;public virtual int Read(char[] buffer, int index, int count) {    if (buffer == null) {        throw new ArgumentNullException(&quot;buffer&quot;);    }    if (index &lt; 0) {        throw new ArgumentOutOfRangeException(&quot;index&quot;);    }    if (count &lt; 0) {        throw new ArgumentOutOfRangeException(&quot;count&quot;);    }    if (buffer.Length - index &lt; count) {        throw new ArgumentException();    }    ...} </code></pre><!-- This is broken for a number of reasons.  It's laboriously verbose, of course.  All that ceremony!  But we have to go wayout of our way to document the exceptions when developers really ought not to ever catch them.  Instead, they shouldfind the bug during development and fix it.  All this exception nonsense encourages very bad behavior. --><p>出于多种原因，这段代码已经不能编译。这样的代码当然是非常啰嗦的，充满了繁文缛节，但当开发者真的不应该去捕捉他们时，我们必须尽力去将异常文档化，相反，他们应该在开发过程中找到错误并修复它。所有这些异常都会无意义地助长非常糟糕的行为。</p><!-- If we use Midori-style contracts, on the other hand, this collapses to: --><p>另一方面，如果我们使用Midori风格的合约，那么上面的代码则折叠成如下的形式：</p><pre><code>/// &lt;summary&gt;/// .../// &lt;/summary&gt;public virtual int Read(char[] buffer, int index, int count)    requires buffer != null    requires index &gt;= 0    requires count &gt;= 0    requires buffer.Length - index &gt;= count {    ...} </code></pre><!-- There are a few appealing things about this.  First, it's more concise.  More importantly, however, it self-describesthe contract of the API in a way that documents itself and is easy to understand by callers.  Rather than requiringprogrammers to express the error condition in English, the actual expressions are available for callers to read, andtools to understand and leverage.  And it uses abandonment to communicate failure. --><p>这种方式有一些吸引人之处。首先，它更加的简洁；然而，更重要的是，它以一种记录自身并且调用者易于理解的方式自我描述API的合约。其实际表达式也可供调用者来阅读，以及供工具来理解和利用，而不要求程序员用通俗语言来表达错误条件。另外，它也使用Abandonment方式来传递失败。</p><!-- I should also mention we had plenty of contracts helpers to help developers write common preconditions.  The aboveexplicit range checking is very messy and easy to get wrong.  Instead, we could have written: --><p>我还提到我们有很多合约辅助工具来帮助开发者编写常见的前置条件。上面的显式范围检查非常混乱，且容易出错，相反地，我们可以这样编写：</p><pre><code>public virtual int Read(char[] buffer, int index, int count)    requires buffer != null    requires Range.IsValid(index, count, buffer.Length) {    ...} </code></pre><!-- And, totally aside from the conversation at hand, coupled with two advanced features -- arrays as slices and non-nulltypes -- we could have reduced the code to the following, while preserving the same guarantees:--><p>另外除了交互之外，还有两个高级的功能：数组作为切片（slice）和非零类型。我们可以将上面的代码简化到如下的形式，并同时保留相同的保证：</p><pre><code>public virtual int Read(char[] buffer) {    ...} </code></pre><!-- But I'm jumping way ahead ... --><p>我们再向前迈了一步……</p><!-- ### Humble Beginnings --><h3 id="谦虚的开始"><a href="#谦虚的开始" class="headerlink" title="谦虚的开始"></a>谦虚的开始</h3><!-- Although we landed on the obvious syntax that is very Eiffel- and Spec#-like -- coming full circle -- as I mentionedearlier, we really didn't want to change the language at the outset.  So we actually began with a simple API approach: --><p>虽然我们提到了语法与Eiffel和Spec#的一样清晰明了，但正如我之前所提到的那样，我们真的不想在一开始就改变语言，所以实际上我们从一个简单的API方法作为开始：</p><pre><code>public bool PublishPosition() {    Contract.Requires(RemainingSize == 0);    Contract.Ensures(UnpublishedSize == 0);    ...} </code></pre><!-- There are a number of problems with this approach, as the [.NET Code Contracts](http://research.microsoft.com/en-us/projects/contracts/) effort discovered the hard way. --><p>这种方法存在许多问题，正如在<a href="http://research.microsoft.com/en-us/projects/contracts/" target="_blank" rel="noopener">.NET Code Contracts</a>的努力已经汲取了这方面的教训。</p><!-- First, contracts written this way are part of the API's *implementation*, whereas we want them to be part of the*signature*.  This might seem like a theoretical concern but it is far from being theoretical.  We want the resultingprogram to contain built-in metadata so tools like IDEs and debuggers can display the contracts at callsites.  And wewant tools to be in a position to auto-generate documentation from the contracts.  Burying them in the implementationdoesn't work unless you somehow disassemble the method to extract them later on (which is a hack). --><p>首先，以这种方式编写的合约是API<em>实现</em>的一部分，但我们却希望它们成为<em>签名</em>的一部分，这似乎是一个理论上的问题，但它实际上远非理论上的。我们希望生成的程序包含内置的元数据，使得IDE和调试器等工具可以在调用点上显示合约，同时我们还希望工具能够从合约中自动生成文档。除非你之后以某种反汇编手段从方法中提取它们（这是一种黑客的行为），否则将它们隐藏在实现中是行不通的。</p><!-- This also makes it tough to integrate with a backend compiler which we found was necessary for good performance. --><p>另外，这种方式也使得很难与后端编译器集成，而我们发现与后端的集成对于良好的性能是必要的。</p><!-- Second, you might have noticed an issue with the call to `Contract.Ensures`.  Since `Ensures` is meant to hold on allexit paths of the function, how would we implement this purely as an API?  The answer is, you can't.  One approach isrewriting the resulting MSIL, after the language compiler emitted it, but that's messy as all heck.  At this point, youbegin to wonder, why not simply acknowledge that this is a language expressivity and semantics issue, and add syntax? --><p>其次，你可能已经注意到对<code>Contract.Ensures</code>的调用存在问题。由于<code>Ensures</code>意味着会保留函数的所有返回路径，那么我们如何将其仅仅实现为API的形式？答案是：这是做不到的。一种方法是在语言编译器生成代码之后重写生成的MSIL，但这非常麻烦。此时，你不禁开始怀疑，为什么不简单地承认这是语言表达性和语义问题，并添加相应的语法呢？</p><!-- Another area of perpetual struggle for us was whether contracts are conditional or not.  In many classical systems,you'd check contracts in debug builds, but not the fully optimized ones.  For a long time, we had the same three levelsfor contracts that we did assertions mentioned earlier: --><p>对我们来说，长期挣扎的另一个领域是合约是否是有条件的。在许多经典系统中，只需要检查调试版本中的合约，而对完全优化版本的合约则不检查。正如对前面的断言一样，在很长一段时间里，我们对合约也有三个相应的级别：</p><!-- * Weak, indicated by `Contract.Weak.*`, meaning debug-only.* Normal, indicated simply by `Contract.*`, leaving it as an implementation decision when to check them.* Strong, indicated by `Contract.Strong.*`, meaning always checked. --><ul><li>最弱，由<code>Contract.Weak.*</code>表示，表示仅调试的合约</li><li>正常，简单地用<code>Contract.*</code>表示，留给实现决定何时检查它们</li><li>最强，由<code>Contract.Strong.*</code>表示，表示总是对其检查的合约</li></ul><!-- I'll admit, I initially found this to be an elegant solution.  Unfortunately, over time we found that there was constantconfusion about whether "normal" contracts were on in debug, release, or all of the above (and so people misused weakand strong accordingly).  Anyway, when we began integrating this scheme into the language and backend compilertoolchain, we ran into substantial issues and had to backpedal a little bit. --><p>我承认，我最初认为这是一个优雅的解决方案。但不幸的是，随着时间的推移，我们发现开发者在调试，发布或上述所有内容中是否存在“正常”级别的合约时常常存在着混淆（因此人们经常会误用最弱级别和最强级别）。无论如何，当我们开始将这个方案集成到语言和后端编译器工具链中时，我们遇到了很多问题，所以不得不稍微把目标退后一点。</p><!-- First, if you simply translated `Contract.Weak.Requires` to `weak requires` and `Contract.Strong.Requires` to`strong requires`, in my opinion, you end up with a fairly clunky and specialized language syntax, with more policy thanmade me comfortable.  It immediately calls out for parameterization and substitutability of the `weak`/`strong` policies. --><p>首先，如果你简单地将<code>Contract.Weak.Requires</code>翻译成<code>weak requires</code>，以及将<code>Contract.Strong.Requires</code>翻译成<code>strong requires</code>，那么在我看来，你最终得到一个相当笨重和专门化的语法，以及更多让我感到不舒服的策略。所以这种方式会立即让人呼吁参数化和<code>weak</code>/<code>strong</code>策略的可替代性。</p><!-- Next, this approach introduces a sort of new mode of conditional compilation that, to me, felt awkward.  In otherwords, if you want a debug-only check, you can already say something like: --><p>接下来，这种方法引入了一种对我来说感觉很尴尬的新条件编译模式，换句话说，如果你想对仅调试版本进行检查，可以这样写：</p><pre><code>#if DEBUG    requires X#endif </code></pre><!-- Finally -- and this was the nail in the coffin for me -- contracts were supposed to be part of an API's signature.  Whatdoes it even *mean* to have a conditional contract?  How is a tool supposed to reason about it?  Generate differentdocumentation for debug builds than release builds?  Moreover, as soon as you do this, you lose a critical guarantee,which is that code doesn't run if its preconditions aren't met. --><p>最后，对我来说最后的一击是，合约应该被当作API签名的一部分。那么有条件的合约<em>意味</em>着什么？工具应该如何推理呢？为发布版本生成和调试版本不同的文档？而且只要存在这样的合约，就会失去如果不满足其前置条件则代码将无法运行的关键性保证。</p><!-- As a result, we nuked the entire conditional compilation scheme. --><p>最终，我们完成了整个条件编译的方案。</p><!-- We ended up with a single kind of contract: one that was part of an API's signature and checked all the time.  If acompiler could prove the contract was satisfied at compile-time -- something we spent considerable energy on -- it wasfree to elide the check altogether.  But code was guaranteed it would never execute if its preconditions weren'tsatisfied.  For cases where you wanted conditional checks, you always had the assertion system (described above). --><p>我们最终得到了单一类型的合约，它是API签名的一部分，并且在所有时刻都会被检查。如果编译器在编译时可以证明合约永远会得到满足（我们花了相当大的精力在这上面），那么完全可以免除检查，但是如果不满足其先决条件，则将保证代码永远不会执行。对于需要进行条件检查的情况，则始终有断言系统可以利用（如上所述）。</p><!-- I felt better about this bet when we deployed the new model and found that lots of people had been misusing the "weak"and "strong" notions above out of confusion.  Forcing developers to make the decision led to healthier code. --><p>当我们部署这样的新模型时，我感到这样的方式更好。并且发现许多人因混淆而滥用上面的“weak”和“strong”概念，因此，迫使开发者做出决定给它们带来更好的代码。</p><!-- ## Future Directions --><h2 id="未来的方向"><a href="#未来的方向" class="headerlink" title="未来的方向"></a>未来的方向</h2><!-- A number of areas of development were at varying stages of maturity when our project wound down.--><p>当我们的项目结束时，许多领域的发展处于不同的成熟阶段。</p><!-- ### Invariants --><h3 id="不变量"><a href="#不变量" class="headerlink" title="不变量"></a>不变量</h3><!-- We experimented **a lot** with invariants.  Anytime we spoke to someone versed in design-by-contract, they wereborderline appalled that we didn't have them from day one.  To be honest, our design did include them from the outset.But we never quite got around to finishing the implementation and deploying it.  This was partly just due to engineeringbandwidth, but also because some difficult questions remained.  And honestly the team was almost always satisfied withthe combination of pre- and post-conditions plus assertions.  I suspect that in the fullness of time we'd have addedinvariants for completeness, but to this day some questions remain for me.  I'd need to see it in action for a while. --><p>我们在不变量上进行了<em>很多</em>的实验。每当我们与熟悉合约设计的人交谈时，他们都会对我们从第一天起就没有使用不变量而感到宽慰。说实话，我们的设计从一开始就包含了它们，但是却从来没有完成它的实现和部署，这仅仅是由于工程的产出量不足和一些困难的问题仍然存在所导致的。老实说，团队基本上满意于前置/后置条件和断言的结合，因此我怀疑在充足的时间里我们是否应完成不变量的实现。但到目前为止，我仍然对此有一些问题，所以我需要在行动中再看一段时间。</p><!-- The approach we had designed was where an `invariant` becomes a member of its enclosing type; for example: --><p>我们设计的方法是，<code>invariant</code>成为其封闭类型的成员。例如：</p><pre><code>public class List&lt;T&gt; {    private T[] array;    private int count;    private invariant index &gt;= 0 &amp;&amp; index &lt; array.Length;...} </code></pre><!-- Notice that the `invariant` is marked `private`.  An invariant's accessibility modifier controlled which members theinvariant was required to hold for.  For example, a `public invariant` only had to hold at the entry and exit offunctions with `public` accessibility; this allowed for the common pattern of `private` functions temporarily violatinginvariants, so long as `public` entrypoints preserved them.  Of course, as in the above example, a class was free todeclare a `private invariant` too, which was required to hold at all function entries and exits. --><p>请注意，<code>invariant</code>标记为<code>private</code>，不变量的访问性修饰符控制了需要保持不变性的成员。例如，<code>public invariant</code>变量只需要在具有<code>public</code>访问性的函数的进入和退出时保持不变，它允许<code>private</code>函数的常见模式暂时违反不变性，只要在<code>public</code>的入口点保持它们即可。当然，如上例所示，类也可以自由声明为<code>private invariant</code>，这需要在所有函数入口和出口保持不变性。</p><!-- I actually quite liked this design, and I think it would have worked.  The primary concern we all had was the silentintroduction of checks all over the place.  To this day, that bit still makes me nervous.  For example, in the `List<T>`example, you'd have the `index >= 0 && index < array.Length` check at the beginning and end of *every single function*of the type.  Now, our compiler eventually got very good at recognizing and coalescing redundant contract checks; andthere were ample cases where the presence of a contract actually made code quality *better*.  However, in the extremeexample given above, I'm sure there would have been a performance penalty.  That would have put pressure on us changingthe policy for when invariants are checked, which would have possibly complicated the overall contracts model. --><p>我其实很喜欢这样的设计，并且觉得它会有用。我们所关心的主要问题是在所有地方静默地引入检查。直到今天为止，这一点仍让我感到紧张，例如，在<code>List&lt;T&gt;</code>的示例中，你将在类型的<em>每个函数</em>的开头和结尾进行<code>index &gt; = 0 &amp;&amp; index &lt;array.Length</code>的检查。现在，我们的编译器最终非常善于识别和合并冗余合约检查，并且在很多情况下，合约的存在实际上使代码质量更好。但是，在上面给出的极端例子中，我确信它会造成性能损失，因此会对我们改变检查不变量的策略施加压力，从而可能会使整体合约模型变得复杂。</p><!-- I really wish we had more time to explore invariants more deeply.  I don't think the team sorely missed not having them-- certainly I didn't hear much complaining about their absence (probably because the team was so performance conscious)-- but I do think invariants would have been a nice icing to put on the contracts cake. --><p>我真的希望我们有更多的时间来更深入地探索不变量，我不认为团队严重错过它们，当然我没有听到太多抱怨缺失他们的声音（可能是因为团队非常注重性能），但我确实认为不变量会是合约系统上的闪光之处。</p><!-- ### Advanced Type Systems --><h3 id="高级类型系统"><a href="#高级类型系统" class="headerlink" title="高级类型系统"></a>高级类型系统</h3><!-- I always liked to say that contracts begin where the type system leaves off.  A type system allows you to encodeattributes of variables using types.  A type limits the expected range values that a variable might hold.  A contractsimilarly checks the range of values that a variable holds.  The difference?  Types are proven at compile-time throughrigorous and composable inductive rules that are moderately inexpensive to check local to a function, usually, but notalways, aided by developer-authored annotations.  Contracts are proven at compile-time *where possible* and at runtimeotherwise, and as a result, permit far less rigorous specification using arbitrary logic encoded in the language itself. --><p>我喜欢说合约始于类型系统触及不到之处。类型系统允许你使用类型对变量的属性进行编码，对变量可能包含的预期范围值进行限制。类似地，合约也对变量所持有值的范围进行检查。那么它们的区别在什么地方？类型在编译时通过严格且可组合的归纳规则得以验证，这些规则对于函数局部检查而言开销更小，但通常并不总是由开发者编写的注解所辅助。合约在<em>可能的情况下</em>在编译时进行证明，否则在运行时被证明，因此，合约被允许使用语言本身编码的任意逻辑进行远非严格的规范。</p><!-- Types are preferable, because they are *guaranteed* to be compile-time checked; and *guaranteed* to be fast to check.The assurances given to the developer are strong and the overall developer productivity of using them is better. --><p>类型是一种可取的方式，因为它们<em>保证</em>在编译时进行检查，同时<em>保证</em>检查的快速。它给开发者的保证很强，使得使用它们的整体效率更高。</p><!-- Limitations in a type system are inevitable, however; a type system needs to leave *some* wiggle room, otherwise itquickly grows unwieldly and unusable and, in the extreme, devolves into bi-value bits and bytes.  On the other hand, Iwas always disappointed by two specific areas of wiggle room that required the use of contracts:--><p>然而，类型系统的局限也是不可避免的。类型系统需要留下<em>一些</em>协调空间，否则它会迅速膨胀并且可用性很差，并且在极端情况下会转换为双值位和字节。另一方面，我总是对需要使用合约的两个特定的协调空间区域感到失望：</p><!-- 1. Nullability.2. Numeric ranges. --><ol><li>空值性；</li><li>数值范围。</li></ol><!-- Approximately 90% of our contracts fell into these two buckets.  As a result, we seriously explored more sophisticatedtype systems to classify the nullability and ranges of variables using the type system instead of contracts. --><p>我们大约90%的合约都属于这两者类型。因此，我们认真研究了更复杂的类型系统，以使用类型系统而不是合约的方式对变量的空值性和范围进行分类。</p><!-- To make it concrete, this was the difference between this code which uses contracts: --><p>具体来说，这是与使用合约的代码之间的区别：</p><pre><code>public virtual int Read(char[] buffer, int index, int count)    requires buffer != null    requires index &gt;= 0    requires count &gt;= 0    requires buffer.Length - index &lt; count {    ...} </code></pre><!-- And this code which didn't need to, and yet carried all the same guarantees, checked statically at compile-time: --><p>下面这段代码在编译时静态检查，虽然不需要但仍然保留了所有相同的保证：</p><pre><code>public virtual int Read(char[] buffer) {    ...} </code></pre><!-- Placing these properties in the type system significantly lessens the burden of checking for error conditions.  Letssay that for any given 1 producer of state there are 10 consumers.  Rather than having each of those 10 defendthemselves against error conditions, we can push the responsibility back onto that 1 producer, and either require asingle assertion that coerces the type, or even better, that the value is stored into the right type in the first place. --><p>将这些属性放置在类型系统中可以显著地减轻错误条件检查带来的负担。我们说，对于任何给定的一个状态的生产者，都有10个对应的消费者，那么可以将责任推回到生产者身上，而不是让每个消费者自身来抵御错误条件。这么做只需要一个强制类型的断言，或者首先将值存储到正确的类型这种更好的方式。</p><!-- #### Non-Null Types --><h4 id="非空类型"><a href="#非空类型" class="headerlink" title="非空类型"></a>非空类型</h4><!-- The first one's really tough: guaranteeing statically that variables do not take on the `null` value.  This is whatTony Hoare has famously called his ["billion dollar mistake"](http://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare).  Fixing this for good is arighteous goal for any language and I'm happy to see newer language designers tackling this problem head-on. --><p>其中之一的非空性真的很难做到：保证静态变量不会使用<code>null</code>值，而这就是Tony Hoare的所谓的<a href="http://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare" target="_blank" rel="noopener">“十亿美元的错误”</a>。解决这个问题对于任何语言来说都是一个正确的目标，我很高兴看到新的编程语言的设计师正在正面解决这个问题。</p><!-- Many areas of the language fight you every step of the way on this one.  Generics, zero-initialization, constructors,and more.  Retrofitting non-null into an existing language is tough! --><p>语言的许多方面都会在这一步中与你发生冲突，比如说泛型、零初始化和构造函数等，在现有语言中实现非空性真的很难！</p><!-- ##### The Type System --><h5 id="类型系统"><a href="#类型系统" class="headerlink" title="类型系统"></a>类型系统</h5><!-- In a nutshell, non-nullability boiled down to some simple type system rules: --><p>简而言之，非空值性可以归结为一些简单的类型系统的规则：</p><!-- 1. All unadorned types `T` were non-null by default.2. Any type could be modified with a `?`, as in `T?`, to mark it nullable.3. `null` is an illegal value for variables of non-null types.4. `T` implicitly converts to `T?`.  In a sense, `T` is a subtype of `T?` (although not entirely true).5. Operators exist to convert a `T?` to a `T`, with runtime checks that abandoned on `null`. --><ol><li>默认情况下，所有未加修饰的类型<code>T</code>都是非空的；</li><li>任何类型都可以使用<code>?</code>进行修饰，如<code>T?</code>，以将其标记为可为空；</li><li><code>null</code>对于任何非空类型变量来说都是非法值；</li><li><code>T</code>可以隐式转换为<code>T?</code>，从某种意义上说，<code>T</code>是<code>T?</code>的子类型（尽管不完全如此）；</li><li>存在运算符将<code>T?</code>转换为<code>T</code>，并进行运行时检查，如果值为<code>null</code>则触发放弃机制。</li></ol><!-- Most of this is probably "obvious" in the sense that there aren't many choices.  The name of the game is systematicallyensuring all avenues of `null` are known to the type system.  In particular, no `null` can ever "sneakily" become thevalue of a non-null `T` type; this meant addressing zero-initialization, perhaps the hardest problem of all. --><p>大多数情况可能是“显而易见的”，因为可以选择的方法并不多。主要的原则是系统地确保所有<code>null</code>值对于类型系统都是可知的，特别是，没有<code>null</code>可以“偷偷摸摸”成为非空的<code>T</code>类型值，这意味着其解决了零初始化问题——可能是所有问题当中最困难的一个。</p><!-- ##### The Syntax --><h5 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h5><!-- Syntactically, we offered a few ways to accomplish #5, converting from `T?` to `T`.  Of course, we discouraged this, andpreferred you to stay in "non-null" space as long as possible.  But sometimes it's simply not possible.  Multi-stepinitialization happens from time to time -- especially with collections data structures -- and had to be supported. --><p>从语法上来讲，我们提供了几种方法来完成任务第五项，也就是将<code>T?</code>转换为<code>T</code>的方法。当然，我们不鼓励这样做，并且希望你尽可能长时间留在“非空”的环境中。但有时它根本不可能做得到，多步骤初始化不时发生，特别是对于集合数据结构来说更是如此，因此这必须得到支持。</p><!-- Imagine for a moment we have a map: --><p>设想一下，我们有如下的map数据结构：</p><pre><code>Map&lt;int, Customer&gt; customers = ...; </code></pre><!-- This tells us three things by construction: --><p>通过构造，我们知晓了三件事：</p><!-- 1. The `Map` itself is not null.2. The `int` keys inside of it will not be `null`.3. The `Customer` values inside of it will also not be null. --><p>1.<code>Map</code>本身不为null；2.其中的<code>int</code>类型的键不为<code>null</code>；3.其中的<code>Customer</code>类型的值也不为<code>null</code>。</p><!-- Let's now say that the indexer actually returns `null` to indicate the key was missing: --><p>我们可以说，索引器实际上返回了<code>null</code>以表示<code>key</code>键不存在。</p><pre><code>public TValue? this[TKey key] {    get { ... }} </code></pre><!-- Now we need some way of checking at callsites whether the lookup succeeded.  We debated many syntaxes. --><p>现在我们需要一些方法来检查调用是否成功，对此我们讨论了多种语法。</p><!-- The easiest we landed on was a guarded check: --><p>最容易想到的是“guarded check”：</p><!--     Customer? customer = customers[id];    if (customer != null) {        // In here, `customer` is of non-null type `Customer`.    } --><pre><code>Customer? customer = customers[id];if (customer != null) {    // 这里的customer变量的类型是非null类型Customer } </code></pre><!-- I'll admit, I was always on the fence about the "magical" type coercions.  It annoyed me that it was hard to figure outwhat went wrong when it failed.  For example, it didn't work if you compared `c` to a variable that held the `null`value, only the literal `null`.  But the syntax was easy to remember and usually did the right thing. --><p>我承认，我总是对“魔法”类型的强制转换持观望态度。但让我感到恼火的是，当它出现失败时很难弄清楚出了什么问题。例如，如果将<code>c</code>与只持有字面<code>null</code>值的变量进行比较，那么它就不起作用了，但它的语法很容易记住，通常能够发挥作用。</p><!-- These checks dynamically branch to a different piece of logic if the value is indeed `null`.  Often you'd want to simplyassert that the value is non-null and abandon otherwise.  There was an explicit type-assertion operator to do that: --><p>如果值确实为<code>null</code>，则这些检查动态会分派到不同的逻辑上，通常，你只想断言该值为非null并在断言失败则触发放弃。有显式的类型断言运算符可以做到这一点：</p><pre><code>Customer? maybeCustomer = customers[id];Customer customer = notnull(maybeCustomer); </code></pre><!-- The `notnull` operator turned any expression of type `T?` into an expression of type `T`. --><p>除此之外，<code>notnull</code>操作符将类型为<code>T?</code>的任何表达式转换为<code>T</code>类型的表达式。</p><!-- ##### Generics --><h5 id="泛型"><a href="#泛型" class="headerlink" title="泛型"></a>泛型</h5><!-- Generics are hard, because there are multiple levels of nullability to consider.  Consider: --><p>泛型很难，因为要考虑多个级别的空值性。考虑如下的代码：</p><pre><code>class C {    public T M&lt;T&gt;();    public T? N&lt;T&gt;();}var a = C.M&lt;object&gt;();var b = C.M&lt;object?&gt;();var c = C.N&lt;object&gt;();var d = C.N&lt;object?&gt;(); </code></pre><!-- The basic question is, what are the types of `a`, `b`, `c`, and `d`? --><p>最基本问题是，<code>a</code>，<code>b</code>，<code>c</code>和<code>d</code>的类型是什么？</p><!-- I think we made this one harder initially than we needed to largely because C#'s existing nullable is a pretty odd duckand we got distracted trying to mimic it too much.  The good news is we finally found our way, but it took a while. --><p>我认为我们最初将其变得比我们需要的更困难，因为C#现有的可空系统非常的古怪，而我们在试图模仿它上面分心太多。不过，好消息是我们终于找到了方向，虽然这还需要一段时间。</p><!-- To illustrate what I mean, let's go back to the example.  There are two camps: --><p>为了说明我的所表达的意思，让我们回到这个例子。有如下的两个不同阵营：</p><!-- * The .NET camp: `a` is `object`; `b`, `c`, and `d` are `object?`.* The functional language camp: `a` is `object`; `b` and `c` are `object?`; `d` is `object??`.--><ul><li>.NET阵营：<code>a</code>是<code>object</code>，<code>b</code>、<code>c</code>和<code>d</code>是<code>object?</code>；</li><li>函数式语言阵营：<code>a</code>是<code>object</code>，<code>b</code>和<code>c</code>是<code>object?</code>，而<code>d</code>是<code>object??</code>。</li></ul><!-- In other words, the .NET camp thinks you should collapse any sequence of 1 or more `?`s into a single `?`.  Thefunctional language camp -- who understands the elegance of mathematical composition -- eschews the magic and lets theworld be as it is.  We eventually realized that the .NET route is incredibly complex, and requires runtime support. --><p>换句话说，.NET阵营认为你应该将一个或更多<code>?</code>的序列折叠成一个单独的<code>?</code>；而函数式语言阵营，由于了解组合数学的优雅，从而避免了魔法操作，并让整个代码方式也变得如此。 我们最终意识到.NET的路线方式非常复杂，且需要运行时的支持。</p><!-- The functional language route does bend your mind slightly at first.  For example, the map example from earlier: --><p>函数语言的做法最初会稍微改变一下你的想法。例如，对于前面的map示例：</p><!--     Map<int, Customer?> customers = ...;    Customer?? customer = customers[id];    if (customer != null) {        // Notice, `customer` is still `Customer?` in here, and could still be `null`!    } --><pre><code>Map&lt;int, Customer?&gt; customers = ...;Customer?? customer = customers[id];if (customer != null) {    // 请注意，这里的customer仍然是“Customer?”类型，并且值依然可以是`null` } </code></pre><!-- In this model, you need to peel off one layer of `?` at a time.  But honestly, when you stop to think about it, thatmakes sense.  It's more transparent and reflects precisely what's going on under here.  Best not to fight it. --><p>在这个模型中，你需要一次剥掉一层<code>?</code>，但老实说，当你停下来思考为什么需要这么做时，它是有道理的。它更透明，并准确反映了正在发生的事情，因此最好不要排斥它。</p><!-- There's also the question of implementation.  The easiest implementation is to expand `T?` into some "wrapper type,"like `Maybe<T>`, and then inject the appropriate wrap and unwrap operations.  Indeed, that's a reasonable mental modelfor how the implementation works.  There are two reasons this simple model doesn't work, however. --><p>在实现上也有问题的。最简单的实现是将<code>T?</code>扩展为一些“包装（wrapper）类型”，如<code>Maybe&lt;T&gt;</code>，然后注入适当的包装和解包操作。实际上，这是实现工作方式的符合心理模型，但存在两个原因，使得这个简单的模型不起作用。</p><!-- First, for reference type `T`, `T?` must not carry a wasteful extra bit; a pointer's runtime representation can carry`null` as a value already, and for a systems language, we'd like to exploit this fact and store `T?` as efficiently as`T`.  This can be done fairly easily by specializing the generic instantiation.  But this does mean that non-null canno longer simply be a front-end trick.  It requires back-end compiler support. --><p>首先，对于引用类型<code>T</code>来说，<code>T?</code>一定不能占用额外的存储空间，指针的运行时表示已经可以以<code>null</code>作为值。并且对于系统语言而言，我们想利用这个事实来与<code>T</code>一样高效地存储<code>T?</code>，这一点通过专门化泛型实例可以相当容易地完成。但也确实意味着非空不再仅仅是一个前端技巧，它现在也需要后端编译器的支持。</p><!-- (Note that this trick is not so easy to extend to `T??`!) --><p>（注意，这个技巧不是那么容易能够扩展到<code>T??</code>！）</p><!-- Second, Midori supported safe covariant arrays, thanks to our mutability annotations.  If `T` and `T?` have a differentphysical representation, however, then converting `T[]` to `T?[]` is a non-transforming operation.  This was a minorblemish, particularly since covariant arrays become far less useful once you plug the safety holes they already have. --><p>其次，多亏了我们的可变性注解，Midori得以支持安全的协变数组。但是如果<code>T</code>和<code>T?</code>具有不同的物理表示，那么将<code>T[]</code>转换为<code>T?[]</code>将是非变换操作。但这仅算得上是微小的瑕疵，特别是因为协变数组在插入已有的安全孔后变得不那么有用了。</p><!-- Anyway, we eventually burned the ships on .NET `Nullable<T>` and went with the more composable multi-`?` design. --><p>无论如何，最终我们彻底放弃了.NET的<code>Nullable&lt;T&gt;</code>方式，转向了更多可组合的多<code>?</code>操作符设计。</p><!-- ##### Zero-Initialization --><h4 id="零初始化"><a href="#零初始化" class="headerlink" title="零初始化"></a>零初始化</h4><!-- Zero-initialization is a real pain in the butt.  To tame it meant: --><p>零初始化是真正的痛苦之处，征服它意味着要做到：</p><!-- * All non-null fields of a class must be initialized at construction time.* All arrays of non-null elements must be fully initialized at construction time. --><ul><li>必须在构造时初始化类的所有非空字段；</li><li>所有非空元素数组必须在构造时完全被初始化。</li></ul><!-- But it gets worse.  In .NET, value types are implicitly zero-initialized.  The initial rule was therefore: --><p>但它变得更加糟糕。在.NET中，值的类型隐式地被零初始化。因此，最初的规则变成了：</p><!-- * All fields of a struct must be nullable. --><ul><li>结构的所有字段都必须是可空的。</li></ul><!-- But that stunk.  It infected the whole system with nullable types immediately.  My hypothesis was that nullability onlytruly works if nullable is the uncommon (say 20%) case.  This would have destroyed that in an instant. --><p>但是这是臭名昭著的，它会立刻以可空类型污染整个系统。我的假设是，只有可空是不常见的情况（例如20%）中，可空性才真正起作用。而上述的规则会在瞬间毁掉这样的条件。</p><!-- So we went down the path of eliminating automatic zero-initialization semantics.  This was quite a large change.  (C# 6went down the path of allowing structs to provide their own zero-arguments constructors and [eventually had to back itout](https://github.com/dotnet/roslyn/issues/1029) due to the sheer impact this had on the ecosystem.)  Itcould have been made to work but veered pretty far off course, and raised some other problems that we probably got toodistracted with.  If I could do it all over again, I'd just eliminate the value vs. reference type distinctionaltogether in C#.  The rationale for that'll become clearer in an upcoming post on battling the garbage collector. --><p>因此，我们沿着消除自动零初始化语义的道路走了下去，而这却是一个很大的变化。（C# 6选择了允许结构提供零参数构造函数的路径走了下去，并最终因为它对生态系统产生了巨大的影响而<a href="https://github.com/dotnet/roslyn/issues/1029" target="_blank" rel="noopener">不得不支持它</a>）它本可以很好的工作但是严重地偏离了路线，并引发了一些可能让我们分散注意力的问题。如果我可以再来一次，我会在C#中完全消除了值与引用类型的区别。在即将发布的关于与垃圾回收器做斗争的文章中，这个理由将被阐述的更加清晰。</p><!-- ##### The Fate of Non-Null Types --><h5 id="非空类型的命运"><a href="#非空类型的命运" class="headerlink" title="非空类型的命运"></a>非空类型的命运</h5><!-- We had a solid design, and several prototypes, but never deployed this one across the entire operating system.  Thereason why was tied up in our desired level of C# compatibility.  To be fair, I waffled on this one quite a bit, and Isuppose it was ultimately my decision.  In the early days of Midori, we wanted "cognitive familiarity."  In the laterdays of the project, we actually considered whether all of the features could be done as "add on" extensions to C#.  Itwas that later mindset that prevented us from doing non-null types in earnest.  My belief to this day is that additiveannotations just won't work; Spec# tried this with `!` and the polarity always felt inverted.  Non-null needs to be thedefault for this to have the impact we desired. --><p>对于非空类型而言，我们有坚实的设计和多个原型，但却从未在整个操作系统中部署非空类型。之所以这样是因为被捆绑在我们期望的C#兼容性水平上，公平地说，我认为这一点最终是我的决定。在Midori的早期，我们所需要的是“认知熟悉度”，而在项目的后期，我们实际上考虑了是否所有功能都可以作为C#的“附加”扩展来完成。正是后来的思维模式阻止了我们认真地完成非空类型。现在，我对此的信念是，可加注解起不了作用，Spec#尝试利用<code>!</code>达到这个目的而极性总是被翻转。非空必须成为默认值才能实现我们想要的影响力。</p><!-- One of my biggest regrets is that we waited so long on non-null types.  We only explored it in earnest once contractswere a known quantity, and we noticed the thousands of `requires x != null`s all over the place.  It would have beencomplex and expensive, however this would have been a particularly killer combination if we nuked the value typedistinction at the same time.  Live and learn! --><p>我最大的遗憾之一就是我们在非空类型上等待了很久，在合约达到相当数量时，我们才对此进行过认真地探索，并且我们注意到了在项目中有数千个<code>requires x != null</code>。它本来是复杂而且高代价的，但如果我们同时确定值类型的区别，这将是相当杀手锏的组合。活到老，学到老！</p><!-- If we shipped our language as a standalone thing, different from C# proper, I'm convinced this would have made the cut. --><p>如果我们把我们的语言作为一个独立的项目来交付，而不同于C#本身，我相信这会有所成就的。</p><!-- #### Range Types --><h4 id="范围类型"><a href="#范围类型" class="headerlink" title="范围类型"></a>范围类型</h4><!-- We had a design for adding range types to C#, but it always remained one step beyond my complexity limit. --><p>我们有用于向C#添加范围类型的设计，但它总是更进一步超过了我们的复杂性限制。</p><!-- The basic idea is that any numeric type can be given a lower and upper bound type parameter.  For example, say you hadan integer that could only hold the numbers 0 through 1,000,000, exclusively.  It could be stated as`int<0..1000000>`.  Of course, this points out that you probably should be using a `uint` instead and the compilerwould warn you.  In fact, the full set of numbers could be conceptually represented as ranges in this way: --><p>其基本思想是，任何数字类型都可以给出一个下限和上限的类型参数。例如，假设有一个整型，只能持有数字0到1,000,000之间，那么它可以表示为<code>int&lt;0..1000000&gt;</code>。当然，应该指出的是你可能应该使用<code>uint</code>，所以编译器会对你发出警告。实际上，完整的数字集合可以通过这种方式在概念上表示为范围：</p><!--     typedef byte number<0..256>;    typedef sbyte number<-128..128>;    typedef short number<-32768..32768>;    typedef ushort number<0..65536>;    typedef int number<-2147483648..2147483648>;    typedef uint number<0..4294967295>;    // And so on ... --><pre><code>typedef byte number&lt;0..256&gt;;typedef sbyte number&lt;-128..128&gt;;typedef short number&lt;-32768..32768&gt;;typedef ushort number&lt;0..65536&gt;;typedef int number&lt;-2147483648..2147483648&gt;;typedef uint number&lt;0..4294967295&gt;;// 等等... </code></pre><!-- The really "cool" -- but scary complicated -- part is to then use [dependent types](https://en.wikipedia.org/wiki/Dependent_type) to permit symbolic range parameters.  For example, say I have an array andwant to pass an index whose range is guaranteed to be in-bounds.  Normally I'd write: --><p>真正出彩但又是可怕的复杂的部分是，使用<a href="https://en.wikipedia.org/wiki/Dependent_type" target="_blank" rel="noopener">依赖类型</a>来允许符号范围参数。例如，假设我有一个数组，并希望传入一个索引，其范围保证是在范围之内的。那么通常我会写成：</p><pre><code>T Get(T[] array, int index)        requires index &gt;= 0 &amp;&amp; index &lt; array.Length {    return array[index];} </code></pre><!-- Or maybe I'd use a `uint` to eliminate the first half of the check: --><p>或者也许我会用<code>uint</code>来消除检查的前半部分：</p><pre><code>T Get(T[] array, uint index)        index &lt; array.Length {    return array[index];} </code></pre><!-- Given range types, I can instead associate the upper bound of the number's range with the array length directly:--><p>在范围类型的情形下，我可以直接将数字范围的上限与数组长度相关联：</p><pre><code>T Get(T[] array, number&lt;0, array.Length&gt; index) {    return array[index];} </code></pre><!-- Of course, there's no guarantee the compiler will eliminate the bounds check, if you somehow trip up its alias analysis.But we would hope that it does no worse a job with these types than with normal contracts checks.  And admittedly thisapproach is a more direct encoding of information in the type system. --><p>当然，如果以某种方式欺骗了编译器的别名分析，则无法保证编译器会消除边界检查，但我们希望这类型的工作不会比正常的合约检查更糟糕，并且老实地说，这种方法是对类型系统中信息的更直接编码。</p><!-- Anyway, I still chalk this one up to a cool idea, but one that's still in the realm of "nice to have but not critical." --><p>无论如何，我仍然把这归结为很酷的想法，但是仍然处于“很不错但不是关键性”的范畴内。</p><!-- The "not critical" aspect is especially true thanks to slices being first class in the type system.  I'd say 66% or moreof the situations where range checks were used would have been better written using slices.  I think mainly people werestill getting used to having them and so they'd write the standard C# thing rather than just using a slice.  I'll coverslices in an upcoming post, but they removed the need for writing range checks altogether in most code. --><p>由于切片（slice）在类型系统中是一等公民，因此其“非关键”方面是尤其突出的，我可以说66%或更多使用范围检查的情况可以更好地使用切片来编写。我认为主要是人们仍然习惯于拥有它们，因此他们会编写标准的C#而不仅仅是使用切片类型。我将在即将发布的帖子中介绍切片，这样他们在大多数代码中都不再需要编写范围检查。</p><!-- # Recoverable Errors: Type-Directed Exceptions --><h1 id="可恢复错误：类型导向的异常"><a href="#可恢复错误：类型导向的异常" class="headerlink" title="可恢复错误：类型导向的异常"></a>可恢复错误：类型导向的异常</h1><!-- Abandonment isn't the only story, of course.  There are still plenty of legitimate situations where an error theprogrammer can reasonably recover from occurs.  Examples include: --><p>当然，Abandonment不是唯一的主题，程序中仍然存在程序员可以合理地从中恢复错误的大量合法情况。这样的例子包括：</p><!-- * File I/O.* Network I/O.* Parsing data (e.g., a compiler parser).* Validating user data (e.g., a web form submission). --><ul><li>文件I/O</li><li>网络I/O</li><li>解析数据（例如，编译器解析器）</li><li>验证用户数据（例如，Web提交的表单）</li></ul><!-- In each of these cases, you usually don't want to trigger abandonment upon encountering a problem.  Instead, theprogram expects it to occur from time to time, and needs to deal with it by doing something reasonable.  Often bycommunicating it to someone: the user typing into a webpage, the administrator of the system, the developer using atool, etc.  Of course, abandonment is one method call away if that's the most appropriate action to take, but it's oftentoo drastic for these situations.  And, especially for IO, it runs the risk of making the system very brittle.  Imagineif the program you're using decided to wink out of existence every time your network connection dropped a packet! --><p>在所有的情况下，你通常不希望在一遇到问题时就触发Abandonment机制，而相反地，该程序在预期上就可能会不时地发生错误，并需要通过一些合理的操作来对其进行处理。这通常通过将错误交给其他对象处理：向网页中输入的用户、系统管理员和使用工具的开发者等等。当然，如果Abandonment是最合适的行为，那么它不失为一种做法，但它通常也被认为是这些情况下最为极为激烈的反应。而且，特别是对于IO，它的存在使系统承担非常脆弱的风险。想象一下，如果你使用的程序在每次网络连接丢包时都因Abandonment而终止，那这简直是不可想象的！</p><!-- ## Enter Exceptions --><h2 id="进入异常"><a href="#进入异常" class="headerlink" title="进入异常"></a>进入异常</h2><!-- We used exceptions for recoverable errors.  Not the unchecked kind, and not quite the Java checked kind, either. --><p>对于可恢复错误，我们使用异常，这里的异常不是那种非检查性的类型，也不是Java那种检查性类型。</p><!-- First thing's first: although Midori had exceptions, a method that wasn't annotated as `throws` could never throw one.Never ever ever.  There were no sneaky `RuntimeException`s like in Java, for instance.  We didn't need them anyway,because the same situations Java used runtime exceptions for were instead using abandonment in Midori. --><p>首要的原则是：虽然Midori有异常机制，但是一个没有注解为<code>throws</code>的方法永远不会抛出异常，永远永远不会。例如，Midori没有在Java中那种悄悄抛出的<code>RuntimeException</code>，我们无论如何都不需要这样的方式。因为在Java中使用运行时异常的相同情况下，Midori中采用的是Abandonment机制。</p><!-- This led to a magical property of the result system.  90-something% of the functions in our system could not throwexceptions!  By default, in fact, they could not.  This was a stark contrast to systems like C++ where you must go outof your way to abstain from exceptions and state that fact using `noexcept`.  APIs could still fail due to abandonment,of course, but only when callers fail meet the stated contract, similar to passing an argument of the wrong type. --><p>这导致产生的系统具有神奇特性：我们系统中90%的函数都不会抛出异常！事实上，默认情况下他们不能，这就与像C++这样的系统形成了鲜明对比。在这些系统中，你必须不遗余力地避免异常并使用<code>noexcept</code>来申明这一事实。当然，API仍可能因Abandonment机制而调用失败，但只有当调用者未能满足所述合约时才会发生，而这一点上类似于向函数传递了错误类型的参数。</p><!-- Our choice of exceptions was controversial at the outset.  We had a mixture of imperative, procedural, object oriented,and functional language perspective on the team.  The C programmers wanted to use error codes and were worried we wouldrecreate the Java, or worse, C# design.  The functional perspective would be to use dataflow for all errors, butexceptions were very control-flow-oriented.  In the end, I think what we chose was a nice compromise between all of theavailable recoverable error models available to us.  As we'll see later, we did offer a mechanism for treating errors asfirst class values for that rare case where a more dataflow style of programming was what the developer wanted. --><p>我们选择的异常在一开始时就存在争议，我们在团队中融合了命令式、过程式、面向对象和函数式语言的视角。C程序员想使用错误码的方式，并担心我们所做的却是重新创造了Java，或者更糟糕的C#设计。函数式的视角是对所有错误使用数据流，但异常却是十分以控制流为导向的方式。最后，我认为我们选择的是我们所有可用的可恢复错误模型之间的一种妥协。正如我们稍后将看到的，我们确实提供了一种将错误视为一等类型的机制，在这种情况下，开发者想要的是更多的数据流风格的编程。</p><!-- Most importantly, however, we wrote a lot of code in this model, and it worked very well for us.  Even the functionallanguage guys came around eventually.  As did the C programmers, thanks to some cues we took from return codes. --><p>然而最重要的是，我们在这个模型中编写了很多代码，它对我们来说非常有用。即便是函数式语言的开发者也最终加入进来。多亏了我们从返回码中获得的一些线索，C程序员也加入了。</p><!-- ### Language and Type System --><h3 id="语言和类型系统"><a href="#语言和类型系统" class="headerlink" title="语言和类型系统"></a>语言和类型系统</h3><!-- At some point, I made a controversial observation and decision.  Just as you wouldn't change a function's return typewith the expectation of zero compatibility impact, you should not be changing a function's exception type with such anexpectation.  *In other words, an exception, as with error codes, is just a different kind of return value!* --><p>在某个时间点上，我做了一个有争议的观察和决定：正如你不会期望在零兼容性影响条件下更改函数的返回类型一样，你也不应该以这样的期望方式更改函数的异常类型。<em>换句话说，与错误代码一样，异常只是另一种不同的返回值！</em></p><!-- This has been one of the parroted arguments against checked exceptions.  My answer may sound trite, but it's simple:too bad.  You're in a statically typed programming language, and the dynamic nature of exceptions is precisely thereason they suck.  We sought to address these very problems, so therefore we embraced it, embellished strong typing,and never looked back.  This alone helped to bridge the gap between error codes and exceptions.--><p>这是针对检查性异常的一个有争议的论点。对此，我的回答可能听起来有点老生常谈，但其实很简单：这太糟糕了。在静态类型的编程语言中，异常的动态性正是它们出问题的地方。我们试图解决这些问题，因此我们拥抱了该决定，用于为强类型提供辅助，并再也没有回头过。仅此一点就有助于弥合错误码和异常之间的鸿沟。</p><!-- Exceptions thrown by a function became part of its signature, just as parameters and return values are.  Remember,due to the rare nature of exceptions compared to abandonment, this wasn't as painful as you might think.  And a lot ofintuitive properties flowed naturally from this decision. --><p>函数抛出的异常成为其签名的一部分，就像参数和返回值一样。请记住，由于异常与Abandonment相比的不常见性，这一点也并不像你想象的那么痛苦，并且很多直观的属性也自然而然地也从这个决定中衍生出来。</p><!-- The first thing is the [Liskov substitution principle](https://en.wikipedia.org/wiki/Liskov_substitution_principle).  Inorder to avoid the mess that C++ found itself in, all "checking" has to happen statically, at compile time.  As aresult, all of those performance problems mentioned in [the WG21 paper](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2010/n3051.html) were not problems for us.  This type system must bebulletproof, however, with no backdoors to defeat it.  Because we needed to address those performance challenges bydepending on `throws` annotations in our optimizing compiler, type safety hinged on this property. --><p>因此，首要的原则就是<a href="https://en.wikipedia.org/wiki/Liskov_substitution_principle" target="_blank" rel="noopener">Liskov替代原则</a>：为了避免在C++中所发现的混乱，所有的“检查”必须在编译时静态发生。因此，<a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2010/n3051.html" target="_blank" rel="noopener">WG21的文章</a>中提到的所有这些性能问题对我们来说都不再是问题。这种类型的系统必须是可以抵御攻击的，没有后门可以攻陷它，因为我们需要依赖于优化编译器中的<code>throws</code>来解决这些性能挑战，所以类型安全取决于该属性。</p><!-- We tried many many different syntaxes.  Before we committed to changing the language, we did everything with C#attributes and static analysis.  The user experience wasn't very good and it's hard to do a real type system that way.Furthermore, it felt too bolted on.  We experimented with approaches from the Redhawk project -- what eventually became.NET Native and [CoreRT](https://github.com/dotnet/corert) -- however, that approach also didn't leverage the languageand relied instead on static analysis, though it shares many similar principles with our final solution. --><p>我们尝试了许多不同的语法。在我们致力于改变语言之前，我们使用C#属性和静态分析完成了所有工作，但是这种方式的用户体验不是很好，并且很难用这种方式实现一个真正的类型系统。此外，感觉它太简单了。我们尝试了Redhawk项目中，也就是最终成为.NET Native和<a href="https://github.com/dotnet/corert" target="_blank" rel="noopener">CoreRT</a>的方法。然而，尽管这种方法与我们的最终解决方案有许多类似的原则，但它也没有利用语言而仅仅是依赖于静态分析。</p><!-- The basic gist of the final syntax was to simply state a method `throws` as a single bit: --><p>最终语法的基本要点是，简单地通过单个bit来声明<code>throws</code>方法：</p><pre><code>void Foo() throws {    ...} </code></pre><!-- (For many years, we actually put the `throws` at the beginning of the method, but that read wrong.) --><p>（多年来，我们实际上把<code>throws</code>放在了方法的头部位置，但那没有区别）</p><!-- At this point, the issue of substitutability is quite simple.  A `throws` function cannot take the place of a non-`throws` function (illegal strengthening).  A non-`throws` function, on the other hand, can take the place of a `throws`function (legal weakening).  This obviously impacts virtual overrides, interface implementation, and lambdas. --><p>在这一点上，可替代性问题非常简单，有<code>throws</code>的函数不能代替无<code>throws</code>的函数（这叫做非法加强）；而另一方面，无<code>throws</code>的函数可以代替有<code>throws</code>的函数（合法弱化）。而这显然会对虚拟覆盖、接口实现和lambda带来影响。</p><!-- Of course, we did the expected co- and contravariance substitution bells and whistles.  For example, if `Foo` werevirtual and you overrode it but didn't throw exceptions, you didn't need to state the `throws` contract.  Anybodyinvoking such a function virtually, of course, couldn't leverage this but direct calls could. --><p>当然，我们做了所期望的协同和逆变替代等华而不实的功能。例如，如果<code>Foo</code>是虚拟函数并且它被没有抛出异常的函数覆盖，则不需要再声明<code>throws</code>。当然，虚拟地调用这样一个函数的任何调用者都无法利用这个功能，但直接调用却是可以的。</p><!-- For example, this is legal: --><p>例如，下面的代码是合法的：</p><!--     class Base {        public virtual void Foo() throws {...}    }    class Derived : Base {        // My particular implementation doesn't need to throw:        public override void Foo() {...}    } --><pre><code>class Base {    public virtual void Foo() throws {...}}class Derived : Base {    // 特定的实现不需要throws关键字：     public override void Foo() {...}} </code></pre><!-- and callers of `Derived` could leverage the lack of `throws`; whereas this is wholly illegal: --><p><code>Derived</code>的调用者可以利用无<code>throws</code>的条件，而反之则是完全非法的：</p><pre><code>class Base {    public virtual void Foo () {...}}class Derived : Base {    public override void Foo() throws {...}} </code></pre><!-- Encouraging a single failure mode was quite liberating.  A vast amount of the complexity that comes with Java's checkedexceptions evaporated immediately.  If you look at most APIs that fail, they have a single failure mode anyway (once allbug failure modes are done with abandonment): IO failed, parsing failed, etc.  And many recovery actions a developertends to write don't actually depend on the specifics of *what exactly* failed when, say, doing an IO.  (Some do, andfor those, the keeper pattern is often the better answer; more on this topic shortly.)  Most of the information inmodern exceptions are *not* actually there for programmatic use; instead, they are for diagnostics. --><p>对单一故障模式的鼓励是相当自由的。Java的检查性异常带来的大量复杂性将立即消失。如果你观察大多数调用失败的API，它们无一例外都有单一的故障模式（一旦Abandonment处理完成所有bug故障模式），这包括IO故障，解析失败等等。开发者倾向于编写的许多恢复操作，实际上并不依赖于在做IO时<em>究竟哪些地方</em>出现故障的具体细节。（有些操作会这样做，对于它们来说，守护者模式通常是更好的方案，等一下就会有关于这个守护者模式的更多内容）。现代异常中的大多数信息实际上并<em>不</em>适合程序使用，而相反它们是用于诊断的。</p><!-- We stuck with just this "single failure mode" for 2-3 years.  Eventually I made the controversial decision tosupport multiple failure modes.  It wasn't common, but the request popped up reasonably often from teammates, and thescenarios seemed legitimate and useful.  It did come at the expense of type system complexity, but only in all the usualsubtyping ways.  And more sophisticated scenarios -- like aborts (more on that later) -- required that we do this. --><p>我们坚持这种“单一故障模式”有2到3年，最终我做出了支持多种故障模式的有分歧的决定。这种情况虽然并不常见，但是这样的请求经常会从团队成员中涌现出，而这些使用情景似乎是合法且有用的。它确实是以类型系统的复杂性为代价的，但仅限于所有常见的子类型方式。在更复杂的，例如中止场景中（包括后面的更多场景），则要求我们这样做。</p><!-- The syntax looked like this: --><p>语法如下所示：</p><pre><code>int Foo() throws FooException, BarException {    ...} </code></pre><!-- In a sense, then, the single `throws` was a shortcut for `throws Exception`. --><p>从某种意义上说，单个<code>throws</code>是<code>throws Exception</code>的快捷方式。</p><!-- It was very easy to "forget" the extra detail if you didn't care.  For example, perhaps you wanted to bind a lambda tothe above `Foo` API, but didn't want callers to care about `FooException` or `BarException`.  That lambda must be marked`throws`, of course, but no more detail was necessary.  This turned out to be a very common pattern: An internal systemwould use typed exceptions like this for internal control flow and error handling, but translate all of them into justplain `throws` at the public boundary of the API, where the extra detail wasn't required. --><p>如果你不在乎的话，很容易“忘记”额外的细节。例如，你可能希望将lambda绑定到上面的<code>Foo</code>API中，但不希望调用者关心<code>FooException</code>或<code>BarException</code>。当然，lambda必须标记为<code>throws</code>，但不需要更多细节。这被证明是一种非常常见的模式：内部系统会使用类似的类型异常来进行内部控制流和错误处理，但是将它们全部转换为API的公共边界上的普通<code>throws</code>时，其中额外的细节是非必需的。</p><!-- All of this extra typing added great power to recoverable errors.  But if contracts outnumbered exceptions by 10:1,then simple `throws` exceptional methods outnumbered multi-failure-mode ones by another 10:1. --><p>所有这些额外的类型为可恢复的错误增加了强大的功能。但是如果合约的数量超出了异常数量的10倍，那么简单的<code>throws</code>异常方法的数量也将超过了多故障模式方法数量的10倍。</p><!-- At this point, you may be wondering, what differentiated this from Java's checked exceptions? --><p>在这一点上，你可能会疑惑的是，这与Java的检查性异常有何区别？</p><!-- 1. The fact that the lion's share of errors were expressed using abandonment meant most APIs didn't throw.2. The fact that we encouraged a single mode of failure simplified the entire system greatly.  Moreover, we made it easy   to go from the world of multiple modes, to just a single and back again. --><ol><li>大部分错误使用Abandonment表达方式的事实意味着大多数API都没有抛出；</li><li>我们鼓励单一故障模式的事实大大简化了整个系统；此外，还可以轻松地从多种模式过渡到单一故障模式，并再次回到多种模式。</li></ol><!-- The rich type system support around weakening and strengthening also helped, as did something else we did to that helpedbridge the gap between return codes and exceptions, improved code maintainability, and more ...--> <p>利用丰富的类型系统支持弱化和强化也有帮助的，正如我们所做的其他事情一样，也有助于缩小返回码和异常之间的差距，提高代码可维护性等等……</p><!-- ### Easily Auditable Callsites --><h3 id="易于审核的调用点（Callsite）"><a href="#易于审核的调用点（Callsite）" class="headerlink" title="易于审核的调用点（Callsite）"></a>易于审核的调用点（Callsite）</h3><!-- At this point in the story, we still haven't achieved the full explicit syntax of error codes.  The declarations offunctions say whether they can fail (good), but callers of those functions still inherit silent control flow (bad). --><p>在整个故事叙述到这个时候，我们仍然没有实现错误码的完整显式语法。函数的声明说明它们是否可能出现故障（好的方面），但这些函数的调用者仍然继承静默的控制流（坏的方面）。</p><!-- This brings about something I always loved about our exceptions model.  A callsite needs to say `try`: --><p>这给我带来了异常模型中最喜欢的一些东西。调用点需要声明<code>try</code>：</p><pre><code>int value = try Foo(); </code></pre><!-- This invokes the function `Foo`, propagates its error if one occurs, and assigns the return value to `value` otherwise. --><p>这将调用函数<code>Foo</code>，如果发生错误则传播其错误，否则将返回值赋值给<code>value</code>。</p><!-- This has a wonderful property: all control flow remains explicit in the program.  You can think of `try` as a kind ofconditional `return` (or conditional `throw` if you prefer).  I *freaking loved* how much easier this made codereviewing error logic!  For example, imagine a long function with a few `try`s inside of it; having the explicitannotation made the points of failure, and therefore control flow, as easy to pick out as `return` statements: --><p>它有一个很不错的属性：所有控制流在程序中都是显式的。你可以将<code>try</code>视为条件<code>return</code>（如果你愿意，也可以认为是条件<code>throw</code>）。我<em>非常喜欢</em>这种让代码审查错误逻辑变得更容易的方式！例如，设想有一个长函数，里面有数个<code>try</code>语句，具有显式注释使得失败点成为控制流，因此易于选择<code>return</code>语句：</p><!--     void doSomething() throws {        blah();        var x = blah_blah(blah());        var y = try blah(); // <-- ah, hah! something that can fail!        blahdiblahdiblahdiblahdi();        blahblahblahblah(try blahblah()); // <-- another one!        and_so_on(...);    } --><pre><code>void doSomething() throws {    blah();    var x = blah_blah(blah());    var y = try blah(); // &lt;-- 啊哈！可能产生故障的调用！     blahdiblahdiblahdiblahdi();     blahblahblahblah(try blahblah()); // &lt;-- 另一个可能产生故障的调用！     and_so_on(...);} </code></pre><!-- If you have syntax highlighting in your editor, so the `try`s are bold and blue, it's even better. --><p>如果你的编辑器中有语法高亮，那么<code>try</code>将是蓝色粗体的，这样的话就更好了。</p><!-- This delivered many of the strong benefits of return codes, but without all the baggage. --><p>这提供了许多返回码的强大好处，却没有它的所有包袱。</p><!-- (Both Rust and Swift now support a similar syntax.  I have to admit I'm sad we didn't ship this to the general publicyears ago.  Their implementations are very different, however consider this a huge vote of confidence in their syntax.) --><p>（Rust和Swift现在都支持类似的语法。我不得不承认，我对于我们没有在几年前将这种语法交付给公众而感到难过。它们的实现方式非常不同，但对此的考虑给它们的语法带来了很大的信心。）</p><!-- Of course, if you are `try`ing a function that throws like this, there are two possibilities: --><p>当然，如果你正在尝试这样对如此抛出的函数采取<code>try</code>方法，就有一下两种可能：</p><!-- * The exception escapes the calling function.* There is a surrounding `try`/`catch` block that handles the error. --><ul><li>异常从被调用函数中逃逸出来；</li><li>周围有一个<code>try</code>/<code>catch</code>块来处理错误。</li></ul><!-- In the first case, you are required to declare that your function `throws` too.  It is up to you whether to propagatestrong typing information should the callee declare it, or simply leverage the single `throws` bit, of course. --><p>在第一种情况下，你也需要对你的函数声明<code>throws</code>。当然，是传播由被调用函数声明的强类型信息，还是利用单个<code>throws</code>位，这是由你来决定的。</p><!-- In the second case, we of course understood all the typing information.  As a result, if you tried to catch somethingthat wasn't declared as being thrown, we could give you an error about dead code.  This was yet another controversialdeparture from classical exceptions systems.  It always bugged me that `catch (FooException)` is essentially hiding adynamic type test.  Would you silently permit someone to call an API that returns just `object` and automatically assignthat returned value to a typed variable?  Hell no!  So we didn't let you do that with exceptions either. --><p>在第二种情况下，我们当然理解所有的输入类型信息，因此，如果你尝试捕捉未声明被抛出的内容，我们可能会给你一个dead code的错误，这是与传统异常系统的另一个有争议的背离。它总是提示我<code>catch（FooException）</code>本质上隐藏了动态类型测试。 你是否会默默地允许调用仅返回<code>object</code>的API，并自动将这个返回值分配给具有类型的变量？那一定是不行的！ 因此，我们也不会让你在异常上这样做。</p><!--Here too CLU influenced us.  Liskov talks about this in [A History of CLU](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.127.8460&rep=rep1&type=pdf):--><p>CLU也对我们产生了影响，Liskov在<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.127.8460&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">一篇关于CLU历史的文章</a>中谈到：</p><!--> CLU's mechanism is unusual in its treatment of unhandled exceptions. Most mechanisms pass these through: if the caller> does not handle an exception raised by a called procedure, the exception is propagated to its caller, and so on. We> rejected this approach because it did not fit our ideas about modular program construction. We wanted to be able to> call a procedure knowing just its specification, not its implementation. However, if exceptions are propagated> automatically, a procedure may raise an exception not described in its specification. --><blockquote><p>CLU在对待未处理的异常方面的机制是不寻常的。大多数机制都通过这样的方式传递：如果调用者没有处理被调用过程引发的异常，则异常将传播给它的调用者，依此类推下去。而我们拒绝使用这种方法，因为它不符合我们关于模块化程序构建的思想。我们希望能够通过只需知道其规范而不是其实现的方式来调用过程，但如果异常自动地传播下去，则过程可能会触发其规范中未描述的异常。</p></blockquote><!-- Although we discouraged wide `try` blocks, this was conceptually a shortcut for propagating an error code.  To see whatI mean, consider what you'd do in a system with error codes.  In Go, you might say the following: --><p>虽然我们不鼓励大范围的<code>try</code>块，但这是在概念上传播错误码的快捷方式。如果要了解我的意思，请考虑在具有错误码的系统中你是怎么做的，比如在Go中，你可能会写出如下的代码：</p><pre><code>if err := doSomething(); err != nil {    return err} </code></pre><!-- In our system, you say: --><p>而在我们的系统中，你需要这样编码：</p><pre><code>try doSomething(); </code></pre><!-- But we used exceptions, you might say!  It's completely different!  Sure, the runtime systems differ.  But from alanguage "semantics" perspective, they are isomorphic.  We encouraged people to think in terms of error codes and notthe exceptions they knew and loved.  This might seem funny: Why not just use return codes, you might wonder?  In anupcoming section, I will describe the true isomorphism of the situation to try to convince you of our choice. --><p>但你可能会说，我们使用了异常，这是完全不同的！当然，运行时系统会不一样，但从语言“语义学”的角度来看，它们其实是同构的。我们鼓励人们根据错误码来思考，而不是他们所熟悉和喜爱的异常。这可能会有点意思：你可能想知道，为什么不使用返回码？在接下来的部分中，我将描述真正的场景的同构，以试图说服你我们的选择。</p><!-- ### Syntactic Sugar --><h3 id="语法糖"><a href="#语法糖" class="headerlink" title="语法糖"></a>语法糖</h3><!-- We also offered some syntactic sugar for dealing with errors.  The `try`/`catch` block scoping construct is a bitverbose, especially if you're following our intended best practices of handling errors as locally as possible.  It alsostill retains a bit of that unfortunate `goto` feel for some, especially if you are thinking in terms of return codes.That gave way to a type we called `Result<T>`, which was simply *either* a `T` value *or* an `Exception`. --><p>我们还提供了一些处理错误的语法糖。<code>try</code>/<code>catch</code>块作用域结构有点冗长，特别是如果你尽量局部地遵循我们处理错误的预期最佳实践。对于某些地方来说，它仍然不幸地保留了一些<code>goto</code>的感觉，特别是如果就返回码考虑而言。这种方式让位于我们称为<code>Result&lt;T&gt;</code>的类型，它只是一个简单的<code>T</code>值<em>或</em><code>Exception</code>。</p><!-- This essentially bridged from the world of control-flow to the world of dataflow, for scenarios in which the latter wasmore natural.  Both certainly had their place, although most developers preferred the familiar control flow syntax. --><p>对于数据流来说更自然的场景中，这基本上是从控制流世界到数据流世界之间的桥梁，虽然大多数开发者更喜欢熟悉的控制流语法，但两者肯定都有自己的一席之地。</p><!-- To illustrate common usage, imagine you want to log all errors that occur, before repropagating the exception.  Thoughthis is a common pattern, using `try`/`catch` blocks feels a little too control flow heavy for my taste: --><p>为了说明常见的用法，假设你希望在重新传播异常之前记录发生的所有错误，虽然这是一种常见的模式，但使用<code>try</code>/<code>catch</code>块会让我觉得有点过于控制流式：</p><!--     int v;    try {        v = try Foo();        // Maybe some more stuff...    }    catch (Exception e) {        Log(e);        rethrow;    }    // Use the value `v`... --><pre><code>int v;try {    v = try Foo();    // 或许有更多的语句...}catch (Exception e) {    Log(e);    rethrow;}// 使用v值... </code></pre><!-- The "maybe some more stuff" bit entices you to squeeze more than you should into the `try` block.  Compare this to using`Result<T>`, leading to a more return-code feel and more convenient local handling: --><p>“或许有更多的语句”位置处会吸引你在<code>try</code>块处挤进应该比更多的东西，将此与使用<code>Result&lt;T&gt;</code>进行比较，从而产生更多的返回码感觉和更方便的本地处理：</p><!--     Result<int> value = try Foo() else catch;    if (value.IsFailure) {        Log(value.Exception);        throw value.Exception;    }    // Use the value `value.Value`... --><pre><code>Result&lt;int&gt; value = try Foo() else catch;if (value.IsFailure) {    Log(value.Exception);    throw value.Exception;}// 使用值`value.Value` ...</code></pre><!-- The `try ... else` construct also permitted you to substitute your own value instead, or even trigger abandonment, inresponse to failure: --><p>作为对故障的响应，<code>try</code>/<code>else</code>构造还允许你替换成自己的值，甚至触发Abandonment机制：</p><pre><code>int value1 = try Foo() else 42;int value2 = try Foo() else Release.Fail(); </code></pre><!-- We also supported NaN-style propagation of dataflow errors by lifting access to `T`s members out of the `Result<T>`.For example, let's say I have two `Result<int>`s and want to add them together.  I can do so:--><p>我们还通过从<code>Result&lt;T&gt;</code>中抽取成员<code>T</code>的访问来支持NaN样式的数据流错误的传播。例如，假设有两个<code>Result&lt;int&gt;</code>，并希望对它们求和，那么我可以这样做：</p><pre><code>Result&lt;int&gt; x = ...;Result&lt;int&gt; y = ...;Result&lt;int&gt; z = x + y; </code></pre><!-- Notice that third line, where we added the two `Result<int>`s together, yielding a -- that's right -- third `Result<T>`.This is the NaN-style dataflow propagation, similar to C#'s new `.?` feature. --><p>注意第三行，我们将两个<code>Result&lt;int&gt;</code>加到一起，没错是的，产生了第三个<code>Result&lt;T&gt;</code>。这是NaN风格的数据流传播，类似于C#的<code>.?</code>新功能。</p><!-- This approach blends what I found to be an elegant mixture of exceptions, return codes, and dataflow error propagation. --><p>我发现这种方法是异常、返回码和数据流错误传播的优雅混合。</p><!-- ## Implementation --><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><!-- The model I just described doesn't have to be implemented with exceptions.  It's abstract enough to be reasonablyimplemented using either exceptions or return codes.  This isn't theoretical.  We actually tried it.  And this is whatled us to choose exceptions instead of return codes for performance reasons. --><p>我刚才所描述的模型不必用异常来实现，因为它已经足够抽象，可以使用返回码来合理地实现。这不仅仅是理论上可行性，我们实际上就这么尝试过，这也是导致我们出于性能原因选择异常而不是返回码方式的原因。</p><!-- To illustrate how the return code implementation might work, imagine some simple transformations: --><p>为了说明返回码实现是如何工作的，设想如下的一些简单的转换：</p><pre><code>int foo() throws {    if (...p...) {        throw new Exception();    }    return 42;} </code></pre><!-- becomes: --><p>变成:</p><pre><code>Result&lt;int&gt; foo() {    if (...p...) {        return new Result&lt;int&gt;(new Exception());    }    return new Result&lt;int&gt;(42);} </code></pre><!-- And code like this: --><p>以及，该代码：</p><pre><code>int x = try foo(); </code></pre><!-- becomes something more like this: --><p>变成如下的代码：</p><pre><code>int x;Result&lt;int&gt; tmp = foo();if (tmp.Failed) {    throw tmp.Exception;}x = tmp.Value; </code></pre><!-- An optimizing compiler can represent this more efficiently, eliminating excessive copying.  Especially with inlining. --><p>优化编译器可以对其进行更有效地表示，特别是通过内联，消除了过多的复制操作。</p><!-- If you try to model `try`/`catch`/`finally` this same way, probably using `goto`, you'll quickly see why compilers havea hard time optimizing in the presence of unchecked exceptions.  All those hidden control flow edges! --><p>如果你通过<code>goto</code>的方式尝试以同样的方式来对<code>try</code>/<code>catch</code>/<code>finally</code>进行建模，你会很快看到为什么编译器在存在非检查性异常时很难进行优化，所有都隐藏在控制流的边缘上！</p><!-- Either way, this exercise very vividly demonstrates the drawbacks of return codes.  All that goop -- which is meant tobe rarely needed (assuming, of course, that failure is rare) -- is on hot paths, mucking with your program's golden pathperformance.  This violates one of our most important principles. --><p>无论以哪种方式，这个例子都非常生动地展示了返回码的缺点。所有的跳转语句都处在热路径上，但它们其实是很少需要的（当然，假设故障是罕见的），并混淆你程序的黄金路径上的性能，因此违反了我们最重要的原则之一。</p><!-- I described the results of our dual mode experiment in [my last post](http://joeduffyblog.com/2015/12/19/safe-native-code/).  In summary, the exceptions approach was 7% smaller and 4%faster as a geomean across our key benchmarks, thanks to a few things: --><p>我在<a href="/2019/02/17/midori/4-safe-native-code/">上一篇文章</a>中描述了我们双模式实验的结果。总之，由于以下几点，异常方法在我们的关键基准测试中，以几何平均的方式，代码缩小了7％，速度提高了4％：</p><!-- * No calling convention impact.* No peanut butter associated with wrapping return values and caller branching.* All throwing functions were known in the type system, enabling more flexible code motion.* All throwing functions were known in the type system, giving us novel EH optimizations, like turning try/finally  blocks into straightline code when the try could not throw. --><ul><li>没有给调用约定带来影响；</li><li>没有与包装返回值和调用者分支相关联的“花生酱”开销；</li><li>所有抛出函数在类型系统中都是已知的，从而实现更灵活的代码移动；</li><li>所有抛出函数在类型系统中都是已知的，为我们提供了新颖的异常处理优化；例如在try无法抛出时将try/finally块转换为直接代码。</li></ul><!-- There were other aspects of exceptions that helped with performance.  I already mentioned that we didn't grovel thecallstack gathering up metadata as most exceptions systems do.  We left diagnostics to our diagnostics subsystem.Another common pattern that helped, however, was to cache exceptions as frozen objects, so that each `throw` didn'trequire an allocation: --><p>还有其他方面的异常也有助于提高性能。我已经提到过，我们并没有像大多数异常系统那样使用调用点来收集元数据，并将诊断留给了诊断子系统。然而，另一个常见的模式是将异常缓存为冻结对象，因此每次<code>throw</code>都不需要再次分配：</p><pre><code>const Exception retryLayout = new Exception();...throw retryLayout; </code></pre><!-- For systems with high rates of throwing and catching -- as in our parser, FRP UI framework, and other areas -- this wasimportant to good performance.  And this demonstrates why we couldn't simply take "exceptions are slow" as a given.--><p>对于具有高概率抛出和捕获的系统，例如我们的解析器，FRP UI框架和其他领域来说，这对于良好的性能至关重要，这也说明了为什么我们不能简单地将“异常很慢”视为理所应当的。</p><!-- ## Patterns --><h2 id="模式"><a href="#模式" class="headerlink" title="模式"></a>模式</h2><!-- A number of useful patterns came up that we embellished in our language and libraries. --><p>许多有用的模式被用于修饰我们的语言和库。</p><!-- ### Concurrency --><h3 id="并发"><a href="#并发" class="headerlink" title="并发"></a>并发</h3><!-- Back in 2007, I wrote [this note about concurrency and exceptions](http://joeduffyblog.com/2009/06/23/concurrency-and-exceptions/).  I wrote it mainly from the perspective of parallel,shared memory computations, however similar challenges exist in all concurrent orchestration patterns.  The basic issueis that the way exceptions are implemented assumes single, sequential stacks, with single failure modes.  In aconcurrent system, you have many stacks and many failure modes, where 0, 1, or many may happen "at once." --><p>早在2007年，我就写了<a href="http://joeduffyblog.com/2009/06/23/concurrency-and-exceptions/" target="_blank" rel="noopener">关于并发和异常的注解</a>的文章，虽然当时我主要是从并行共享内存计算的角度，但是所有并发编排模式都存在类似的挑战。基本的问题是单一顺序栈和单一故障模式的假设下来实现异常的方式，而在并发系统中，存在许多类型的栈和多种故障模式，可能会有0个，1个或多个模式“同时”发生。</p><!-- A simple improvement that Midori made was simply ensuring all `Exception`-related infrastructure handled cases withmultiple inner errors.  At least then a programmer wasn't forced to decide to toss away 1/N'th of the failureinformation, as most exceptions systems encourage today.  More than that, however, our scheduling and stack crawlinginfrastructure fundamentally knew about cactus-style stacks, thanks to our asynchronous model, and what to do with them. --><p>Midori所做的一个简单改进就是，确保所有与<code>Exception</code>相关的基础架构处理具有多个内部错误的情况，至少那时程序员没有被迫决定，像今天大多数异常系统所鼓励的那样，抛弃故障信息。然而，更重要的是，由于我们的异步模型以及其交互方式，调度和堆栈基础架构从根本了解cactus stack（仙人掌堆栈）。</p><!-- At first, we didn't support exceptions across asynchronous boundaries.  Eventually, however, we extended the ability todeclare `throws`, along with optional typed exceptions clauses, across asynchronous process boundaries.  This brought arich, typed programming model to the asynchronous actors programming model and felt like a natural extension.  Thisborrowed a page from CLU's successor, [Argus](https://en.wikipedia.org/wiki/Argus_(programming_language)). --><p>起初，我们并不支持跨异步边界的异常，但最终我们还是在跨异步进程边界上，扩展了声明<code>throws</code>以及可选类型化异常子句的能力。这为异步actor编程模型带来了丰富的类型化编程模型并感觉这就像是一种自然的扩展，而这一点从CLU的继任者<a href="http://t.cn/EIkpXHd" target="_blank" rel="noopener">Argus</a>那里获得的灵感。</p><!-- Our diagnostics infrastructure embellished this to give developers debugging experiences with full-blown cross-processcausality in their stack views.  Not only are stacks cactuses in a highly concurrent system, but they are often smearedacross process message passing boundaries.  Being able to debug the system this way was a big time-saver. --><p>我们的代码诊断基础设施对其进行了修饰，以便为开发者在栈视图中提供全面的跨进程因果关系调试体验。cactus stack不仅在高度并发的系统中存在，而且它们经常在进程消息传递边界上出现，以这种方式调试系统可以节省大量时间。</p><!-- ### Aborts --><h3 id="中止"><a href="#中止" class="headerlink" title="中止"></a>中止</h3><!-- Sometimes a subsystem needs to "get the hell out of Dodge."  Abandonment is an option, but only in response to bugs.And of course nobody in the process can stop it in its tracks.  What if we want to back out the callstack to some point,know that no-one on the stack is going to stop us, but then recover and keep going within the same process? --><p>有时一个子系统需要彻底从故障的环境中摆脱出来。因此Abandonment也是一种选择，但它也只应为应对bug而存在，并且在进程中没有什么可以阻止它。但如果我们想要退回到调用栈的某个点上，并已知栈中没有其他会阻止我们，然后恢复并继续在同一个进程中运行的话，那又会怎么样呢？</p><!-- Exceptions were close to what we wanted here.  But unfortunately, code on the stack can catch an in-flight exception,thereby effectively suppressing the abort.  We wanted something unsuppressable. --><p>对于这种情况，异常更接近于我们所想要的。但不幸的是，栈上的代码可以捕获正被抛出中的异常，从而有效地抑制中止的发生。因此，我们想要的是一些不可被抑制的构造。</p><!-- Enter aborts.  We invented aborts mainly in support of our UI framework which used Functional Reactive Programming(FRP), although the pattern came up in a few spots.  As an FRP recalculation was happening, it's possible that eventswould enter the system, or new discoveries got made, that invalidate the current recalculation.  If that happened --typically deep within some calculation whose stack was an interleaving of user and system code -- the FRP engine neededto quickly get back to top of its stack where it could safely begin a recalculation.  Thanks to all of that user codeon the stack being functionally pure, aborting it mid-stream was easy.  No errant side-effects would be left behind.And all engine code that was traversed was audited and hardened thoroughly, thanks to typed exceptions, to ensureinvariants were maintained. --><p>接着来聊聊中止（Abort）。我们发明中止主要是为了支持我们使用函数反应式编程（FRP）的UI框架，而FRP模式本身将会在未来的几篇文章中出现。当FRP重计算（recalculation）发生时，事件可能会进入系统，或者新的发现出现，从而使当前的重计算失效。通常这种情况都深埋在用户和系统代码交织的计算中，如果这种情况发生，FRP引擎需要快速返回其堆栈顶部，以便可以安全地开始重计算。由于堆栈上的所有用户代码都是纯函数式的，因此在流的中途进行中止很容易做到，并且不会留下任何导致错误的副作用。并且多亏了类型化异常，所有遍历的引擎代码都经过审核和彻底修改，以确保和维护不变性。</p><!-- The abort design borrows a page from the [capability playbook](http://joeduffyblog.com/2015/11/10/objects-as-secure-capabilities/).  First, we introduce a base type called`AbortException`.  It may be used directly or subclassed.  One of these is special: nobody can catch-and-ignore it.  Theexception is reraised automatically at the end of any catch block that attempts to catch it.  We say that suchexceptions are *undeniable*. --><p>中止的设计借用了<a href="/2018/11/18/midori/2-objects-as-secure-capabilities/">权能的设计</a>。首先，我们引入了一种名为<code>AbortException</code>的基础类型，它可以直接或以子类继承的方式使用。但它有一点却是特殊的：在他被捕获后就无法再被忽略掉，在尝试捕获它的任何catch块的末尾都会自动重新抛出该异常。因此，我们可以说这种异常是<em>不可否认</em>的。</p><!-- But someone's got to catch an abort.  The whole idea is to exit a context, not tear down the entire process a laabandonment.  And here's where capabilities enter the picture.  Here's the basic shape of `AbortException`: --><p>但总得有代码去捕捉abort。为此，整体的想法就是离开所处的上下文，而像Abandonment一样销毁整个进程，所以这就是权能发挥作用的地方。下面是<code>AbortException</code>的基本样子：</p><!--     public immutable class AbortException : Exception {        public AbortException(immutable object token);        public void Reset(immutable object token);        // Other uninteresting members omitted...    } -->    <pre><code>public immutable class AbortException : Exception {    public AbortException(immutable object token);    public void Reset(immutable object token);    // 省略掉其他不关心的成员...} </code></pre><!-- Notice that, at the time of construction, an immutable `token` is provided; in order to suppress the throw, `Reset` iscalled, and a matching `token` must be provided.  If the `token` doesn't match, abandonment occurs.  The idea is thatthe throwing and intended catching parties of an abort are usually the same, or at least in cahoots with one another,such that sharing the `token` securely with one another is easy to do.  This is a great example of objects asunforgeable capabilities in action.--><p>请注意，在调用构造函数时，需提供不可变的<code>token</code>，而为了抑制抛出的异常，需调用<code>Reset</code>，并且必须提供相匹配的<code>token</code>，一旦<code>token</code>不匹配，则会触发Abandonment。这里的想法是，abort的抛出和预期的捕获方通常是相同的，或者至少是彼此相关联，所以这样可以容易地安全地彼此共享<code>token</code>。所以这是对象作为不可伪造的权能，在实践中一个很好的例子。</p><!-- And yes, an arbitrary piece of code on the stack can trigger an abandonment, but such code could already do that bysimply dereferencing `null`.  This technique prohibits executing in the aborting context when it might not have beenready for it. --><p>是的，栈上的任意一段代码都可以触发Abandonment，但是这样的代码已经可以简单地通过解引用<code>null</code>来实现。该技术禁止在尚未准备好的情况下中止上下文中的代码执行。</p><!-- Other frameworks have similar patterns.  The .NET Framework has `ThreadAbortException` which is also undeniable unlessyou invoke `Thread.ResetAbort`; sadly, because it isn't capability-based, a clumsy combination of security annotationsand hosting APIs are required to stop unintended swallowing of aborts.  More often, this goes unchecked. --><p>其他框架具有类似的模式，.NET Framework中有<code>ThreadAbortException</code>类型的异常，除非你调用<code>Thread.ResetAbort</code>，否则它也是不可否认的。但遗憾的是，由于它不基于权能，因此需要使用安全注释和托管API的笨拙组合来阻止Abort被透明地意外处理。并且更常见的情况是，这是未经检查的异常。</p><!-- Thanks to exceptions being immutable, and the `token` above being immutable, a common pattern was to cache these guysin static variables and use singletons.  For example: --><p>由于异常是不可变的，并且上面的<code>token</code>也是不可变的，因此常见的模式是将这些它们缓存在静态变量中并以单例的方式使用。例如：</p><!--     class MyComponent {        const object abortToken = new object();        const AbortException abortException = new AbortException(abortToken);        void Abort() throws AbortException {            throw abortException;        }        void TopOfTheStack() {            while (true) {                // Do something that calls deep into some callstacks;                // deep down it might Abort, which we catch and reset:                let result = try ... else catch<AbortException>;                if (result.IsFailed) {                    result.Exception.Reset(abortToken);                }            }        }    } --><pre><code>class MyComponent {    const object abortToken = new object();    const AbortException abortException = new AbortException(abortToken);    void Abort() throws AbortException {        throw abortException;    }    void TopOfTheStack() {        while (true) {            // 调用函数，使得调用栈变得很深，             // 在调用栈深处位置可能会发生Abort，这里进行捕获并将其重置：             let result = try ... else catch&lt;AbortException&gt;;            if (result.IsFailed) {                result.Exception.Reset(abortToken);            }        }    }} </code></pre><!-- This pattern made aborts very efficient.  An average FRP recalculation aborted multiple times.  Remember, FRP was thebackbone of all UI in the system, so the slowness often attributed to exceptions was clearly not acceptable.  Evenallocating an exception object would have been unfortunate, due to the ensuing GC pressure. --><p>这种模式使Abort非常高效，平均下来FRP的重计算会发生多次Abort。请记住，FRP是系统中所有UI的主干部分，因此出现通常归因于异常的缓慢显然是不可接受的。由于随之而来的GC压力，即使异常对象的分配也是一件不幸的事。</p><!-- ### Opt-in "Try" APIs --><h3 id="可选的“Try”API"><a href="#可选的“Try”API" class="headerlink" title="可选的“Try”API"></a>可选的“Try”API</h3><!-- I mentioned a number of operations that abandoned upon failure.  That included allocating memory, performing arithmeticoperations that overflowed or divided-by-zero, etc.  In a few of these instances, a fraction of the uses are appropriatefor dynamic error propagation and recovery, rather than abandonment.  Even if abandonment is better in the common case. --><p>我提到过因故障而导致Abandonment的操作，这包括分配内存，算术溢出或除零等。在其中一些实例中，一部分适用于动态错误传播和恢复，而不是Abandonment，即使在通常情况下Abandonment是更好的选择。</p><!-- This turned out to be a pattern.  Not terribly common, but it came up.  As a result, we had a whole set of arithmeticAPIs that used a dataflow-style of propagation should overflow, NaN, or any number of things happen. --><p>结果证明这是一种模式，虽然不是非常普遍，但它确实出现了。因此，我们有使用数据流方式传播溢出、NaN或任何数量的可能发生的错误的一整套算术API。</p><!-- I also already mentioned a concrete instance of this earlier, which is the ability to `try new` an allocation, when OOMyields a recoverable error rather than abandonment.  This was super uncommon, but could crop up if you wanted to, say,allocate a large buffer for some multimedia operation. --><p>我之前已经提到了一个具体的实例，即当OOM产生可恢复的错误而不是Abandonment时，能够通过<code>try new</code>尝试进行新的分配。这种情况非常罕见，但如果你想为某些多媒体操作分配一个大缓冲区时，它可能会发挥作用。</p><!-- ### Keepers --><h3 id="守护者"><a href="#守护者" class="headerlink" title="守护者"></a>守护者</h3><!-- The last pattern I'll cover is called *the keeper pattern*. --><p>我将介绍的最后一个模式，叫做<em>守护者（keeper）模式</em>。</p><!-- In a lot of ways, the way recoverable exceptions are handled is "inside out."  A bunch of code is called, passingarguments down the callstack, until finally some code is reached that deems that state unacceptable.  In the exceptionsmodel, control flow is then propagated back up the callstack, unwinding it, until some code is found that handles theerror.  At that point if the operation is to be retried, the sequence of calls must be reissued, etc. --><p>在很多方面，处理可恢复异常的方式是“由内而外”的：调用一堆代码，在调用栈中传递参数，直到最后达到一些被认为是不可接受的状态。在异常模型中，控制流而后在调用栈中向上传播并展开，直到找到处理对应错误的代码。这个时候如果需要重试，则必须进行重新调用。</p><!-- An alternative pattern is to use a keeper.  The keeper is an object that understands how to recover from errors "insitu," so that the callstack needn't be unwound.  Instead, the code that would have otherwise thrown an exceptionconsults the keeper, who instructs the code how to proceed.  A nice aspect of keepers is that often, when done as aconfigured capability, surrounding code doesn't even need to know they exist -- unlike exceptions which, in our system,had to be declared as part of the type system.  Another aspect of keepers is that they are simple and cheap. --><p>另一种供选择的模式是使用keeper。keeper是一个知道如何在“原地”从错误中恢复的对象，因此就不再需要展开调用栈，而只需要抛出异常的代码询问keeper如何处理异常，keeper则告知代码如何继续执行。Keeper的很好的优势是，当作为配置的功能时，代码甚至不需要知道它们存在，在这一点上，不像我们的系统中必须被声明为类型系统的一部分的异常。除此之外，Keeper的另一个优势是它们简单而开销较低。</p><!-- Keepers in Midori could be used for prompt operations, but more often spanned asynchronous boundaries. --><p>Midori中的Keeper可以被用作快速操作，但更常见的用法是作为跨越异步边界的异常。</p><!-- The canonical example of a keeper is one guarding filesystem operations.  Accessing files and directories on a filesystem typically has failure modes such as: --><p>Keeper的规范化示例是保护文件系统的操作，访问文件系统上的文件和目录通常具有以下的一些故障模式：</p><!-- * Invalid path specification.* File not found.* Directory not found.* File in use.* Insufficient privileges.* Media full.* Media write-protected. --><ul><li>无效的路径规范</li><li>文件未找到</li><li>目录未找到</li><li>文件正在使用</li><li>权限不足</li><li>媒体已满</li><li>媒体写保护</li></ul><!-- One option is to annotate each filesystem API with a `throws` clause for each.  Or, like Java, to create an`IOException` hierarchy with each of these as subclasses.  An alternative is to use a keeper.  This ensures the overallapplication doesn't need to know or care about IO errors, permitting recovery logic to be centralized.  Such a keeperinterface might look like this: --><p>一种选择是使用<code>throws</code>子句为每个文件系统API进行注解，或者，像Java一样，创建一个<code>IOException</code>类型的层次结构，每种故障都作为其子类而存在。另一种方法是使用Keeper模式，它可以确保整个应用程序无需知道或关心IO错误，也允许集中式地恢复逻辑。这样的keeper接口可能像如下的形式：</p><pre><code>async interface IFileSystemKeeper {    async string InvalidPathSpecification(string path) throws;    async string FileNotFound(string path) throws;    async string DirectoryNotFound(string path) throws;    async string FileInUse(string path) throws;    async Credentials InsufficientPrivileges(Credentials creds, string path) throws;    async string MediaFull(string path) throws;    async string MediaWriteProtected(string path) throws;} </code></pre><!-- The idea is that, in each case, the relevant inputs are provided to the keeper when failure occurs.  The keeper is thenpermitted to perform an operation, possibly asynchronous, to recover.  In many cases, the keeper can optionally returnupdated arguments for the operation.  For example, `InsufficientPrivileges` could return alternative `Credentials` touse.  (Maybe the program prompted the user and she switched to an account with write access.)  In each case shown, thekeeper could throw an exception if it didn't want to handle the error, although this part of the pattern was optional. --><p>当发生故障时，在每种情况下相关输入被提供给keeper，然后keeper执行可能异步的操作以进行恢复,在许多情况下，keeper可以选择返回操作更新后的参数。例如，<code>InsufficientPrivileges</code>可以返回将会用到的<code>Credentials</code>（也许这时程序会提示用户，然后用户切换到具有写访问权限的帐户上）。在以上显式的每种故障情况中，一种可选的操作是，如果keeper不想处理该错误，则可以继续采取抛出异常的方式。</p><!-- Finally, I should note that Windows's Structured Exception Handling (SEH) system supports "continuable" exceptions whichare conceptually attempting to achieve this same thing.  They let some code decide how to restart the faultingcomputation.  Unfortunately, they're done using ambient handlers on the callstack, rather than first class objects inthe language, and so are far less elegant -- and significantly more error prone -- than the keepers pattern. --><p>最后，注意到的是Windows的结构化异常处理（SEH）系统支持“可持续（continuable）”的异常，这种异常在概念上试图实现同样的目的，它们让一些代码决定如何重新启动错误的计算。不幸的是，它们是在调用栈上使用环境处理程序上完成的，而不是作为语言中的一等对象，因此它远不如keeper模式那么优雅，而且更容易出错。</p><!-- ### Future Directions: Effect Typing --><h3 id="未来方向：效果类型化"><a href="#未来方向：效果类型化" class="headerlink" title="未来方向：效果类型化"></a>未来方向：效果类型化</h3><!-- Most people asked us about whether having `async` and `throws` as type system attributes bifurcated the entire universeof libraries.  The answer was "No, not really."  But it sure was painful in highly polymorphic library code. --><p>大多数人问我们是否将<code>async</code>和<code>throws</code>作为类型系统的属性分叉到整个库环境中，我对此的答案是，“不，这不是真的”。但在高度多态的库代码中这样肯定是痛苦的。</p><!-- The most jarring example was combinators like map, filter, sort, etc.  In those cases, you often have arbitraryfunctions and want the `async` and `throws` attributes of those functions to "flow through" transparently. --><p>最令人震惊的例子是组合类型，如map、filter和sort等。在这些情况下，你经常有任意函数，并希望这些函数的<code>async</code>和<code>throws</code>属性透明地“传播”。</p><!-- The design we had to solve this was to let you parameterize over effects.  For instance, here is a universal mappingfunction, `Map`, that propagates the `async` or `throws` effect of its `func` parameter: --><p>我们必须解决的设计是让你对效果进行参数化。例如，现在有一个通用的映射函数<code>Map</code>，它传播其<code>func</code>参数的<code>async</code>或<code>throws</code>效果：</p><pre><code>U[] Map&lt;T, U, effect E&gt;(T[] ts, Func&lt;T, U, E&gt; func) E {    U[] us = new U[ts.Length];    for (int i = 0; i &lt; ts.Length; i++) {        us[i] = effect(E) func(ts[i]);    }    return us;} </code></pre><!-- Notice here that we've got an ordinary generic type, `E`, except that its declaration is prefixed by the keyword `effect`.  We then use `E` symbolically in place of the effects list of the `Map` signature, in addition to using it inthe "propagate" position via `effect(E)` when invoking `func`.  It's a pretty trivial exercise in substitution,replacing `E` with `throws` and `effect(E)` with `try`, to see the logical transformation. --><p>请注意，我们有一个普通的泛型类型<code>E</code>，除了它的声明以关键字<code>effect</code>为前缀之外。然后除了在调用<code>func</code>时通过 <code>effect(E)</code>在“传播”位置使用它之外，我们象征性地使用<code>E</code>来代替<code>Map</code>签名的效果列表。这是一个非常简单的替代操作，用<code>effect(E)</code>替换<code>try</code>，以及用<code>E</code>替换<code>throws</code>，来看看逻辑的转换。</p><!-- A legal invocation might be: --><p>一种合法的调用如下：</p><pre><code>int[] xs = ...;string[] ys = try Map&lt;int, string, throws&gt;(xs, x =&gt; ...); </code></pre><!-- Notice here that the `throws` flows through, so that we can pass a callback that throws exceptions. --><p>请注意，这里的<code>throws</code>已经透明地传播下去，因此我们可以传递一个抛出异常的回调。</p><!-- As a total aside, we discussed taking this further, and allowing programmers to declare arbitrary effects.  I've[hypothesized about such a type system previously](http://joeduffyblog.com/2010/04/25/from-simple-annotations-for-analysis-to-effect-typing/).  We were concerned, however,that this sort of higher order programming might be gratuitously clever and hard to understand, no matter how powerful.The simple model above probably would've been a sweet spot and I think we'd have done it given a few more months. --><p>总而言之，我们将上述的讨论更进了一步，并允许程序员声明任意的效果。我<a href="http://joeduffyblog.com/2010/04/25/from-simple-annotations-for-analysis-to-effect-typing/" target="_blank" rel="noopener">以前曾经对这种类型的系统做过假设</a>，然而我所担心的是，无论它多么强大，这种高阶编程都可能是过度的精巧且难以理解。但上述的简单模型应该会是有意义的，我想如果多几个月时间，我们会将其加以实现。</p><!-- # Retrospective and Conclusions --><h1 id="回顾与总结"><a href="#回顾与总结" class="headerlink" title="回顾与总结"></a>回顾与总结</h1><!-- We've reached the end of this particular journey.  As I said at the outset, a relatively predictable and tame outcome.But I hope all that background helped to take you through the evolution as we sorted through the landscape of errors. --><p>我们已经来到了这段特殊旅程的终点，正如我在一开始所说的，最终是一个相对可预测和温和的结果。通过我们对错误的情况进行分类的基本原则，希望所有这些背景都能帮助你完成项目中错误处理的进化。</p><!-- In summary, the final model featured: --><p>总之，最终的模型具有以下的特点：</p><!-- * An architecture that assumed fine-grained isolation and recoverability from failure.* Distinguishing between bugs and recoverable errors.* Using contracts, assertions, and, in general, abandonment for all bugs.* Using a slimmed down checked exceptions model for recoverable errors, with a rich type system and language syntax.* Adopting some limited aspects of return codes -- like local checking -- that improved reliability. --><ul><li>一种假设细粒度隔离和从故障中的可恢复性的体系结构；</li><li>区分bug和可恢复的错误；</li><li>使用合约、断言、以及面向所有bug的更通用的Abandonment；</li><li>对于可恢复的错误，使用向下精简的，具有丰富的类型系统和语言语法的检查性异常模型；</li><li>采用返回码的有限的某些特性，比如局部检查，以提高可靠性。</li></ul><!-- And, though this was a multi-year journey, there were areas of improvement we were actively working on right up untilour project's untimely demise.  I classified them differently because we didn't have enough experience using them toclaim success.  I would have hoped we'd have tidied up most of them and shipped them if we ever got that far.  Inparticular, I'd have liked to put this one into the final model category: --><p>虽然这是一个多年的旅程，但我们一直致力于多个领域中积极努力的改进，直到我们的项目过早地夭折。因为我们没有使用它们的足够经验以宣称获得了成功，所以我对它们进行了不同的分类。如果能走得更远，我希望我们能将大部分内容整理出来并将它们交付出去。特别地，我想把下列原则放到最终模型的类别中：</p><!-- * Leveraging non-null types by default to eliminate a large class of nullability annotations. --><ul><li>默认情况下，利用非空类型可以消除大量的可空性注解。</li></ul><!-- Abandonment, and the degree to which we used it, was in my opinion our biggest and most successful bet with the ErrorModel.  We found bugs early and often, where they are easiest to diagnose and fix.  Abandonment-based errors outnumberedrecoverable errors by a ratio approaching 10:1, making checked exceptions rare and tolerable to the developer. --><p>Abandonment以及我们对它的使用程度，在我看来是在错误模型上最大和最成功的赌注。我们进程很早地发现bug，它们是最容易诊断和修复的，基于Abandonment的错误数量超过可恢复错误的比例接近10:1，因此，这样使得检查性异常很少出现并且可以被开发者所容忍。</p><!-- Although we never had a chance to ship this, we have since brought some of these lessons learned to other settings. --><p>虽然从未有机会将这些项目发布出来，但我们已经将其中的一些经验教训带到了其他的项目中去。</p><!-- During the Microsoft Edge browser rewrite from Internet Explorer, for example, we adopted abandonment in a few areas.The key one, applied by a Midori engineer, was OOM.  The old code would attempt to limp along as I described earlierand almost always did the wrong thing.  My understanding is that abandonment has found numerous lurking bugs, as was ourexperience regularly in Midori when porting existing codebases.  The great thing too is that abandonment is more of anarchitectural discipline that can be adopted in existing code-bases ranging in programming languages. --><p>例如，在从Internet Explorer重写Microsoft Edge浏览器期间，我们在一些区域采用了Abandonment机制。由Midori的工程师处理的一个关键领域是OOM。如前所述，旧代码会艰难地执行，并且几乎总是会出现错误，而Abandonment发现了许多潜伏的错误，就像我们常常在Midori中移植现有代码库时所经历的那样。更重要的是，Abandonment更多的是一种架构上的原则，可以在编程语言的现有代码库中所采用。</p><!-- The architectural foundation of fine-grained isolation is critical, however many systems have an informal notion of thisarchitecture.  A reason why OOM abandonment works well in a browser is that most browsers devote separate processes toindividual tabs already.  Browsers mimic operating systems in many ways and here too we see this playing out. --><p>细粒度隔离的架构基础至关重要，但许多系统都有这种架构的非正式概念。面向OOM的Abandonment机制在浏览器中运行良好的原因是，大多数浏览器已经将单独进程专门用于各个选项卡，浏览器正以多种方式模仿操作系统的行为，并且在这里我们也看到了同样的情况。</p><!-- More recently, we've been exploring proposals to bring some of this discipline -- [including contracts](https://www.youtube.com/watch?v=Hjz1eBx91g8) -- to C++.  There are also [concrete proposals](https://github.com/dotnet/roslyn/issues/119) to bring some of these features to C# too.  We are actively iterating on[a proposal that would bring some non-null checking to C#](https://github.com/dotnet/roslyn/issues/5032).  I have toadmit, I wish all of those proposals the best, however nothing will be as bulletproof as an entire stack written in thesame error discipline.  And remember, the entire isolation and concurrency model is essential for abandonment at scale. --><p>最近，我们一直在探索将这些，<a href="https://www.youtube.com/watch?v=Hjz1eBx91g8" target="_blank" rel="noopener">包括合约</a>在内的原则带到C++的提议，另外还有一些将这些功能带到C#中的<a href="https://github.com/dotnet/roslyn/issues/119" target="_blank" rel="noopener">具体提议</a>，我们还正在积极地迭代出<a href="https://github.com/dotnet/roslyn/issues/5032" target="_blank" rel="noopener">给C#带来一些非空检查的提议</a>。我不得不承认，我希望所有这些提案都是最好的，但是没有一个能像在同一个错误规则中写入整个栈那样具有防弹性。请记住，整个隔离和并发模型对于大规模Abandonment至关重要。</p><!-- I am hopeful that continued sharing of knowledge will lead to even more wide-scale adoption some of these ideas. --><p>我希望继续分享这些知识，使得能够更广泛地采用其中的一些想法。</p><!-- And, of course, I've mentioned that Go, Rust, and Swift have given the world some very good systems-appropriate errormodels in the meantime.  I might have some minor nits here and there, but the reality is that they're worlds beyondwhat we had in the industry at the time we began the Midori journey.  It's a good time to be a systems programmer! --><p>当然，我已经提到，Go、Rust和Swift在此期间为业界提供了一些非常好的有关系统的错误模型。我们的设计可能会在各个地方有一些微小的瑕疵，但现实情况是，这些语言（Go、Rust和Swift）所诞生的环境，已经超出了在我们开始Midori之旅时在行业中所具有的环境。所以现在正是成为一名系统程序员的好时机！</p><!-- Next time I'll talk more about the language.  Specifically, we'll see how Midori was able to tame the garbage collectorusing a magical elixir of architecture, language support, and libraries.  I hope to see you again soon! --><p>下一次我会更多地谈谈这门语言，具体来说将会看到Midori是如何利用架构、语言支持和库的万能钥匙来驯服垃圾收集器的（<em>译者注：实际上作者后文中并没有写到GC</em>）。我希望很快能和你们再次见面！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- 
[Midori](http://joeduffyblog.com/2015/11/03/blogging-about-midori/) was written in [an ahead-of-time compiled, type-safe
language base
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="操作系统" scheme="https://blog.zhangpf.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="Midori" scheme="https://blog.zhangpf.com/tags/Midori/"/>
    
      <category term="翻译" scheme="https://blog.zhangpf.com/tags/%E7%BF%BB%E8%AF%91/"/>
    
      <category term="编译" scheme="https://blog.zhangpf.com/tags/%E7%BC%96%E8%AF%91/"/>
    
      <category term="错误模型" scheme="https://blog.zhangpf.com/tags/%E9%94%99%E8%AF%AF%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="语言设计" scheme="https://blog.zhangpf.com/tags/%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>Midori博客系列翻译（4）——安全的原生代码</title>
    <link href="https://blog.zhangpf.com/2019/02/17/midori/4-safe-native-code/"/>
    <id>https://blog.zhangpf.com/2019/02/17/midori/4-safe-native-code/</id>
    <published>2019-02-17T06:46:00.000Z</published>
    <updated>2020-01-29T02:19:38.935Z</updated>
    
    <content type="html"><![CDATA[<!-- In my [first Midori post](http://joeduffyblog.com/2015/11/03/a-tale-of-three-safeties/), I described how safety was thefoundation of everything we did.  I mentioned that we built an operating system out of safe code, and yet stayedcompetitive with operating systems like Windows and Linux written in C and C++.  In many ways, system architectureplayed a key role, and I will continue discussing how in future posts.  But, at the foundation, an optimizing compilerthat often eeked out native code performance from otherwise "managed", type- and memory-safe code, was one of our mostimportant weapons.  In this post, I'll describe some key insights and techniques that were essential to our success. --><p>在我的<a href="/2018/10/24/midori/1-a-tale-of-three-safeties/">第一篇Midori文章</a>中，我描述了安全是我们所做的一切的基础。 我提到我们使用安全代码构建了操作系统，但仍然保持与使用C和C++编写的Windows和Linux等操作系统相比有竞争力的性能。 系统架构在许多方面发挥了关键作用，关于这点我将在未来的帖子中继续讨论。 但是，在系统的基石部分，一个从“托管”的，强类型的和内存安全的源代码中榨取原生代码性能的优化编译器，是我们最重要的武器之一。 在本篇文章中，我将就此描述一些对我们成功至关重要的关键思考和技巧。</p><!-- # Overview --><h1 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h1><!-- When people think of C#, Java, and related languages, they usually think of [Just-In-Time (JIT) compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation).  Especially back in the mid-2000s when Midori began.  ButMidori was different, using more C++-like [Ahead-Of-Time (AOT) compilation](https://en.wikipedia.org/wiki/Ahead-of-time_compilation) from the outset. --><p>当人们想到C#，Java及相关语言时，他们通常会想到<a href="https://en.wikipedia.org/wiki/Just-in-time_compilation" target="_blank" rel="noopener">Just-In-Time（JIT）编译</a>，尤其是在Midori开始的2000年代中期。 但是Midori却采用了不同的方式，它一开始便使用类<a href="https://en.wikipedia.org/wiki/Ahead-of-time_compilation" target="_blank" rel="noopener">C++的Ahead-Of-Time（AOT）</a> 编译方式。</p><!-- AOT compiling managed, [garbage collected code](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science))presents some unique challenges compared to C and C++.  As a result, many AOT efforts don't achieve parity with theirnative counterparts.  .NET's NGEN technology is a good example of this.  In fact, most efforts in .NET have exclusivelytargeted startup time; this is clearly a key metric, but when you're building an operating system and everything on top,startup time just barely scratches the surface. --><p>与C和C++相比，AOT编译托管的<a href="https://en.wikipedia.org/wiki/Garbage_collection_(computer_science" target="_blank" rel="noopener">垃圾回收代码</a>)存有一些独特的挑战。 因此，许多关于AOT的努力与其对应的原生代码相比并没有获得相匹配的性能，.NET的NGEN技术就是一个很好的例子。实际上，.NET中的大部分工作都专门针对于启动时间再进行优化，这显然是算的上一个关键指标。但是当构建一个操作系统及其之上的所有模块时，启动时间几乎仅仅只是皮毛而已。</p><!-- Over the course of 8 years, we were able to significantly narrow the gap between our version of C# and classical C/C++systems, to the point where basic code quality, in both size of speed dimensions, was seldom the deciding factor whencomparing Midori's performance to existing workloads.  In fact, something counter-intuitive happened.  The ability toco-design the language, runtime, frameworks, operating system, and the compiler -- making tradeoffs in one area to gainadvantages in other areas -- gave the compiler far more symbolic information than it ever had before about the program'ssemantics and, so, I dare say, was able to exceed C and C++ performance in a non-trivial number of situations. --><p>在8年的时间里，我们显著地缩小我们版本的C#与经典C/C++系统之间的差距，当同时在两个不同的速度维度上利用现有的任务上比较Midori的性能时，基本的代码质量都很少成为决定性因素。事实上，反直觉的事情发生了——协同设计语言、运行时、框架、操作系统和编译器的能力——使得在一个方面上的折衷可以在其他方面获得优势，这也为编译器提供了比以往更多的关于程序语义的符号信息。因此，我敢说，我们的语言能够在相当多的情况下超过C和C++的性能。</p><!-- Before diving deep, I have to put in a reminder.  The architectural decisions -- like [Async Everywhere](http://joeduffyblog.com/2015/11/19/asynchronous-everything/) and Zero-Copy IO (coming soon) -- had more to do with usnarrowing the gap at a "whole system" level.  Especially the less GC-hungry way we wrote systems code.  But thefoundation of a highly optimizing compiler, that knew about and took advantage of safety, was essential to our results. --><p>在深入探究编译器之前，我必须有所提醒。架构上的决策，如<a href="/2018/11/25/midori/3-asynchronous-everything/">无处不在的Async</a>和（即将推出的）Zero-Copy IO（零拷贝IO）文章，在“整个系统”级别缩小性能差距上与我们更加相关， 特别是在我们编写缺乏GC的系统代码时。但是，高度优化的编译器打下的基础，了解并利用其安全性，对我们的结果至关重要。</p><!-- I would also be remiss if I didn't point out that the world has made considerable inroads in this area alongside us.[Go](https://golang.org/) has straddled an elegant line between systems performance and safety.  [Rust](http://rust-lang.org/) is just plain awesome.  The [.NET Native](https://msdn.microsoft.com/en-us/vstudio/dotnetnative.aspx) and, related, [Android Runtime](https://en.wikipedia.org/wiki/Android_Runtime) projects have brought a nice taste of AOT to C# and Java in a morelimited way, as a "silent" optimization technique to avoid mobile application lag caused by JITting.  Lately, we'vebeen working on bringing AOT to a broader .NET setting with the [CoreRT project](https://github.com/dotnet/corert).Through that effort I hope we can bring some of the lessons learned below to a real-world setting.  Due to thedelicate balance around breaking changes it remains to be seen how far we can go.  It took us years to geteverything working harmoniously, measured in man-decades, however, so this transfer of knowledge will take time. --><p>如果我没有指出这个领域中其他团队与我们同时取得了巨大进展的工作，那么会是我的失职。这里的进展包括，<a href="https://golang.org/" target="_blank" rel="noopener">Go语言</a>跨越了系统性能和安全性之间的优雅界限；另外，<a href="http://rust-lang.org/" target="_blank" rel="noopener">Rust</a>在这方面也做的很棒。 <a href="https://msdn.microsoft.com/en-us/vstudio/dotnetnative.aspx" target="_blank" rel="noopener">.NET Native</a>和相关的<a href="https://en.wikipedia.org/wiki/Android_Runtime" target="_blank" rel="noopener">Android Runtime</a>项目以更强限制的方式为C#和Java带来了AOT编译，并将其作为一种“静默”优化技术，以避免在移动应用程序上由JIT引起的延迟。 最近，我们一直致力于通过<a href="https://github.com/dotnet/corert" target="_blank" rel="noopener">CoreRT项目</a>以广泛地方式将AOT带入.NET环境中，通过这项努力，我希望我们能够将下面的一些经验教训带到真实世界的编程环境中。 虽然由于突破性变化之间的微妙平衡，我们还能走多远还有待观察，然而，我们花了几年时间，数十人年的工作量，才能使所有事情和谐有序地工作，因此这种知识的转移需要一定的时间。</p><!-- First thing's first.  Let's quickly recap: What's the difference between native and managed code, anyway? --><p>首先，让我们简要地回顾一个问题：原生代码和托管代码之间到底有什么区别？</p><!-- ## What's the same --><h2 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h2><!-- I despise the false dichotomy "native and managed," so I must apologize for using it.  After reading this article, Ihope to have convinced you that it's a continuum.  C++ is [safer these days than ever before](https://github.com/isocpp/CppCoreGuidelines), and likewise, C# performant.  It's amusing how many of these lessons applydirectly to the work my team is doing on Safe C++ these days. --><p>由于我对将“原生和托管”进行错误的二分的方式有所鄙夷，因此我必须为使用这种说法而道歉。在阅读完这篇文章后，我希望能说服你它们其实是连续统一体。 <a href="https://github.com/isocpp/CppCoreGuidelines" target="_blank" rel="noopener">C++现在比以往任何时候都更安全</a>，同样地，C#表现地更好。 有趣的是，这些大量的教训直接适用于我们团队最近在安全C++上所做的工作。</p><!-- So let's begin by considering what's the same. --><p>所以让我们首先讨论它们的共同点。</p><!-- All the basic [dragon book](https://en.wikipedia.org/wiki/Principles_of_Compiler_Design) topics apply to managed asmuch as they do native code. --><p>所有<a href="https://en.wikipedia.org/wiki/Principles_of_Compiler_Design" target="_blank" rel="noopener">龙书</a>中的基础主题，只要适用于原生代码，对于托管代码都同样适用。</p><!-- In general, compiling code is a balancing act between, on one hand emitting the most efficient instruction sequencesfor the target architecture, to execute the program quickly; and on the other hand emitting the smallest encoding ofinstructions for the target architecture, to store the program compactly and effectively use the memory system on thetarget device.  Countless knobs exist on your favorite compiler to dial between the two based on your scenario.  Onmobile, you probably want smaller code, whereas on a multimedia workstation, you probably want the fastest. --><p>通常，编译代码的过程是下列两种行为的平衡，一方面为目标体系结构产生最高效的指令序列，以快速执行程序；另一方面，为目标体系结构发出最小的指令编码，以便在目标设备上紧凑且有效地使用存储系统来存储程序。 你最喜欢的编译器上存在无数个旋钮，可根据你的场景在两者之间进行切换。 或许在移动设备上，你需要更小的代码体积，而在多媒体工作站上，可能需要最快的代码。</p><!-- The choice of managed code doesn't change any of this.  You still want the same flexibility.  And the techniques you'duse to achieve this in a C or C++ compiler are by and large the same as what you use for a safe language. --><p>选择托管代码不会改变这种平衡关系的任何地方，你仍然需要相同的灵活性。 在C或C++编译器中用于实现此目的的技术基本上与用在安全语言的技术相同。</p><!-- You need a great [inliner](https://en.wikipedia.org/wiki/Inline_expansion).  You want [common subexpressionelimination (CSE)](https://en.wikipedia.org/wiki/Common_subexpression_elimination), [constant propagation and folding](https://en.wikipedia.org/wiki/Constant_folding), [strength reduction](https://en.wikipedia.org/wiki/Strength_reduction),and an excellent [loop optimizer](https://en.wikipedia.org/wiki/Loop_optimization).  These days, you probably want touse [static single assignment form (SSA)](https://en.wikipedia.org/wiki/Static_single_assignment_form), and some uniqueSSA optimizations like [global value numbering](https://en.wikipedia.org/wiki/Global_value_numbering) (although you needto be careful about working set and compiler throughput when using SSA everywhere).  You will need specialized machinedependent optimizers for the target architectures that are important to you, including [register allocators](https://en.wikipedia.org/wiki/Register_allocation).  You'll eventually want a global analyzer that does interproceduraloptimizations, link-time code-generation to extend those interprocedural optimizations across passes, a [vectorizer](https://en.wikipedia.org/wiki/Automatic_vectorization) for modern processors (SSE, NEON, AVX, etc.), and most definitely[profile guided optimizations (PGO)](https://en.wikipedia.org/wiki/Profile-guided_optimization) to inform all of theabove based on real-world scenarios. --><p>你需要一个出色的<a href="https://en.wikipedia.org/wiki/Inline_expansion" target="_blank" rel="noopener">内联器</a>，同时还需要<a href="https://en.wikipedia.org/wiki/Common_subexpression_elimination" target="_blank" rel="noopener">公共子表达式消除（CSE）</a>、<a href="https://en.wikipedia.org/wiki/Constant_folding" target="_blank" rel="noopener">常量传播和折叠</a>，<a href="https://en.wikipedia.org/wiki/Strength_reduction" target="_blank" rel="noopener">强度折减</a>以及出色的<a href="https://en.wikipedia.org/wiki/Loop_optimization" target="_blank" rel="noopener">循环优化器</a>。 目前，你可能还希望使用<a href="https://en.wikipedia.org/wiki/Static_single_assignment_form" target="_blank" rel="noopener">静态单赋值形式（SSA）</a>和一些独特的SSA优化方法，如<a href="https://en.wikipedia.org/wiki/Global_value_numbering" target="_blank" rel="noopener">全局值编号</a>（尽管在任何地方使用SSA时都需注意工作集和编译器的吞吐量）。 除此之外，对你来说很重要的目标体系结构，还需要专门的机器相关的优化器，包括<a href="https://en.wikipedia.org/wiki/Register_allocation" target="_blank" rel="noopener">寄存器分配器</a>等。 最后，你还需要一个进行过程间优化的全局分析器，以跨pass的方式来扩展过程间优化的链接时代码生成，和现代处理器的<a href="https://en.wikipedia.org/wiki/Automatic_vectorization" target="_blank" rel="noopener">矢量化器</a>（SSE，NEON，AVX等），以及可根据实际情况通知所有以上优化部件的<a href="https://en.wikipedia.org/wiki/Profile-guided_optimization" target="_blank" rel="noopener">配置文件引导优化（PGO）</a>。</p><!-- Although having a safe language can throw some interesting curveballs your way that are unique and interesting -- whichI'll cover below -- you'll need all of the standard optimizing compiler things. --><p>虽然拥有一种安全的语言可以以一种独特而有趣的方式带来意想不到的变化，对此我将在下面进行介绍，但你同时也需要所有以上这些标准的编译器优化方法。</p><!-- I hate to say it, but doing great at all of these things is "table stakes."  Back in the mid-2000s, we had to writeeverything by hand.  Thankfully, these days you can get an awesome off-the-shelf optimizing compiler like [LLVM](http://llvm.org) that has most of these things already battle tested, ready to go, and ready for you to help improve. --><p>虽然我讨厌这样说，但在所有这些优化技术上做得很好就是你的“筹码”。早在2000年代中期，我们不得不以手工方式实现所有这些方法。 值得庆幸的是，现在已经有现成的非常棒优化编译器，如<a href="http://llvm.org" target="_blank" rel="noopener">LLVM</a>，其中大部分的功能已经通过了测试，并随时准备帮助你进行改进。</p><!-- ## What's different --><h2 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h2><!-- But, of course, there are differences.  Many.  This article wouldn't be very interesting otherwise. --><p>但当然，托管和原生在许多地方是存在差异的，否则，这篇文章也不会很有趣。</p><!-- The differences are more about what "shapes" you can expect to be different in the code and data structures thrown atthe optimizer.  These shapes come in the form of different instruction sequences, logical operations in the code thatwouldn't exist in the C++ equivalent (like more bounds checking), data structure layout differences (like extra objectheaders or interface tables), and, in most cases, a larger quantity of supporting runtime data structures.--><p>差异更多体现在优化器所生成的代码和数据结构的“形状”上，这些“形状”如下的形式出现：不同的指令序列、不存在对应C++等价代码的逻辑操作（如更多的边界检查）、数据结构布局差异（如额外的对象头信息或接口表）以及最大的不同之处在于大量的支撑运行时的数据结构。</p><!-- Objects have "more to them" in most managed languages, compared to frugal data types in, say, C.  (Note that C++ datastructures are not nearly as frugal as you might imagine, and are probably closer to C# than your gut tells you.)  InJava, every object has a vtable pointer in its header.  In C#, most do, although structs do not.  The GC can imposeextra layout restrictions, such as padding and a couple words to do its book-keeping.  Note that none of this is reallyspecific to managed languages -- C and C++ allocators can inject their own words too, and of course, many C++ objectsalso carry vtables -- however it's fair to say that most C and C++ implementations tend to be more economical in theseareas.  In most cases, for cultural reasons more than hard technical ones.  Add up a few thousand objects in a heap,especially when your system is built of many small processes with isolated heaps, like Midori, and it adds up quickly. --><p>与C语言中的简单的数据类型相比，对象在大多数托管语言中都有“比实际更大的体积”（请注意：C++数据结构并不像你想象的那样简单，并且可能比你的直觉更接近于C#）。在Java中，每个对象的头部中都有一个vtable指针，而在C#中，尽管结构除外，但大多数情况也都是如此。 GC可以对布局施加额外的限制，例如填充和增加几个字节来于记录信息。 请注意，这些都不是特定于托管语言的特征——C和C++分配器也可能注入额外的信息，当然，许多C++对象也都带有vtable。但是可以公正地说，大多数C和C++在这些方面的实现上往往更经济。 在大多数情况下，出于文化不是技术原因，在堆中添加几千个对象，尤其是当系统由带有隔离堆空间的大量轻量级进程（如Midori中）组成时，内存消耗会快速增加。</p><!-- In Java, you've got a lot more virtual dispatch, because methods are virtual by default.  In C#, thankfully, methodsare non-virtual by default.  (We even made classes sealed by default.)  Too much virtual dispatch can totally screwinlining which is a critical optimization to have for small functions.  In managed languages you tend to have moresmall functions for two reasons: 1) properties, and 2) higher level programmers tend to over-use abstraction. --><p>另外，在Java中有更多的虚拟调度（virtual dispatch），因为默认情况下方法都是虚拟的，而在C#中，幸运的是，默认情况下方法是非虚拟的（我们甚至默认将类密封）。太多的虚拟调度完全可以内联化，而这对于小型函数来讲也是关键的优化技术。 在托管语言中，代码则倾向于具有更多的小型函数，其原因出于以下两个：1）属性值；2）更高级别的程序员倾向于过度使用抽象。</p><!-- Although it's seldom described this formally, there's an "ABI" ([Application Binary Interface](https://en.wikipedia.org/wiki/Application_binary_interface)) that governs interactions between code and the runtime.The ABI is where the rubber meets the road.  It's where things like calling conventions, exception handling, and, mostnotably, the GC manifest in machine code.  This is *not* unique to managed code!  C++ has a "runtime" and therfore anABI too.  It's just that it's primarily composed of headers, libraries like allocators, and so on, that are moretransparently linked into a program than with classical C# and Java virtual machines, where a runtime is non-negotiable(and in the JIT case, fairly heavy-handed).  Thinking of it this way has been helpful to me, because the isomorphismswith C++ suddenly become immediately apparent. --><p>尽管很少有这方面的形式化描述，但也存在“ABI”（<a href="https://en.wikipedia.org/wiki/Application_binary_interface" target="_blank" rel="noopener">应用程序二进制接口</a>）用于管理代码和运行时之间的交互。 ABI可以看做是车辆与道路之间接触部分，它包含了调用约定，异常处理，尤其是机器代码中的GC清单（manifest）。 虽然这<em>不是</em>托管代码所特有的概念，比如C++同样存在运行时和ABI， 只是C++中主要由对象头部信息，以及像分配器这样的库等组成，并以更透明的方式链接到程序，而不像传统的C#和Java的虚拟机，其运行时却是不可协商的（JIT情况下也是相当粗暴的）。 由于托管代码和C++的同构关系突然变得明显起来，因此以这种方式思考对我来说是有帮助的。</p><!-- The real biggie is array bounds checks.  A traditional approach is to check that the index is within the bounds of anarray before accessing it, either for loading or storing.  That's an extra field fetch, compare, and conditionalbranch.  [Branch prediction](https://en.wikipedia.org/wiki/Branch_predictor) these days is quite good, however it's justplain physics that if you do more work, you're going to pay for it.  Interestingly, the work we're doing with C++'s`array_view<T>` incurs all these same costs. --><p>真正的大问题是数组边界检查。传统的做法是在访问索引之前检查索引是否在数组范围内，以便加载或存储，因此存在一个额外的字段提取、比较和条件分支。当前的<a href="https://en.wikipedia.org/wiki/Branch_predictor" target="_blank" rel="noopener">分支预测</a>已经做的相当不错，但存在一个简单的道理是：如果你想完成更多的工作，则需要更大的开销。 有趣的是，我们在C++的<code>array_view&lt;T&gt;</code>上所做的工作会产生所有这些相同的开销。</p><!-- Related to this, there can be null checks where they didn't exist in C++.  If you perform a method dispatch on a nullobject pointer in C++, for example, you end up running the function anyway.  If that function tries to access `this`,it's bound to [AV](https://en.wikipedia.org/wiki/Segmentation_fault), but in Java and .NET, the compiler is required(per specification) to explicitly check and throw an exception in these cases, before the call even occurs.  Theselittle branches can add up too.  We eradicated such checks in favor of C++ semantics in optimized builds. --><p>与此相关的是，还需要对托管代码中不存在于C++的null类型进行检查。 例如，如果在C++中对空对象指针执行方法调度，则无论如何都会运行该函数，但如果该函数试图访问<code>this</code>，则它将会产生<a href="https://en.wikipedia.org/wiki/Segmentation_fault" target="_blank" rel="noopener">内存错误</a>，但在Java和.NET中，编译器需要（根据规范）甚至在调用发生之前，显式地进行检查，并在这些情况下抛出异常。这些细小的分支同样可以叠加到一起，而我们将根据optimized编译版本的C++语义消除这样的检查。</p><!-- In Midori, we compiled with overflow checking on by default.  This is different from stock C#, where you must explicitlypass the `/checked` flag for this behavior.  In our experience, the number of surprising overflows that were caught,and unintended, was well worth the inconvenience and cost.  But it did mean that our compiler needed to get really goodat understanding how to eliminate unnecessary ones. --><p>在Midori中，我们在编译中默认使用溢出检查，这与现行的C#中必须为此行为显式传递<code>/checked</code>标志有所不同。 根据我们的经验，捕获的意外和无意的溢出的数量，相比于带来不便和开销来讲是非常值得的，但这确实意味着我们的编译器需要非常善于理解如何消除不必要的检查。</p><!-- Static variables are very expensive in Java and .NET.  Way more than you'd expect.  They are mutable and so cannot bestored in the readonly segment of an image where they are shared across processes.  And my goodness, the amount oflazy-initialization checking that gets injected into the resulting source code is beyond belief.  Switching from`preciseinit` to `beforefieldinit` semantics in .NET helps a little bit, since the checks needn't happen on everyaccess to a static member -- just accesses to the static variable in question -- but it's still disgusting compared toa carefully crafted C program with a mixture of constant and intentional global initialization. --><p>另外，在Java和.NET中，静态变量开销非常大，甚至超出了你所期望的。 由于它们是可变的，因此不能存储于用作进程间共享的可执行文件只读段中。 而且令人惊讶的是，被注入到生成代码中的延迟初始化检查数量超出了你的想象。虽然从.NET的<code>preciseinit</code>切换到<code>beforefieldinit</code>语义能够带来一些帮助，因为每次访问静态成员都不需要再进行检查，而只需检查可能有问题的静态变量，但与精心设计的，具有常量和定向的全局初始化的C程序相比，它仍然具有很大的开销。</p><!-- The final major area is specific to .NET: structs.  Although structs help to alleviate GC pressure and hence are agood thing for most programs, they also carry some subtle problems.  The CLI specifies surprising behavior around theirinitialization, for example.  Namely if an exception happens during construction, the struct slot must remain zero-initialized.  The result is that most compilers make defensive copies.  Another example is that the compiler mustmake a defensive copy anytime you call a function on a readonly struct.  It's pretty common for structs to be copiedall over the place which, when you're counting cycles, hurts, especially since it often means time spent in `memcpy`.We had a lot of techniques for addressing this and, funny enough, I'm pretty sure when all was said and done, ourcode quality here was *better* than C++'s, given all of its RAII, copy constructor, destructor, and so on, penalties. --><p>最后的主要内容则特定于.NET：结构。 虽然结构有助于缓解GC压力，这对大多数程序来说都是好事，但它们也带有一些微妙的问题。 例如，CLI指定了初始化时一些奇怪的的行为，例如如果在构造期间发生异常，则结构的空位必须初始化为全零，其结果导致大多数编译器都需制作防御性的副本。 而另一个相似的例子是，只要调用readonly结构上的函数，编译器就必须制作防御性副本。 将结构复制到整个其他地方是非常常见的，因此当你计算程序执行周期数时则会出现偏差，尤其是因为它通常意味着花费在<code>memcpy</code>中的时间开销，我们提出了很多技术解决这个问题。有趣的是，当所有问题被提出和解决时，我很确定的一点是，考虑到它的RAII，复制构造函数，析构函数以及开销等所有因素，我们的代码质量是<em>优于</em>C++的。</p><!-- # Compilation Architecture --><h1 id="编译架构"><a href="#编译架构" class="headerlink" title="编译架构"></a>编译架构</h1><!-- Our architecture involved three major components: --><p>我们的架构涉及到三个主要的部件：</p><!-- * [C# Compiler](https://github.com/dotnet/roslyn): Performs lexing, parsing, and semantic analysis.  Ultimately  translates from C# textual source code into a [CIL](https://en.wikipedia.org/wiki/Common_Intermediate_Language)-based  [intermediate representation (IR)](https://en.wikipedia.org/wiki/Intermediate_language).* [Bartok](https://en.wikipedia.org/wiki/Bartok_(compiler)): Takes in said IR, does high-level MSIL-based analysis,  transformations, and optimizations, and finally lowers this IR to something a bit closer to a more concrete machine  representation.  For example, generics are gone by the time Bartok is done with the IR.* [Phoenix](https://en.wikipedia.org/wiki/Phoenix_(compiler_framework)): Takes in this lowered IR, and goes to town on  it.  This is where the bulk of the "pedal to the metal" optimizations happen.  The output is machine code. --><ul><li><a href="https://github.com/dotnet/roslyn" target="_blank" rel="noopener">C#编译器</a>：执行词法分析，语法解析和语义分析，最终将C#源代码文本转换为基于<a href="https://en.wikipedia.org/wiki/Common_Intermediate_Language" target="_blank" rel="noopener">CIL</a>的<a href="https://en.wikipedia.org/wiki/Intermediate_language" target="_blank" rel="noopener">中间表示（IR）</a>。</li><li><a href="http://t.cn/EVXZ53S" target="_blank" rel="noopener">Bartok</a>：以前一阶段的IR作为输入，进行基于MSIL的高层次分析，转换和优化，最后将IR削弱到更具体的机器表示形式。例如，当Bartok处理完IR时，泛型就已不存在了。</li><li><a href="http://t.cn/EVXZCoJ" target="_blank" rel="noopener">Phoenix</a>：以前一阶段的低层次IR作为输入进行频繁的处理。这就是大多数“把油门踩到底”的相关优化的地方，其最后输出的结果是机器代码。</li></ul><!-- The similarities here with Swift's compiler design, particularly [SIL](http://llvm.org/devmtg/2015-10/slides/GroffLattner-SILHighLevelIR.pdf), are evident.  The .NET Native project alsomirrors this architecture somewhat.  Frankly, most AOT compilers for high level languages do. --><p>这与Swift编译器的设计（特别是<a href="http://llvm.org/devmtg/2015-10/slides/GroffLattner-SILHighLevelIR.pdf" target="_blank" rel="noopener">SIL</a>）的相似之处是显而易见的，.NET Native项目也在某种程度上体现了这种架构。坦率地说，大多数高级语言的AOT编译器都是如此。</p><!-- In most places, the compiler's internal representation leveraged [static single assignment form (SSA)](https://en.wikipedia.org/wiki/Static_single_assignment_form).  SSA was preserved until very late in the compilation.This facilitated and improved the use of many of the classical compiler optimizations mentioned earlier. --><p>在大多数地方，编译器的内部表示都利用了<a href="https://en.wikipedia.org/wiki/Static_single_assignment_form" target="_blank" rel="noopener">静态单赋值形式（SSA）</a>，SSA形式一直保持到编译的最后阶段，这促成并改进了前面提到的许多经典编译器优化的使用。</p><!-- The goals of this architecture included: --><p>该架构的目标包括：</p><!-- * Facilitate rapid prototyping and experimentation.* Produce high-quality machine code on par with commerical C/C++ compilers.* Support debugging optimized machine code for improved productivity.* Facilitate profile-guided optimizations based on sampling and/or instrumenting code.* Suitable for self-host:    - The resulting compiled compiler is fast enough.    - It is fast enough that the compiler developers enjoy using it.    - It is easy to debug problems when the compiler goes astray. --><ul><li>促进快速原型设计和实验；</li><li>生成与商业C/C++编译器相当的高质量机器代码；</li><li>支持调试优化的机器代码以提高生产率；</li><li>基于采样和检测代码，提升配置文件引导优化；</li><li>适合自托管（self-host）：<ul><li>生成的编译器编译足够快；</li><li>足够快，因此编译器开发人员乐于使用它；</li><li>当编译器出现bug时，很容易对问题进行调试。</li></ul></li></ul><!-- Finally, a brief warning.  We tried lots of stuff.  I can't remember it all.  Both Bartok and Phoenix existed for yearsbefore I even got involved in them.  Bartok was a hotbed of research on managed languages -- ranging from optimizationsto GC to software transactional memory -- and Phoenix was meant to replace the shipping Visual C++ compiler.  So,anyway, there's no way I can tell the full story.  But I'll do my best. --><p>最后是一个简要警告：我们进行了很多方面的尝试，多到我已无法全部回忆起来。在我参与之前，Bartok和Phoenix都已存在多年： Bartok是托管语言研究的温床，其包含从优化到GC再到软件事务内存的各个方面；而Phoenix本来是作为取代已发布的Visual C++编译器而存在。 所以，无论怎样我都无法讲完所有的故事，但我会尽我所能。</p><!-- # Optimizations --><h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><!-- Let's go deep on a few specific areas of classical compiler optimizations, extended to cover safe code. --><p>让我们深入研究一些扩展到涵盖安全代码的经典编译器优化方法的特定领域。</p><!-- ## Bounds check elimination --><h2 id="边界检查消除"><a href="#边界检查消除" class="headerlink" title="边界检查消除"></a>边界检查消除</h2><!-- C# arrays are bounds checked.  So were ours.  Although it is important to eliminate superfluous bounds checks in regularC# code, it was even more so in our case, because even the lowest layers of the system used bounds checked arrays.  Forexample, where in the bowels of the Windows or Linux kernel you'd see an `int*`, in Midori you'd see an `int[]`. --><p>C#数组是带有边界检查的，而我们的也同样如此。 虽然在常规C#代码中消除多余的边界检查很重要，但在我们的环境下更需如此，因为即使是使用系统的最低层也需使用已检查边界的数组。 例如，在Windows或Linux内核的中，你看到的是<code>int*</code>类型，同样地，在Midori中你看到的是<code>int[]</code>类型。</p><!-- To see what a bounds check looks like, consider a simple example:--><p>为了说明边界检查长什么样子的，请考虑如下的一个简单例子：</p><pre><code>var a = new int[100];for (int i = 0; i &lt; 100; i++) {    ... a[i] ...;}</code></pre><!-- Here's is an example of the resulting machine code for the inner loop array access, with a bounds check: --><p>这是带有边界检查的循环内数组访问所生成的机器代码的示例：</p><!--     ; First, put the array length into EAX:    3B15: 8B 41 08        mov         eax,dword ptr [rcx+8]    ; If EDX >= EAX, access is out of bounds; jump to error:    3B18: 3B D0           cmp         edx,eax    3B1A: 73 0C           jae         3B28    ; Otherwise, access is OK; compute element's address, and assign:    3B1C: 48 63 C2        movsxd      rax,edx    3B1F: 8B 44 81 10     mov         dword ptr [rcx+rax*4+10h],r8d    ; ...    ; The error handler; just call a runtime helper that throws:    3B28: E8 03 E5 FF FF  call        2030 --><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">; 首先，将数组长度放入EAX：</span><br><span class="line">3B15: 8B 41 08        mov         eax,dword ptr [rcx+8]</span><br><span class="line">; 如果EDX&gt;=EAX，则访问超出范围，跳转到错误处理例程：</span><br><span class="line">3B18: 3B D0           cmp         edx,eax</span><br><span class="line">3B1A: 73 0C           jae         3B28</span><br><span class="line">; 否则，访问正常；计算元素的地址，并赋值：</span><br><span class="line">3B1C: 48 63 C2        movsxd      rax,edx</span><br><span class="line">3B1F: 8B 44 81 10     mov         dword ptr [rcx+rax*4+10h],r8d</span><br><span class="line">; ...</span><br><span class="line">; 错误处理，仅仅是调用抛出异常的运行时辅助程序：</span><br><span class="line">3B28: E8 03 E5 FF FF  call        2030</span><br></pre></td></tr></table></figure><!-- If you're doing this bookkeeping on every loop iteration, you won't get very tight loop code.  And you're certianly notgoing to have any hope of vectorizing it.  So, we spent a lot of time and energy trying to eliminate such checks. --><p>如果你在每次循环迭代中采用如此方式做记录，那么将不会得到非常紧凑的循环代码，而且你肯定不会有对其进行矢量化的可能性。 因此，我们花了很多时间和精力试图消除这样的检查。</p><!-- In the above example, it's obvious to a human that no bounds checking is necessary.  To a compiler, however, theanalysis isn't quite so simple.  It needs to prove all sorts of facts about ranges.  It also needs to know that `a`isn't aliased and somehow modified during the loop body.  It's surprising how hard this problem quickly becomes. --><p>在上面的例子中，对于人类来讲显然不需要进行边界检查，然而对于编译器而言，分析并不是那么简单。它需要证明关于范围的各种事实，还需要证明<code>a</code>在循环体中没有别名或以某种方式被修改过。 这个问题变得如此困难的速度之快，着实令人惊讶。</p><!-- Our system had multiple layers of bounds check eliminations. --><p>在我们的系统中，有多层次的边界检查消除方法。</p><!-- First it's important to note that CIL severely constraints an optimizer by being precise in certain areas.  For example,accessing an array out of bounds throws an `IndexOutOfRangeException`, similar to Java's `ArrayOutOfBoundsException`.And the CIL specifies that it shall do so at precisely the exception that threw it.  As we will see later on, ourerror model was more relaxed.  It was based fail-fast and permitted code motion that led to inevitable failureshappening "sooner" than they would have otherwise.  Without this, our hands would have been tied for much of what I'mabout to discuss. --><p>首先，重要的是要注意到CIL在某些区域中的精确处理的需求严格限制了优化器的作用。例如，访问数组越界会抛出<code>IndexOutOfRangeException</code>异常，类似于Java的<code>ArrayOutOfBoundsException</code>，并且CIL指定了它应该在抛出异常时准确地这样做。而正如在稍后将看到的那样，我们的错误模型更加轻松，它基于快速失败（fail-fast）和允许代码外提（code motion），使得不可避免的失败能够比其他系统更快地“发生”。 如果没有实现这一点，我们的双手将会被我即将讨论的大部分内容束缚在一起。</p><!-- At the highest level, in Bartok, the IR is still relatively close to the program input.  So, some simple patterns couldbe matched and eliminated.  Before lowering further, the [ABCD algorithm](http://www.cs.virginia.edu/kim/courses/cs771/papers/bodik00abcd.pdf) -- a straightforward value range analysis based onSSA -- then ran to eliminate even more common patterns using a more principled approach than pattern matching.  We werealso able to leverage ABCD in the global analysis phase too, thanks to inter-procedural length and control flow factpropagation. --><p>在最高的一层中，在Bartok中，IR仍然与程序输入相对接近，因此，可以匹配和消除一些简单的模式。 在进一步降低层次之前，<a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=993D087A3E775ED0476B70E3D7CDD52B?doi=10.1.1.118.9991&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">ABCD算法</a>（即基于SSA的直接值范围分析）能够采用比模式匹配原则性更强的方法来消除更常见的模式。 幸亏有程序间长度和控制流的事实传播，我们也能够在全局分析阶段使用上ABCD算法。</p><!-- Next up, the Phoenix Loop Optimizer got its hands on things.  This layer did all sorts of loop optimizations and, mostrelevant to this section, range analysis.  For example: --><p>接下来，Phoenix循环优化器开始发挥作用，这一层处理的是各种循环优化，以及和这一节内容最相关的范围分析。例如：</p><!-- * Loop materialization: this analysis actually creates loops.  It recognizes repeated patterns of code that would be  more ideally represented as loops, and, when profitable, rewrites them as such.  This includes unrolling hand-rolled  loops so that a vectorizer can get its hands on them, even if they might be re-unrolled later on.* Loop cloning, unrolling, and versioning: this analysis creates copies of loops for purposes of specialization.  That  includes loop unrolling, creating architectural-specific versions of a vectorized loop, and so on.* [Induction](https://en.wikipedia.org/wiki/Induction_variable) range optimization: this is the phase we are most  concerned with in this section.  It uses induction range analysis to remove unnecessary checks, in addition to doing  classical induction variable optimizations such as widening.  As a byproduct of this phase, bounds checks were  eliminated and coalesced by hoisting them outside of loops. --><ul><li>循环实体化（Loop Materialization）：该处分析实际上创建了循环。它能够识别理想地表示形式为循环的重复代码模式，并且在有性能收益时，将其重写为这样的形式。 这包括手动展开循环，以便矢量化器可以处理它们，即使它们以后可能重新被展开；</li><li>循环克隆，展开和版本控制：该处分析创建循环副本以用作特殊化处理，这包括循环展开，创建矢量化循环的体系结构特定版本等等；</li><li><a href="https://en.wikipedia.org/wiki/Induction_variable" target="_blank" rel="noopener">归纳（Induction）</a>范围优化：这是我们在本节中最关注的处理阶段。 除了进行经典的归纳变量优化（如加宽）之外，还使用归纳范围分析来删除不必要的检查。 作为该阶段的副产品，边界检查通过将它们外提到循环之外的方式被消除和合并。</li></ul><!-- This sort of principled analysis was more capable than what was shown earlier.  For example, there are ways to writethe earlier loop that can easily "trick" the more basic techniques discussed earlier: --><p>这种原则性强的分析比之前介绍的方法处理能力更强。 例如，存在如下的一些做法可以编写更早期的循环，达到“欺骗”前面讨论的更基础技术的目的：</p><!--     var a = new int[100];    // Trick #1: use the length instead of constant.    for (int i = 0; i < a.length; i++) {        a[i] = i;    }    // Trick #2: start counting at 1.    for (int i = 1; i <= a.length; i++) {        a[i-1] = i-1;    }    // Trick #3: count backwards.    for (int i = a.length - 1; i >= 0; i--) {        a[i] = i;    }    // Trick #4: don't use a for loop at all.    int i = 0;    next:    if (i < a.length) {        a[i] = i;        i++;        goto next;    } --><pre><code>var a = new int[100];// Trick #1：使用变量length而不是常量。for (int i = 0; i &lt; a.length; i++) {    a[i] = i;}// Trick #2：从1开始计数for (int i = 1; i &lt;= a.length; i++) {    a[i-1] = i-1;}// Trick #3：后向计数for (int i = a.length - 1; i &gt;= 0; i--) {    a[i] = i;}// Trick #4：根本不使用for循环。int i = 0;next:if (i &lt; a.length) {    a[i] = i;    i++;    goto next;} </code></pre><!-- You get the point.  Clearly at some point you can screw the optimizer's ability to do anything, especially if youstart doing virtual dispatch inside the loop body, where aliasing information is lost.  And obviously, things get moredifficult when the array length isn't known statically, as in the above example of `100`.  All is not lost, however,if you can prove relationships between the loop bounds and the array.  Much of this analysis requires special knowledgeof the fact that array lengths in C# are immutable. --><p>你已经发现了，很明显，在某些时候可以采用某种方式阻止优化器执行任何操作，特别是如果在循环体内进行虚拟调度，其中的别名信息也将丢失。 显然，当无法静态地知道数组长度时，如上面长度为<code>100</code>的例子所示，事情将会变得更加困难。但如果可以证明循环边界和数组之间的关系时，那所有信息都不会丢失。同时在C#中，大部分的分析方法都需要数组长度是不可变的。</p><!-- At the end of the day, doing a good job at optimizing here is the difference between this: --><p>不管怎么说，做好优化的区别体现在如下的原始版本：<!--     ; Initialize induction variable to 0:    3D45: 33 C0           xor         eax,eax    ; Put bounds into EDX:    3D58: 8B 51 08        mov         edx,dword ptr [rcx+8]    ; Check that EAX is still within bounds; jump if not:    3D5B: 3B C2           cmp         eax,edx    3D5D: 73 13           jae         3D72    ; Compute the element address and store into it:    3D5F: 48 63 D0        movsxd      rdx,eax    3D62: 89 44 91 10     mov         dword ptr [rcx+rdx*4+10h],eax    ; Increment the loop induction variable:    3D66: FF C0           inc         eax    ; If still < 100, then jump back to the loop beginning:    3D68: 83 F8 64        cmp         eax,64h    3D6B: 7C EB           jl          3D58    ; ...    ; Error routine:    3D72: E8 B9 E2 FF FF  call        2030 --><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">; 将归纳变量初始化为0：</span><br><span class="line">3D45: 33 C0           xor         eax,eax</span><br><span class="line">; 将边界值放入EDX：</span><br><span class="line">3D58: 8B 51 08        mov         edx,dword ptr [rcx+8]</span><br><span class="line">; 检查EAX是否仍然在边界之内，如果没有则跳转：</span><br><span class="line">3D5B: 3B C2           cmp         eax,edx</span><br><span class="line">3D5D: 73 13           jae         3D72</span><br><span class="line">; 计算元素的地址并将其存入其中：</span><br><span class="line">3D5F: 48 63 D0        movsxd      rdx,eax</span><br><span class="line">3D62: 89 44 91 10     mov         dword ptr [rcx+rdx*4+10h],eax</span><br><span class="line">; 增加循环归纳变量的值：</span><br><span class="line">3D66: FF C0           inc         eax</span><br><span class="line">; 如果变量值仍然小于100，则跳回到循环开始位置：</span><br><span class="line">3D68: 83 F8 64        cmp         eax,64h</span><br><span class="line">3D6B: 7C EB           jl          3D58</span><br><span class="line">; ...</span><br><span class="line">; 错误处理例程：</span><br><span class="line">3D72: E8 B9 E2 FF FF  call        2030</span><br></pre></td></tr></table></figure></p><!-- And the following, completely optimized, bounds check free, loop: --><p>以及如下完全优化且无边界检查的循环版本之间：</p><!--     ; Initialize induction variable to 0:    3D95: 33 C0           xor         eax,eax    ; Compute the element address and store into it:    3D97: 48 63 D0        movsxd      rdx,eax    3D9A: 89 04 91        mov         dword ptr [rcx+rdx*4],eax    ; Increment the loop induction variable:    3D9D: FF C0           inc         eax    ; If still < 100, then jump back to the loop beginning:    3D9F: 83 F8 64        cmp         eax,64h    3DA2: 7C F3           jl          3D97 --><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">; 将归纳变量初始化为0：</span><br><span class="line">3D95: 33 C0           xor         eax,eax</span><br><span class="line">; 计算元素的地址并将其存入其中：</span><br><span class="line">3D97: 48 63 D0        movsxd      rdx,eax</span><br><span class="line">3D9A: 89 04 91        mov         dword ptr [rcx+rdx*4],eax</span><br><span class="line">; 增加循环归纳变量的值：</span><br><span class="line">3D9D: FF C0           inc         eax</span><br><span class="line">; 如果变量值仍然小于100，则回到循环开始位置：</span><br><span class="line">3D9F: 83 F8 64        cmp         eax,64h</span><br><span class="line">3DA2: 7C F3           jl          3D97</span><br></pre></td></tr></table></figure><!-- It's amusing that I'm now suffering deja vu as we go through this same exercise with C++'s new `array_view<T>` type.Sometimes I joke with my ex-Midori colleagues that we're destined to repeat ourselves, slowly and patiently, over thecourse of the next 10 years.  I know that sounds arrogant.  But I have this feeling on almost a daily basis. --><p>有趣的是，如今当我们使用C++新的<code>array_view&lt;T&gt;</code>类型进行同样的实现时，遭遇到了似曾相识的问题。 有时候我和Midori的前同事开玩笑说，我们注定要在接下来的10年里慢慢地，有耐心地重复自己之前做过的事情。 我知道这听起来很傲慢，但却几乎每天都有这种感觉。</p><!-- ## Overflow checking --><h2 id="溢出检查"><a href="#溢出检查" class="headerlink" title="溢出检查"></a>溢出检查</h2><!-- As mentioned earlier, in Midori we compiled with checked arithmetic by default (by way of C#'s `/checked` flag).  Thiseliminated classes of errors where developers didn't anticipate, and therefore code correctly for, overflows.  Ofcourse, we kept the explicit `checked` and `unchecked` scoping constructs, to override the defaults when appropriate,but this was preferable because a programmer declared her intent. --><p>如前所述，Midori默认使用检查后运算（通过C#的<code>/checked</code>标志）进行编译，这消除了开发者没有预料到错误类别并因此能够为溢出问题正确的编码。当然，我们保留了显式的<code>checked</code>和<code>unchecked</code>的作用域构造方式，并在适当时覆盖默认值，这是一种更可取的方式，因为程序员声明了其对于程序的意图。</p><!-- Anyway, as you might expect, this can reduce code quality too. --><p>不管怎样，正如你所料到的，这也会降低代码的质量。</p><!-- For comparison, imagine we're adding two variables: --><p>为了便于比较，假设添加了以下两个变量：</p><pre><code>int x = ...;int y = ...;int z = x + y; </code></pre><!-- Now imagine `x` is in `ECX` and `y` is in `EDX`.  Here is a standard unchecked add operation: --><p>现假设<code>x</code>在<code>ECX</code>寄存器中，<code>y</code>在<code>EDX</code>中，这是一个标准的未经检查的求和操作：</p><pre><code>03 C2              add         ecx,edx </code></pre><!-- Or, if you want to get fancy, one that uses the `LEA` instruction to also store the result in the `EAX` register usinga single instruction, as many modern compilers might do: --><p>或者，如果你显得更花哨，那么使用单个<code>LEA</code>指令也可将结果存储在<code>EAX</code>寄存器中，正如许多现代编译器的做法：</p><pre><code>8D 04 11           lea         eax,[rcx+rdx] </code></pre><!-- Well, here's the equivalent code with a bounds check inserted into it: --><p>好了，下面是插入边界检查的等价代码：</p><pre><code>3A65: 8B C1              mov         eax,ecx3A67: 03 C2              add         eax,edx3A69: 70 05              jo          3A70; ...3A70: E8 B3 E5 FF FF     call        2028 </code></pre><!-- More of those damn conditional jumps (`JO`) with error handling routines (`CALL 2028`). --><p>大多是那些该死的条件跳转指令（<code>JO</code>）和错误处理例程（<code>CALL 2028</code>）。</p><!-- It turns out a lot of the analysis mentioned earlier that goes into proving bounds checks redundant also apply toproving that overflow checks are redundant.  It's all about proving facts about ranges.  For example, if you can provethat some check is [dominated by some earlier check](https://en.wikipedia.org/wiki/Dominator_(graph_theory)), and thatfurthermore that earlier check is a superset of the later check, then the later check is unnecessary.  If the oppositeis true -- that is, the earlier check is a subset of the later check, then if the subsequent block postdominates theearlier one, you might move the stronger check to earlier in the program. --><p>事实证明，前面提到的很多证明边界检查是多余的方法也同样适用于溢出检查，所有这一切都是为了证明关于范围的一些事实。例如，如果你可以证明某些检查<a href="http://t.cn/RqvLACK" target="_blank" rel="noopener">由某些前期检查所支配</a>，并且前期检查是后续检查的超集，那么后续检查就没有必要了。 如果相反的情况成立，也就是说，前期的检查是后续检查的子集，则可以将更强的检查移至程序的前面。</p><!-- Another common pattern is that the same, or similar, arithmetic operation happens multiple times near one another:--><p>另一种常见模式是相同或类似的算术运算在彼此相邻的位置多次发生：</p><pre><code>int p = r * 32 + 64;int q = r * 32 + 64 - 16; </code></pre><!-- It is obvious that, if the `p` assignment didn't overflow, then the `q` one won't either. --><p>很明显，如果<code>p</code>的赋值没有溢出，那么<code>q</code>的赋值也不会溢出。</p><!-- There's another magical phenomenon that happens in real world code a lot.  It's common to have bounds checks andarithmetic checks in the same neighborhood.  Imagine some code that reads a bunch of values from an array: --><p>在真实世界中的代码可能发生了另一种神奇的现象——在同一邻域中进行边界检查和算术检查是相当常见的。 假设有从数组中读取一堆值的如下代码：</p><!--     int data0 = data[dataOffset + (DATA_SIZE * 0)];    int data1 = data[dataOffset + (DATA_SIZE * 1)];    int data2 = data[dataOffset + (DATA_SIZE * 2)];    int data3 = data[dataOffset + (DATA_SIZE * 3)];    .. and so on ... --><pre><code>int data0 = data[dataOffset + (DATA_SIZE * 0)];int data1 = data[dataOffset + (DATA_SIZE * 1)];int data2 = data[dataOffset + (DATA_SIZE * 2)];int data3 = data[dataOffset + (DATA_SIZE * 3)];... 等等 ... </code></pre><!-- Well C# arrays cannot have negative bounds.  If a compiler knows that `DATA_SIZE` is sufficiently small that anoverflowed computation won't wrap around past `0`, then it can eliminate the range check in favor of the bounds check. --><p>良定义的C#数组不存在值为负的边界，如果编译器知道<code>DATA_SIZE</code>足够小以至于可能发生溢出的计算不会小于<code>0</code>，那么它可以消除边界检查所需的范围检查。</p><!-- There are many other patterns and special cases you can cover.  But the above demonstrates the power of a really goodrange optimizer that is integrated with loops optimization.  It can cover a wide array of scenarios, array bounds andarithmetic operations included.  It takes a lot of work, but it's worth it in the end. --><p>还有许多其他模式和特殊情况可以涵盖，但是前面以及展示了与循环优化集成的非常好的范围优化器所展现的强大功能。 它可以覆盖各种场景，包括数组边界和算术运算。 虽然花费了大量的工作，但最终还是值得的。</p><!-- ## Inlining --><h2 id="内联"><a href="#内联" class="headerlink" title="内联"></a>内联</h2><!-- For the most part, [inlining](https://en.wikipedia.org/wiki/Inline_expansion) is the same as with true native code.  Andjust as important.  Often more important, due to C# developers' tendency to write lots of little methods (like propertyaccessors).  Because of many of the topics throughout this article, getting small code can be more difficult than inC++ -- more branches, more checks, etc. -- and so, in practice, most managed code compilers inline a lot less thannative code compilers, or at least need to be tuned very differently.  This can actually make or break performance. --><p>对于大多数的部分，<a href="https://en.wikipedia.org/wiki/Inline_expansion" target="_blank" rel="noopener">内联</a>与真正的原生代码是一样的。内联也是同等重要，并且由于C#开发者倾向于编写许多微小方法（如属性访问器等），所以显得其更加重要。 由于本文中的许多主题，获取小代码可能比在C++中更加困难——具有更多分支和检查等。因此，在实践中，大多数的托管代码编译器比本机代码编译器所内联的要少得多，或者至少需要以非常不同的方式进行调整，这实际上对性能起到了决定性作用。</p><!-- There are also areas of habitual bloat.  The way lambdas are encoded in MSIL is unintelligable to a naive backendcompiler, unless it reverse engineers that fact.  For example, we had an optimization that took this code: --><p>还有一些习惯性膨胀的领域，例如对于原生后端编译器来讲，lambda算子在MSIL中编码的方式是不可理解的，除非其对此事实进行逆向工程。</p><p>例如，对如下的代码进行优化：</p><pre><code>void A(Action a) {    a();}void B() {    int x = 42;    A(() =&gt; x++);    ...} </code></pre><!-- and, after inlining, was able to turn B into just: --><p>在进行内联之后，能够将B转变为如下的形式：</p><pre><code>void B() {    int x = 43;    ...} </code></pre><!-- That `Action` argument to `A` is a lambda and, if you know how the C# compiler encodes lambdas in MSIL, you'llappreciate how difficult this trick was.  For example, here is the code for B: --><p>类型为<code>Action</code>的参数<code>A</code>是一个lambda算子，并且如果你知道C#编译器是如何在MSIL中编码lambda的，那么你将明白该技巧有多么的困难。 例如，如下是B的MSIL代码：</p><!--     .method private hidebysig instance void        B() cil managed    {        // Code size       36 (0x24)        .maxstack  3        .locals init (class P/'<>c__DisplayClass1' V_0)        IL_0000:  newobj     instance void P/'<>c__DisplayClass1'::.ctor()        IL_0005:  stloc.0        IL_0006:  nop        IL_0007:  ldloc.0        IL_0008:  ldc.i4.s   42        IL_000a:  stfld      int32 P/'<>c__DisplayClass1'::x        IL_000f:  ldarg.0        IL_0010:  ldloc.0        IL_0011:  ldftn      instance void P/'<>c__DisplayClass1'::'<B>b__0'()        IL_0017:  newobj     instance void [mscorlib]System.Action::.ctor(object,                                                                      native int)        IL_001c:  call       instance void P::A(class [mscorlib]System.Action)        IL_0021:  nop        IL_0022:  nop        IL_0023:  ret    } --><pre><code>.method private hidebysig instance void    B() cil managed{    // 代码大小       36 (0x24)    .maxstack  3    .locals init (class P/&apos;&lt;&gt;c__DisplayClass1&apos; V_0)    IL_0000:  newobj     instance void P/&apos;&lt;&gt;c__DisplayClass1&apos;::.ctor()    IL_0005:  stloc.0    IL_0006:  nop    IL_0007:  ldloc.0    IL_0008:  ldc.i4.s   42    IL_000a:  stfld      int32 P/&apos;&lt;&gt;c__DisplayClass1&apos;::x    IL_000f:  ldarg.0    IL_0010:  ldloc.0    IL_0011:  ldftn      instance void P/&apos;&lt;&gt;c__DisplayClass1&apos;::&apos;&lt;B&gt;b__0&apos;()    IL_0017:  newobj     instance void [mscorlib]System.Action::.ctor(object,                                                                  native int)    IL_001c:  call       instance void P::A(class [mscorlib]System.Action)    IL_0021:  nop    IL_0022:  nop    IL_0023:  ret} </code></pre><!-- To get the magic result required constant propagating the `ldftn`, recognizing how delegate construction works(`IL_0017`), leveraging that information to inline `B` and eliminate the lambda/delegate altogether, and then, againmostly through constant propagation, folding the arithmetic into the constant `42` initialization of `x`.  I alwaysfound it elegant that this "fell out" of a natural composition of multiple optimizations with separate concerns. --><p>为了获得这种神奇结果，需要常量传播<code>ldftn</code>，识别委托构造的工作方式（<code>IL_0017</code>），利用该信息内联<code>B</code>并同时消除了lambda和delegate，然后再次主要通过常量传播，将算术折叠成利用常量<code>42</code>初始化<code>x</code>。 我总觉得这种多种不同考量的优化的自然组合的方式是非常“优雅”的。</p><!-- As with native code, profile guided optimization made our inlining decisions far more effective. --><p>与原生代码一样，配置文件引导优化使我们的内联决策更加有效。</p><!-- ## Structs --><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><!-- CLI structs are almost just like C structs.  Except they're not.  The CLI imposes some semantics that incur overheads.These overheads almost always manifest as excessive copying.  Even worse, these copies are usually hidden from yourprogram.  It's worth noting, because of copy constructors and destructors, C++ also has some real issues here, ofteneven worse than what I'm about to describe. --><p>CLI的结构几乎和C的结构很相似，但它们却不完全一样。CLI强加了一些会产生开销的语义，而这些开销几乎总是表现为过度的复制，更糟糕的是，副本通常隐藏于程序中。 值得注意的是，由于复制构造函数和析构函数，C++在这方面也存在一些实际问题，通常甚至比我将要描述的更加糟糕。</p><!-- Perhaps the most annoying is that initializing a struct the CLI way requires a defensive copy.  For example, considerthis program, where the initialzer for `S` throws an exception: --><p>但也许最令人讨厌的是，CLI初始化结构的方式需要防御性的副本。 例如，考虑如下的程序，其中<code>S</code>的初始化方法抛出了异常：</p><pre><code>class Program {    static void Main() {        S s = new S();        try {            s = new S(42);        }        catch {            System.Console.WriteLine(s.value);        }    }}struct S {    public int value;    public S(int value) {        this.value = value;        throw new System.Exception(&quot;Boom&quot;);    }} </code></pre><!-- The program behavior here has to be that the value `0` is written to the console.  In practice, that means that theassignment operation `s = new S(42)` must first create a new `S`-typed slot on the stack, construct it, and *then* andonly then copy the value back over the `s` variable.  For single-`int` structs like this one, that's not a huge deal.For large structs, that means resorting to `memcpy`.  In Midori, we knew what methods could throw, and which could not,thanks to our error model (more later), which meant we could avoid this overhead in nearly all cases. --><p>此处的程序预期行为是：必须将值<code>0</code>写入控制台中。 实际上，这意味着赋值操作<code>s = new S(42)</code>必须首先在栈空间上创建一个新的<code>S</code>类型的slot并对其进行构造，<em>然后</em>仅仅将它的值赋给<code>s</code>变量。 虽然对于像这样的单个<code>int</code>的结构来讲并不是什么大问题，但对于大型的结构，这意味着要使用<code>memcpy</code>函数进行赋值。而在Midori中，我们知道哪些方法可以抛出异常，哪些方法无法抛出，而这些都要归功于我们的错误模型（将在后面进行介绍），所以也意味着我们几乎可以在所有的情况下避免这样的开销。</p><!-- Another annoying one is the following: --><p>另一个令人讨厌的地方出现在如下形式的代码中：</p><pre><code>struct S {    // ...    public int Value { get { return this.value; } }}static readonly S s = new S(); </code></pre><!-- Every single time we read from `s.Value`: --><p>每次从<code>s.Value</code>读取值：</p><pre><code>int x = s.Value; </code></pre><!-- we are going to get a local copy.  This one's actually visible in the MSIL.  This is without `readonly`: --><p>都将得到数据的局部副本。 它实际上在MSIL中可见，且没有<code>readonly</code>关键字：</p><pre><code>ldsflda    valuetype S Program::scall       instance int32 S::get_Value() </code></pre><!-- And this is with it: --><p>也就是如下的形式：</p><pre><code>ldsfld     valuetype S Program::sstloc.0ldloca.s   V_0call       instance int32 S::get_Value() </code></pre><!-- Notice that the compiler elected to use `ldsfld` followed by `lodloca.s`, rather than loading the address directly,by way of `ldsflda` in the first example.  The resulting machine code is even nastier.  I also can't pass the structaround by-reference which, as I mention later on, requires copying it and again can be problematic. --><p>请注意，编译器选择使用<code>ldsfld</code>并由<code>lodloca.s</code>指令紧随其后，而不是通过第一个示例中的<code>ldsflda</code>指令直接加载地址，因此其生成的机器代码将会更加糟糕。这里我们也无法通过引用传递结构体，正如我后面将要提到的那样，它需要数据的复制并且可能再次出现问题。</p><!-- We solved this in Midori because our compiler knew about methods that didn't mutate members.  All statics were immutableto begin with, so the above `s` wouldn't need defensive copies.  Alternatively, or in addition to this, the struct couldhave beem declared as `immutable`, as follows: --><p>由于我们的编译器知道方法不会改变成员的值，因此在Midori中，我们对此问题进行了解决。因为所有的静态值都是不可变（<code>immutable</code>）的，所以上面的<code>s</code>不再需要防御性的副本。 除此之外的另一种方式，结构体也同样可声明为<code>immutable</code>，如下所示：</p><!--     immutable struct S {        // As above ...    }  --><pre><code>immutable struct S {    // 如上 ...}  </code></pre><!-- Or because all static values were immutable anyway.  Alternatively, the properties or methods in question could havebeen annotated as `readable` meaning that they couldn't trigger mutations and hence didn't require defensive copies. --><p>或者可采用另一种方式：由于所有静态值在任何情况下都是不可变的，因此存有疑问的属性或方法可注解为<code>readable</code>，这意味着它们不能被改变，所以也就不再需要防御性的副本。</p><!-- I mentioned by-reference passing.  In C++, developers know to pass large structures by-reference, either using `*` or`&`, to avoid excessive copying.  We got in the habit of doing the same.  For example, we had `in` parameters, as so: --><p>我在前面已经提到了引用传递。 在C++中，开发者知道使用<code>*</code>或<code>&amp;</code>的方式，通过引用传递大型的结构，从而避免过度的复制。我们也养成了同样做法的习惯，例如，我们有名为<code>in</code>的参数：</p><!--     void M(in ReallyBigStruct s) {        // Read, but don't assign to, s ...    } -->   <pre><code>void M(in ReallyBigStruct s) {    // 可读但不可写s的值 ...} </code></pre><!-- I'll admit we probably took this to an extreme, to the point where our APIs suffered.  If I could do it all over again,I'd go back and eliminate the fundamental distinction between `class` and `struct` in C#.  It turns out, pointers aren'tthat bad after all, and for systems code you really do want to deeply understand the distinction between "near" (value)and "far" (pointer).  We did implement what amounted to C++ references in C#, which helped, but not enough.  More onthis in my upcoming deep dive on our programming language. --><p>我承认我们可能把这个问题推向极端，直到对我们的API产生了影响。如果我能从头再来一次，我会回过头来消除C#中<code>class</code>和<code>struct</code>之间的根本区别。 事实证明，指针毕竟没那么糟糕，并且对于系统代码而言，你真的需要深入理解“近”（值）和“远”（指针）之间的区别。 我们确实在C#中实现了C++引用的功能，这能够带来一些帮助，但还不够。 关于此更多内容将出现在未来对编程语言的深入研究中。</p><!-- ## Code size --><h2 id="代码体积"><a href="#代码体积" class="headerlink" title="代码体积"></a>代码体积</h2><!-- We pushed hard on code size.  Even more than some C++ compilers I know. --><p>我们还努力推动减少代码体积，在此花费的努力甚至比我所知道的一些C++编译器还要多。</p><!-- A generic instantiation is just a fancy copy-and-paste of code with some substitutions.  Quite simply, that means anexplosion of code for the compiler to process, compared to what the developer actually wrote.  [I've covered many of theperformance challenges with generics in the past.](http://joeduffyblog.com/2011/10/23/on-generics-and-some-of-the-associated-overheads/)  A major problem there is thetransitive closure problem.  .NET's straightforward-looking `List<T>` class actually creates 28 types in its transitiveclosure!  And that's not even speaking to all the methods in each type.  Generics are a quick way to explode code size. --><p>泛型的实例化只是一些带有替换的代码复制和粘贴。 很明显这意味着与开发人员实际编写的代码量相比，编译器要处理的代码数量将会激增。<a href="http://joeduffyblog.com/2011/10/23/on-generics-and-some-of-the-associated-overheads/" target="_blank" rel="noopener">我在之前的文章中已经介绍了泛型的许多性能挑战</a>，一个主要问题就是传递闭包问题。.NET中直观上的<code>List&lt;T&gt;</code>类实际上在其传递闭包中创建了多大28种类型！ 而且甚至这还没把每种类型的所有方法包括进去。因此，泛型是一种使代码体积爆炸的快速方法。</p><!-- I never forgot the day I refactored our LINQ implementation.  Unlike in .NET, which uses extension methods, we made allLINQ operations instance methods on the base-most class in our collection type hierarchy.  That meant 100-ish nestedclasses, one for each LINQ operation, *for every single collection instantiated*!  Refactoring this was an easy way forme to save over 100MB of code size across the entire Midori "workstation" operating system image.  Yes, 100MB! --><p>我永远不会忘记重构实现LINQ的那段日子， 与在.NET中使用扩展方法不同，我们在集合类型层次结构中的最底层基类上创建了所有LINQ操作的实例方法。 <em>对于实例化的每个集合</em>，这意味着大约100个嵌套类！每个LINQ操作对应于其中一个。对其进行重构是一种可以在整个Midori“工作站”操作系统文件中节省超过100MB空间的简单方法。 是的没错，节省了100MB！</p><!-- We learned to be more thoughtful about our use of generics.  For example, types nested inside an outer generic areusually not good ideas.  We also aggressively shared generic instantiations, even more than [what the CLR does](http://blogs.msdn.com/b/joelpob/archive/2004/11/17/259224.aspx).  Namely, we shared value type generics, where theGC pointers were at the same locations.  So, for example, given a struct S: --><p>我们学会了更加周到地使用泛型，比如说在外部泛型内的嵌套类型通常不是什么好主意。除此之外还积极地共享通用实例，甚至比<a href="http://blogs.msdn.com/b/joelpob/archive/2004/11/17/259224.aspx" target="_blank" rel="noopener">CLR所做</a>的更多。 也就是说，我们共享了GC指针位于相同位置的值类型的泛型。所以说，如果给定一个结构S：</p><pre><code>struct S {    int Field;} </code></pre><!-- we would share the same code representation of `List<int>` with `List<S>`.  And, similarly, given: --><p>那么<code>List&lt;S&gt;</code>将共享与<code>List&lt;int&gt;</code>相同的代码表示。 并且同样地，假定有如下的结构：</p><pre><code>struct S {    object A;    int B;    object C;} struct T {    object D;    int E;    object F;} </code></pre><!-- we would share instantiations between `List<S>` and `List<T>`. --><p>那么<code>List&lt;S&gt;</code>和<code>List&lt;T&gt;</code>之间将共享实例。</p><!-- You might not realize this, but C# emits IL that ensures `struct`s have `sequential` layout: --><p>另外，你可能没有意识到的一点是，C#生成了确保<code>struct</code>结构具有<code>sequential</code>特性布局的IL：</p><pre><code>.class private sequential ansi sealed beforefieldinit S    extends [mscorlib]System.ValueType{    ...} </code></pre><!-- As a result, we couldn't share `List<S>` and `List<T>` with some     `List<U>`: --><p>结果是，我们不能与一些假设的<code>List&lt;U&gt;</code>共享<code>List&lt;S&gt;</code>和<code>List&lt;T&gt;</code>：</p><pre><code>struct U {    int G;    object H;    object I;} </code></pre><!-- For this, among other reasons -- like giving the compiler more flexibility around packing, cache alignment, and so on-- we made `struct`s `auto` by default in our language.  Really, `sequential` only matters if you're doing unsafe code,which, in our programming model, wasn't even legal. --><p>为此，除了其他原因例如让编译器在打包，缓存对齐等方面具有更大的灵活性之外，我们在语言的<code>struct</code>中默认使用<code>auto</code>。 实际上，<code>sequential</code>只对你进行不安全代码时很重要，而在我们的编程模型中，这样的代码甚至都是不合法的。</p><!-- We did not support reflection in Midori.  In principle, we had plans to do it eventually, as a purely opt-in feature.In practice, we never needed it.  What we found is that code generation was always a more suitable solution.  We shavedoff at least 30% of the best case C# image size by doing this.  Significantly more if you factor in systems where thefull MSIL is retained, as is usually the case, even for NGen and .NET AOT solutions. --><p>我们没有在Midori中支持反射机制。原则上，我们最终计划将其作为纯粹的可选功能项，而在实践中从来都不需要它。 我们发现代码生成始终都是更合适的解决方案，通过这种做法，对于C#的文件体积，我们在最佳情况下至少减少了30%。 如果你考虑的是保留完整MSIL的系统，正如通常情况下的做法，即使对于NGen和.NET的AOT解决方案，将能够进一步减少代码的体积。</p><!-- In fact, we removed significant pieces of `System.Type` too.  No `Assembly`, no `BaseType`, and yes, even no `FullName`.The .NET Framework's mscorlib.dll contains about 100KB of just type names.  Sure, names are useful, but our eventingframework leveraged code generation to produce just those you actually needed to be around at runtime. --><p>实际上，我们也删除了很多<code>System.Type</code>，使得没有<code>Assembly</code>，没有<code>BaseType</code>，是的，甚至没有<code>FullName</code>类型。 .NET Framework的mscorlib.dll中仅类型名称就有大约100KB。当然，名称也是很有用的，但我们的事件框架利用代码生成来产生仅在运行时实际需要的那部分代码。</p><!-- At some point, we realized 40% of our image sizes were [vtable](https://en.wikipedia.org/wiki/Virtual_method_table)s.We kept pounding on this one relentlessly, and, after all of that, we still had plenty of headroom for improvements. --><p>在某个时刻，我们意识到生成的可执行文件大小的40%都是<a href="https://en.wikipedia.org/wiki/Virtual_method_table" target="_blank" rel="noopener">vtable</a>。我们一直在坚持不懈地减少这部分的大小，毕竟由于所述的一切，我们仍然有足够的改进空间。</p><!-- Each vtable consumes image space to hold pointers to the virtual functions used in dispatch, and of course has a runtimerepresentation.  Each object with a vtable also has a vtable pointer embedded within it.  So, if you care about size(both image and runtime), you are going to care about vtables. --><p>每个vtable都使用文件中的部分空间来保存指向调度中使用的虚函数的指针，当然同时还有一个运行时的表示。另外，具有vtable的每个对象也同时具有嵌入其中的vtable指针， 所以，如果你关心（整个映像和运行时）的文件大小，你就会对vtable有所关注。</p><!-- In C++, you only get a vtable if your type is [polymorphic](http://www.cplusplus.com/doc/tutorial/typecasting/).  Inlanguages like C# and Java, on the other hand, you get a vtable even if you don't want, need, or use it.  In C#, atleast, you can use a `struct` type to elide them.  I actually love this aspect of Go, where you get a virtual dispatch-like thing, via interfaces, without needing to pay for vtables on every type; you only pay for what you use, at thepoint of coercing something to an interface. --><p>在C++中，如果类型是<a href="http://www.cplusplus.com/doc/tutorial/typecasting/" target="_blank" rel="noopener">多态的</a>，那么它只会有一个vtable。 另一方面，在C#和Java等语言中，即使不想要或不需要使用它，类型也具有vtable，虽然说在C#中，你可以使用<code>struct</code>结构类型来忽略它们。 我真的很喜欢Go的这个方面的做法，因为你可以通过接口获得类似虚拟调度的功能，而只需为某些类型强制实现接口，而无需为每种类型带来vtable的开销。</p><!-- Another vtable problem in C# is that all objects inherit three virtuals from `System.Object`: `Equals`, `GetHashCode`,and `ToString`.  Besides the point that these generally don't do the right thing in the right way anyways -- `Equals`requires reflection to work on value types, `GetHashCode` is nondeterministic and stamps the object header (or sync-block; more on that later), and `ToString` doesn't offer formatting and localization controls -- they also bloat everyvtable by three slots.  This may not sound like much, but it's certainly more than C++ which has no such overhead. --><p>C#中vtable的另一个问题是，所有对象都从<code>System.Object</code>继承了三个虚拟类：<code>Equals</code>，<code>GetHashCode</code>和<code>ToString</code>。 除了这些类通常不会以正确的方式做正确的事情之外，还存在其他的问题：<code>Equals</code>需要反射机制来处理值类型，<code>GetHashCode</code>是非确定性的并且标记了对象头（或同步块，稍后会有更多内容的讨论），而<code>ToString</code>不提供格式化和本地化控制。它们的存在也会使每个vtable增加三个位置，这虽然听起来可能不是很多，但它肯定比没有这些开销的C++体积要更大。</p><!-- The main source of our remaining woes here was the assumption in C#, and frankly most OOP languages like C++ and Java,that [RTTI](https://en.wikipedia.org/wiki/Run-time_type_information) is always available for downcasts.  This wasparticularly painful with generics, for all of the above reasons.  Although we aggressively shared instantiations, wecould never quite fully fold together the type structures for these guys, even though disparate instantiations tendedto be identical, or at least extraordinarily similar.  If I could do it all over agan, I'd banish RTTI.  In 90% of thecases, type discriminated unions or pattern matching are more appropriate solutions anyway. --><p>我们剩下的困扰主要来自于基于C#的假设，坦率地说，对于大多数OOP语言（如C++和Java）而言，<a href="https://en.wikipedia.org/wiki/Run-time_type_information" target="_blank" rel="noopener">RTTI</a>始终可用于向下转换，因此对于泛型来讲，这尤其痛苦。虽然我们激进地采取了共享实例方法，但永远无法完全共享所有这些类型的结构，即使不同的实例往往是相同的或者至少是非常相似的。 如果我能再来一次，我会放弃RTTI，因为在90%的情况下，无论如何类型区分联合体或模式匹配都是更合适的解决方案。</p><!-- ## Profile guided optimizations (PGO) --><h2 id="配置文件引导优化（PGO）"><a href="#配置文件引导优化（PGO）" class="headerlink" title="配置文件引导优化（PGO）"></a>配置文件引导优化（PGO）</h2><!-- I've mentioned [profile guided optimization](https://en.wikipedia.org/wiki/Profile-guided_optimization) (PGO) already.This was a critical element to "go that last mile" after mostly everything else in this article had been madecompetitive.  This gave our browser program boosts in the neighborhood of 30-40% on benchmarks like [SunSpider](https://webkit.org/perf/sunspider/sunspider.html) and [Octane](https://developers.google.com/octane/). --><p>在前文中，我已经提到过<a href="https://en.wikipedia.org/wiki/Profile-guided_optimization" target="_blank" rel="noopener">配置文件引导优化（PGO）</a>。而在本文中的大部分方法都具有竞争力之后，PGO成为“走完最后一英里”的关键因素，它也使我们的浏览器程序在<a href="https://webkit.org/perf/sunspider/sunspider.html" target="_blank" rel="noopener">SunSpider</a>和<a href="https://developers.google.com/octane/" target="_blank" rel="noopener">Octane</a>等基准测试中增加了30%—40%的性能。</p><!-- Most of what went into PGO was similar to classical native profilers, with two big differences. --><p>PGO的大部分工作与传统的原生profiler类似，但有两个地方有很大的不同。</p><!-- First, we tought PGO about many of the unique optimizations listed throughout this article, such as asynchronous stackprobing, generics instantiations, lambdas, and more.  As with many things, we could have gone on forever here. --><p>首先，我们向PGO新增了本文中列出的许多独特优化方法，例如异步堆栈探测，泛型实例化和lambda等。和其他许多地方一样，我们可以在PGO上永远无休止地优化下去。</p><!-- Second, we experimented with sample profiling, in addition to the ordinary instrumented profiling.  This is much nicerfrom a developer perspective -- they don't need two builds -- and also lets you collect counts from real, live runningsystems in the data center.  A good example of what's possible is outlined in [this Google-Wide Profiling (GWP) paper](http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36575.pdf). --><p>其次，除了普通性能测量分析之外，我们还尝试了样本分析。 从开发这的角度上来看，这样做可能会更好，因为它们不需要两次构建，并且还允许从数据中心的实际运行系统中收集和计数。在<a href="http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36575.pdf" target="_blank" rel="noopener">Google-Wide分析方法（GWP）</a>一文中所概述的，可能是一个不错的例子。</p><!-- # System Architecture --><h1 id="系统结构"><a href="#系统结构" class="headerlink" title="系统结构"></a>系统结构</h1><!-- The basics described above were all important.  But a number of even more impactful areas required deeper architecturalco-design and co-evolution with the language, runtime, framework, and operating system itself.  I've written about [theimmense benefits of this sort of "whole system" approach before](http://joeduffyblog.com/2014/09/10/software-leadership-7-codevelopment-is-a-powerful-thing/).  It was kind of magical. --><p>上述的基础知识都是重要的，但在一些更具影响力的领域，则需要与语言、运行时、框架和操作系统本身进行更深层次的架构协同设计和协同演进。 我之前写过<a href="http://joeduffyblog.com/2014/09/10/software-leadership-7-codevelopment-is-a-powerful-thing/" target="_blank" rel="noopener">关于这种“整个系统”方法的巨大好处</a>，它也是有几分神奇的。</p><!-- ## GC --><h2 id="GC"><a href="#GC" class="headerlink" title="GC"></a>GC</h2><!-- Midori was garbage collected through-and-through.  This was a key element of our overall model's safety andproductivity.  In fact, at one point, we had 11 distinct collectors, each with its own unique characteristics.  (Forinstance, see [this study](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.353.9594&rep=rep1&type=pdf).)  Wehad some ways to combat the usual problems, like long pause times.  I'll go through those in a future post, however.For now, let's stick to the realm of code quality. --><p>Midori在各个方面采用了垃圾回收机制，这是我们在整体模型的安全性和生产率上的关键因素。 事实上，在一个收集点上，我们有11个不同的收集器，每个收集器都有自己独特的特征（比如，参考<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.353.9594&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">这项研究</a>）。我们有一些方法可以解决诸如长时间停顿等常见问题。 不过，我会在以后的文章中介绍这些内容，现在，让我们回到代码质量方面。</p><!-- The first top-level decision is: *conservative* or *precise*?  A conserative collector is easier to wedge into anexisting system, however it can cause troubles in certain areas.  It often needs to scan more of the heap to get thesame job done.  And it can falsely keep objects alive.  We felt both were unacceptable for a systems programmingenvironment.  It was an easy, quick decision: we sought precision. --><p>首要的顶层的决定是：<em>保守</em>还是<em>精确</em>？ 一个保守的垃圾收集器更容易融入现有系统，但它可能会在某些地方带来麻烦：它通常需要扫描更多的堆才能完成相同的工作， 并且可能错误地使对象保持存活。 我们认为这两者对于系统编程环境都是不可接受的，所以，我们做出了简单且快速的决定：我们追求精确。</p><!-- Precision costs you something in the code generators, however.  A precise collector needs to get instructions where tofind its root set.  That root set includes field offsets in data structures in the heap, and also places on the stackor, even in some cases, registers.  It needs to find these so that it doesn't miss an object and erroneously collect itor fail to adjust a pointer during a relocation, both of which would lead to memory safety problems.  There was no magictrick to making this efficient other than close integration between runtime and code generator, and being thoughtful. --><p>但是，精确会给代码生成器带来开销。精确的垃圾收集器需要告知它在哪里能够找到根集合（root set），该根集合包括堆中的数据结构的字段偏移，并且还包括栈，甚至在某些情况下还包括寄存器。 所以需要找到它们的全部，使得避免错过任何一个对象、错误地将其回收或者在重定位期间调整指针失败，而这些情况都会导致内存安全问题。而除了运行时和代码生成器之间的紧密集成和周密考虑之外，没有任何神奇的技巧可以使这个过程变得高效。</p><!-- This brings up the topic of *cooperative* versus *preemptive*, and the notion of GC safe-points.  A GC operating incooperative mode will only collect when threads have reached so-called "safe-points."  A GC operating in preemptivemode, on the other hand, is free to stop threads in their tracks, through preemption and thread suspension, so that itmay force a collection.  In general, preemptive requires more bookkeeping, because the roots must be identifiable atmore places, including things that have spilled into registers.  It also makes certain low-level code difficult towrite, of the ilk you'll probably find in an operating system's kernel, because objects are subject to movement betweenarbitrary instructions.  It's difficult to reason about.  (See [this file](https://github.com/dotnet/coreclr/blob/master/src/vm/eecontract.h), and its associated uses in the CLR codebase, if youdon't believe me.)  As a result, we used cooperative mode as our default.  We experimented with automatic safe-pointprobes inserted by the compiler, for example on loop back-edges, but opted to bank the code quality instead.  It didmean GC "livelock" was possible, but in practice we seldom ran into this. --><p>这就进入是<em>协作式</em>还是<em>抢占式</em>的主题以及GC安全点的概念。以协作式运行的GC操作只会在线程达到所谓的“安全点”时进行收集，而以抢占式运行的GC可以通过抢占和线程暂停等方式自由地阻止部分线程，因此可能会强制性回收。一般来说，抢占式需要更多的信息记录，因为它必须在更多的地方识别根集合，包括已经溢出到寄存器中的数据。同时由于对象可能会在任意指令之间移动，它还可能导致某些在操作系统内核中出现的底层代码难以编写，而这些情况都很难进行推断（如果你对此存有怀疑，请参考此<a href="https://github.com/dotnet/coreclr/blob/master/src/vm/eecontract.h" target="_blank" rel="noopener">代码</a>及其在CLR代码库中的相关用法），因此，我们使用了协作式GC作为默认手段。我们以保证代码质量作为目的，尝试使用编译器插入的自动安全点探针，例如在循环后沿上。这种方式确实意味着GC“活锁”是有可能的，但在实践中我们很少遇到这种情况。</p><!-- We used a *generational* collector.  This has the advantage of reducing pause times because less of the heap needs to beinspected upon a given collection.  It does come with one disadvantage from the code generator's perspective, which isthe need to insert write barriers into the code.  If an older generation object ever points back at a younger generationobject, then the collector -- which would have normally preferred to limit its scope to younger generations -- must knowto look at the older ones too.  Otherwise, it might miss something. --><p>我们使用<em>分代</em>收集器，它具有减少暂停时间的优点，因为在给定集合上只需检查较少的堆。 从代码生成器的角度来看，它确实存在一个缺点，即需要在代码中插入写屏障。 如果老年代的对象曾经指向一个新生代的对象，那么收集器通常倾向于将其范围限制在新生代中，这么一来收集器也要检查老年代的对象，否则可能就会错过一些对象的收集。</p><!-- Write barriers show up as extra instructions after certain writes; e.g., note the `call`: --><p>写屏障以跟随特定写入指令的额外指令的形式出现。比如说，注意如下的<code>call</code>指令：</p><pre><code>48 8D 49 08        lea         rcx,[rcx+8]E8 7A E5 FF FF     call        0000064488002028 </code></pre><!-- That barrier simply updates an entry in the card table, so the GC knows to look at that segment the next time it scansthe heap.  Most of the time this ends up as inlined assembly code, however it depends on the particulars of thesituation.  See [this code](https://github.com/dotnet/coreclr/blob/master/src/vm/amd64/JitHelpers_Fast.asm#L462) for anexample of what this looks like for the CLR on x64. --><p>该屏障只是简单地更新表中的项，使得GC知道下次扫描堆时要检查该段， 大多数情况下，它们最终都形成了内联汇编代码，但也取决于具体情况。 对于x64的CLR在这方面的示例做法，请参考<a href="https://github.com/dotnet/coreclr/blob/master/src/vm/amd64/JitHelpers_Fast.asm#L462" target="_blank" rel="noopener">该代码</a>。</p><!-- It's difficult for the compiler to optimize these away because the need for write barriers is "temporal" in nature.  Wedid aggressively eliminate them for stack allocated objects, however.  And it's possible to write, or transform code,into less barrier hungry styles.  For example, consider two ways of writing the same API: --><p>编译器难以对其进行优化的原因是因为对写屏障的需求在本质上是“暂时的”。 但是，我们确实在积极地为栈分配对象消除它们，并且可以将代码编写或转换为更少屏障的风格。比如说，考虑如下相同API的方法：</p><pre><code>bool Test(out object o);object Test(out bool b); </code></pre><!-- In the resulting `Test` method body, you will find a write barrier in the former, but not the latter.  Why?  Because theformer is writing a heap object reference (of type `object`), and the compiler has no idea, when analyzing this methodin isolation, whether that write is to another heap object.  It must be conservative in its analysis and assume theworst.  The latter, of course, has no such problem, because a `bool` isn't something the GC needs to scan. --><p>在生成的<code>Test</code>方法的函数体中，在前者中能找到写屏障指令，而在后者中则不会，这是为什么呢？ 因为前者正在向堆对象引用（类型为<code>object</code>）写入，并且编译器在单独分析此方法时，无法知道写入的是否是另一个堆对象，所以这里的分析必须是保守式，并且需要做最坏的假设。 当然后者没有这样的问题，因为<code>bool</code>不是GC需要扫描的类型。</p><!-- Another aspect of GC that impacts code quality is the optional presence of more heavyweight concurrent read and writebarriers, when using concurrent collection.  A concurrent GC does some collection activities concurrent with the userprogram making forward progress.  This is often a good use of multicore processors and it can reduce pause times andhelp user code make more forward progress over a given period of time. --><p>GC影响代码质量的另一个方面问题是，在使用并发收集时可选地保存更重量级的并发读写屏障。 并发GC是在用户程序运行的同时进行垃圾回收活动，这种方法通常可以很好地利用多核处理器，减少暂停时间，并帮助用户代码在给定的时间段内运行更多的代码。</p><!-- There are many challenges with building a concurrent GC, however one is that the cost of the resulting barriers is high.The original [concurrent GC by Henry Baker](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.5878&rep=rep1&type=pdf) was a copying GC and had the notionof "old" versus "new" space.  All reads and writes had to be checked and, anything operation against the old space hadto be forwarded to the new space.  Subsequent research for the DEC Firefly used hardware memory protection to reduce thecost, but the faulting cases were still exceedingly expensive.  And, worst of all, access times to the heap wereunpredictable.  There has been [a lot of good research into solving this problem](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.1875&rep=rep1&type=pdf), however we abandoned copying. --><p>构建并发GC存在诸多的挑战，但其中一个问题是它产生屏障的开销非常高。 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.5878&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">由Henry Baker提出的原始并发GC方法</a>采用复制GC方式，具有空间“旧”与“新”的概念，所有的读和写都必须进行检查，并且必须将任何针对旧空间的操作都需要转移到新空间中。 DEC Firefly的后续研究使用硬件内存保护来降低开销，但故障情况的处理仍然非常耗时，而且，最糟糕的问题是，堆的访问时间是不可预测的。 为了解决这个问题已经进行了<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.1875&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">很多很好的研究</a>，但我们最终还是放弃了复制的做法。</p><!-- Instead, we used a concurrent mark-sweep compacting collector.  This means only write barriers are needed under normalprogram execution, however some code was cloned so that read barriers were present when programs ran in the presence ofobject movement.  [Our primary GC guy's research was published](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.322&rep=rep1&type=pdf), so you can read all about it.  TheCLR also has a concurrent collector, but it's not quite as good.  It uses copying to collect the youngest generation,mark-sweep for the older ones, and the mark phase is parallelized.  There are unfortunately a few conditions that canlead to sequential pauses (think of this like a big "lock"), sometimes over 10 milliseconds: 1) all threads must behalted and scanned, an operation that is bounded only by the number of threads and the size of their stacks; 2) copyingthe youngest generation is bounded only by the size of that generation (thankfully, in normal configurations, this issmall); and 3) under worst case conditions, compaction and defragmentation, even of the oldest generation, can happen. --><p>相反，我们采用了并发标记-清除压缩收集方法。这意味着在程序正常执行期间只需加入写屏障，但是某些代码需要被克隆，以便在存在对象移动时具有读屏障。<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.322&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">我们的主要GC开发人员的研究已经发表</a>，你可以阅读所有相关的内容。CLR也有一个并发的收集器，但它并非优秀，它主要使用复制方式来收集最年轻的一代，对老年代则采用标记清除方法并将标记阶段并行化处理。遗憾的是，有一些条件会导致顺序性的暂停（想象一下，这就像一把大“锁”），有时甚至超过10毫秒：1）所有线程必须暂停和扫描，这个操作只受线程数和堆大小的限制；2）复制最年轻的一代只受那一代大小的限制（幸运的是，在正常配置下该代内存数量很少）；3）在最坏的情况下，即使是最老的一代也可能发生压缩和碎片整理操作。</p><!-- ## Separate compilation --><h2 id="独立编译"><a href="#独立编译" class="headerlink" title="独立编译"></a>独立编译</h2><!-- The basic model to start with is static linking.  In this model, you compile everything into a single executable.  Thebenefits of this are obvious: it's simple, easy to comprehend, conceptually straightforward to service, and less workfor the entire compiler toolchain.  Honestly, given the move to Docker containers as the unit of servicing, this modelmakes more and more sense by the day.  But at some point, for an entire operating system, you'll want separatecompilation.  Not just because compile times can get quite long when statically linking an entire operating system, butalso because the working set and footprint of the resulting processes will be bloated with significant duplication. --><p>这部分以最基本模型——静态链接作为开始。在此模型中，所有的内容被编译成单个可执行文件， 这样做的好处是显而易见的：它简单易懂，服务起来概念简单，整个编译器工具链的工作量更少。 老实说，考虑到正在发生的将Docker容器作为服务单元的运动，这种模式在当前变得越来越有意义。 但在某些时候，对于整个操作系统而言，需要的可能是独立编译，这不仅仅是因为静态链接编译整个操作系统的时间会很长，而且是因为生成的进程工作集和占用空间存在大量的重复。</p><!-- Separately compiling object oriented APIs is hard.  To be honest, few people have actually gotten it to work.  Problemsinclude the [fragile base class problem](https://en.wikipedia.org/wiki/Fragile_base_class), which is a real killer forversion resilient libraries.  As a result, most real systems use a dumbed down ["C ABI"](https://en.wikipedia.org/wiki/Application_binary_interface) at the boundary between components.  This is why Windows,for example, has historically used flat C Win32 APIs and, even in the shift to more object orientation via WinRT, usesCOM underneath it all.  At some runtime expense, the ObjectiveC runtime addressed this challenge.  As with most thingsin computer science, virtually all problems can be solved with an extra level of indirection; [this one can be too](http://www.sealiesoftware.com/blog/archive/2009/01/27/objc_explain_Non-fragile_ivars.html). --><p>独立编译面向对象的API是一件很难的事情，说实话，很少有人真正将其搞定。这里的问题包括<a href="https://en.wikipedia.org/wiki/Fragile_base_class" target="_blank" rel="noopener">脆弱的基类问题</a>，这也是版本有弹性的库的真正杀手。 因此，大多数真实系统在组件之间的边界处使用了笨拙的<a href="https://en.wikipedia.org/wiki/Application_binary_interface" target="_blank" rel="noopener">“C ABI”</a>。 这就是为什么Windows在历史上使用普通C语言的Win32 API，即使在底层使用COM并通过WinRT转到更多面向对象的情况下也依然如此。 在花费一定运行时的开销的条件下，Objective C的运行时解决了这一挑战。而与计算机科学中的大多数事物一样，几乎所有问题都可以通过额外的间接抽象来解决，<a href="http://www.sealiesoftware.com/blog/archive/2009/01/27/objc_explain_Non-fragile_ivars.html" target="_blank" rel="noopener">这个问题也依然如此</a>。</p><!-- The design pivot we took in Midori was that whole processes were sealed.  There was no dynamic loading, so nothing thatlooked like classical DLLs or SOs.  For those scenarios, we used the [Asynchronous Everything](http://joeduffyblog.com/2015/11/19/asynchronous-everything/) programming model, which made it easy to dynamicallyconnect to and use separately compiled and versioned processes. --><p>我们在Midori中采用的设计思路是所有进程都是密封的（sealed），没有动态加载，所以没有看起来像经典的DLL或SO的库文件。 对于这些场景，我们使用了<a href="/2018/11/25/midori/3-asynchronous-everything/">一切皆异步</a>的编程模型，这使得它和使用独立编译和版本化的进程动态连接变得容易。</p><!-- We did, however, want separately compiled binaries, purely as a developer productivity and code sharing (working set)play.  Well, I lied.  What we ended up with was incrementally compiled binaries, where a change in a root node triggereda cascading recompilation of its dependencies.  But for leaf nodes, such as applications, life was beautiful.  Overtime, we got smarter in the toolchain by understanding precisely which sorts of changes could trigger cascadinginvaliation of images.  A function that was known to never have been inlined across modules, for example, could have itsimplementation -- but not its signature -- changed, without needing to trigger a rebuild.  This is similar to thedistinction between headers and objects in a classical C/C++ compilation model. --><p>但是，我们确实需要独立编译的二进制文件，纯粹是由于开发者的工作效率和代码共享（工作集）所导致。好吧，我承认我之前说谎了。 我们最终得到的是增量编译的二进制文件，其中根节点的更改会触发其依赖项级联式的重新编译，但对于叶节点，比如说应用程序而言，情况却要好得多。 随着时间的推移，我们通过精确了解哪种类型的更改可以触发映像文件的级联失效，使得工具链变得更加智能。 例如，一个从未在模块之间发生内联的函数如果它的实现（而不是函数签名）发生了更改，则无需触发重建，这类似于传统C/C++编译模型中头文件和对象之间的区别。</p><!-- Our compilation model was very similar to C++'s, in that there was static and dynamic linking.  The runtime model, ofcourse, was quite different.  We also had the notion of "library groups," which let us cluster multiple logicallydistinct, but related, libraries into a single physical binary.  This let us do more aggressive inter-moduleoptimizations like inlining, devirtualization, async stack optimizations, and more. --><p>因为我们的编译模型也有静态和动态链接，所以与C++的非常相似，当然运行时模型则是完全不同的。我们还有“库分组”的概念，它允许我们将多个逻辑上不同但相关的库集中到一个物理的二进制文件中，这让我们可以进行更激进的模块间优化，如内联，虚拟化和异步堆栈优化等。</p><!-- ## Parametric polymorphism (a.k.a., generics) --><h2 id="参数多态（也就是泛型）"><a href="#参数多态（也就是泛型）" class="headerlink" title="参数多态（也就是泛型）"></a>参数多态（也就是泛型）</h2><!-- That brings me to generics.  They throw a wrench into everything. --><p>上面的内容让我想到了泛型（generics），它是能把一切都搞砸的特性。</p><!-- The problem is, unless you implement an [erasuremodel](https://docs.oracle.com/javase/tutorial/java/generics/erasure.html) -- which utterly stinks performance-wise dueto boxing allocations, indirections, or both -- there's no way for you to possibly pre-instantiate all possible versionsof the code ahead-of-time.  For example, say you're providing a `List<T>`.  How do you know whether folks using yourlibrary will want a `List<int>`, `List<string>`, or `List<SomeStructYouveNeverHeardOf>`? --><p>这里的问题是，除非你实现一个因为Box分配，间接取值或两者同时具备的，完全以性能为代价的<a href="https://docs.oracle.com/javase/tutorial/java/generics/erasure.html" target="_blank" rel="noopener">擦除模型</a>，你是没有办法预先实例化代码的所有可能版本。 比如说，假设你提供了<code>List&lt;T&gt;</code>，你怎么知道使用库的人需要的是<code>List&lt;int&gt;</code>，<code>List&lt;string&gt;</code>还是<code>List&lt;SomeStructYouveNeverHeardOf&gt;</code>？</p><!-- Solutions abound: --><p>解决的方案有很多：</p><!-- 1. Do not specialize.  Erase everything.2. Specialize only a subset of instantiations, and create an erased instantiation for the rest.3. Specialize everything.  This gives the best performance, but at some complexity. --><ol><li>不专门处理，擦除一切；</li><li>仅专门实例化其中的一个子集，并为其余实例创建擦除后的实例。</li><li>专门处理所有实例，它能够提供最佳的性能，但同时也有些复杂。</li></ol><!--Java uses #1 (in fact, erasure is baked into the language).  Many ML compilers use #2.  .NET's NGen compilationmodel is sort of a variant of #2, where things that can be trivially specialized are specialized, and everything else isJIT compiled.  .NET Native doesn't yet have a solution to this problem, which means 3rd party libraries, separatecompilation, and generics are a very big TBD.   As with everything in Midori, we picked the hardest path, with the mostupside, which meant #3.  Actually I'm being a little glib; we had several ML compiler legends on the team, and #2 isfraught with peril; just dig a little into [some papers](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.2165&rep=rep1&type=pdf) on how hard (and clever) this canget.  It's difficult to know a priori which instantiations are going to be performance critical to a program.  My ownexperience trying to get C# code into the heart of Windows back in the Longhorn days also reinforced this; we didn'twant JIT'ting and the rules for what generics you could and couldn't use in that world were so mind boggling theyeventually led to greek formulas. --><p>Java使用的是方案1（事实上，擦除模型已经合并到其语言中），而许多ML编译器使用方案2。.NET的NGen编译模型是方案2的变体，其中可以简单地专门化处理的都已专门处理，而其他都是通过JIT编译的。 .NET Native还没有这个问题的解决方案，这意味着第三方库，独立编译和泛型是存在着非常大的TBD。和Midori其他一切内容一样，我们选择了最艰难但却最具有上升空间的道路，这里就意味着是方案3。实际上，我说的有点夸张，我们的团队中有几个ML编译器的传奇人物，所以知道方案2充满着危险。如果能稍微深入了解一下<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.2165&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">这篇论文</a>就知道这个方案有多难（和聪明），因为很难先验地知道哪些实例化对程序至关重要。我自己在Longhorn（其正式的名称是Vista）的年代试图将C#代码置入Windows核心的经验也强化了这样的信念：我们不想要JIT，哪些泛型可以使用和哪些泛型不可使用的规则是如此令人难以置信，使得其最终转变为希腊公式。</p><!-- Anyway, Midori's approach turned out to be harder than it sounded at first. --><p>无论如何，Midori的方法比最初听起来更加困难。</p><!-- Imagine you have a diamond.  Library A exports a `List<T>` type, and libraries B and C both instantiate `List<int>`.  Aprogram D then consumes both B and C and maybe even passes `List<T>` objects returned from one to the other.  How do weensure that the versions of `List<int>` are compatible? --><p>设想一下你有如下的菱形关系。 库A导出了<code>List&lt;T&gt;</code>类型，库B和C都实例化了<code>List&lt;int&gt;</code>，而程序D则同时使用了B和C，甚至将<code>List&lt;T&gt;</code>对象从一个库传递到另一个库。那么我们应如何确保<code>List&lt;int&gt;</code>的版本是兼容的？</p><!-- We called this problem the *potentially multiply instantiated*, or PMI for short, problem. --><p>我们称此问题为<em>潜在的多次实例化</em>，或简称为PMI问题。</p><!-- The CLR handles this problem by unifying the instantiations at runtime.  All RTTI data structures, vtables, and whatnot,are built and/or aggressively patched at runtime.  In Midori, on the other hand, we wanted all such data structures tobe in readonly data segments and hence shareable across processes, wherever possible. --><p>CLR通过在运行时统一实例化来处理此问题，所有的RTTI数据结构，vtable和诸如此类的东西都在运行时构建和/或激进地进行修补。 而另一方面，在Midori中，我们希望所有这些数据结构都在只读数据段中出现，因此可以尽可能地在各个进程之间共享。</p><!-- Again, everything can be solved with an indirection.  But unlike solution #2 above, solution #3 permits you to stickindirections only in the rare places where you need them.  And for purposes of this one, that meant RTTI and accessingstatic variables of just those generic types that might have been subject to PMI.  First, that affected a vast subset ofcode (versus #2 which generally affects even loading of instance fields).  Second, it could be optimized away forinstantiations that were known not to be PMI, by attaching state and operations to the existing generic dictionary thatwas gets passed around as a hidden argument already.  And finally, because of all of this, it was pay for play. --><p>再一次地，一切都可以通过间接抽象来解决。 但与上面的方案#2不同的是，方案#3允许只在需要使用它们的地方加入间接抽象。 就本文而言，这意味着RTTI和访问那些可能受PMI影响的泛型类型的静态变量。 首先，它影响了大量代码（相对而言，方案#2通常影响的是实例字段的加载）；其次，它可以通过将状态和操作附加到已经作为隐藏参数传递的，现有的泛型字典来优化明确不是PMI的实例化。 最后，因为以上所有的这一切，也将付出相应的性能上的代价。</p><!-- But damn was it complex. --><p>但该死的是它过于复杂。</p><!-- It's funny, but C++ RTTI for template instantiations actually suffers from many of the same problems.  In fact, theMicrosoft Visual C++ compiler resorts to a `strcmp` of the type names, to resolve diamond issues!  (Thankfully there are[well-known, more efficient ways to do this](http://www6.open-std.org/JTC1/SC22/WG21/docs/papers/1992/WG21%201992/X3J16_92-0068%20WG21_N0145.pdf), which we areactively pursuing for the next release of VC++.) --><p>很有意思的是，用于模板实例化的C++ RTTI实际上遇到了许多相同的问题。 事实上上，Microsoft Visual C++编译器在类名上使用<code>strcmp</code>字符串比较的方法，以解决菱形问题 （值得庆幸的是，有一些<a href="http://www6.open-std.org/JTC1/SC22/WG21/docs/papers/1992/WG21%201992/X3J16_92-0068%20WG21_N0145.pdf" target="_blank" rel="noopener">众所周知的，更有效的方法可以做到这一点</a>，我们正在积极关注下一版VC++）！</p><!-- ## Virtual dispatch --><h2 id="虚拟调度（Virtual-Dispatch）"><a href="#虚拟调度（Virtual-Dispatch）" class="headerlink" title="虚拟调度（Virtual Dispatch）"></a>虚拟调度（Virtual Dispatch）</h2><!-- Although I felt differently when first switching from Java to C#, Midori made me love that C# made methods non-virtualby default.  I'm sure we would have had to change this otherwise.  In fact, we went even further and made classes`sealed` by default, requiring that you explicitly mark them `virtual` if you wanted to facilitate subclasses. --><p>虽然在我首次从Java切换到C#时感觉很不一样，但Midori让我喜欢上了C#默认情况将方法以非虚拟的方式存在的做法，所以我相信我们不得不改变这一点。事实上我们更进了一步，即在默认情况下使用<code>sealed</code>类型的类，如果你想便利地使用子类，则需要明确地使用<code>virtual</code>标记它们。</p><!-- Aggressive devirtualization, however, was key to good performance.  Each virtual means an indirection.  And moreimpactfully, a lost opportunity to inline (which for small functions is essential).  We of course did globalintra-module analysis to devirtualize, but also extended this across modules, using whole program compilation, whenmultiple binaries were grouped together into a library group. --><p>然而，激进的去虚拟化是获得优异性能的关键，那是因为，每个虚拟方法意味着一次抽象，而影响更大的是，虚函数使得其失去了内联的机会（而这对于小型函数来说是必不可少的优化方法）。 当然，我们还进行了全局模块内分析以支持去虚拟化，但当为了编译整个程序而需将多个二进制文件组合成一整个库时，也扩展到模块之间的分析。</p><!-- Although our defaults were right, my experience with C# developers is that they go a little hog-wild with virtuals andoverly abstract code.  I think the ecosystem of APIs that exploded around highly polymorphic abstractions, like LINQ andReactive Extensions, encouraged this and instilled a bit of bad behavior ("gratuitous over-abstraction").  I guess youcould make similar arguments about highly templated code in C++.  As you can guess, there wasn't very much of it in thelowest levels of our codebase -- where every allocation and instruction mattered -- but in higher level code, especiallyin applications that tended to be dominated by high-latency asynchronous operations, the overheads were acceptable andproductivity benefits high.  A strong culture around identifying and trimming excessive fat helped to ensure featureslike this were used appropriately, via code reviews, benchmarks, and aggressive static analysis checking. --><p>尽管我们的默认设置是正确的，但对C#开发者来说，我的经验是他们对虚拟和过于抽象的代码存有兴趣。 围绕高度多态的抽象（如LINQ和Reactive Extensions）的API生态系统的剧增助长了这种情况的发生，并灌输了一些的不良行为（比如“无偿的过度抽象”），我想你可以在C++中高度模板化的代码上得出类似的观点。 正如你所猜测的那样，在我们的每个分配和指令都很重要的代码库最低层，并没有很多这样的问题；但在更高层次的代码中，特别是在那些倾向于由高延迟异步操作主导的，可以接受一定的开销，且非常注重开发效率的应用程序中大量的出现。 而通过代码审查，基准测试和激进的静态分析检查，围绕识别和修剪过多冗余代码的强大文化有助于确保适当地使用此类功能。</p><!-- Interfaces were a challenge. --><p>接口是一个挑战。</p><!-- There are just some poorly designed, inefficient patterns in the .NET Framework.  `IEnumerator<T>` requires *two*interface dispatches simply to extract the next item!  Compare that to C++ iterators which can compile down a pointerincrement plus dereference.  Many of these problems could be addressed simply with better library designs.  (Our finaldesign for enumeration didn't even invole interfaces at all.) --><p>.NET Framework中有一些设计不良，效率低下的模式。 比如说<code>IEnumerator&lt;T&gt;</code>仅仅只是为了获取下一个项的操作，就需要<em>两个</em>接口的指派！ 与其相比较的是，C++迭代器可以编译成只使用指针的递增外加一次解除引用的简单方式。 对于许多类似的问题，都可以通过更好的库设计来解决（比如说，我们最终设计的枚举器甚至根本没有接口的介入）。</p><!-- Plus invoking a C# interface is tricky.  Existing systems do not use pointer adjustment likeC++ does so usually an interface dispatch requires a table search.  First a level of indirection to get to the vtable,then another level to find the interface table for the interface in question.  Some systems attempt to do callsitecaching for monomorphic invocations; that is, caching the latest invocation in the hope that the same object kind passesthrough that callsite time and time again.  This requires mutable stubs, however, not to mention an [incredibly complexsystem of thunks and whatnot](https://github.com/dotnet/coreclr/blob/master/src/vm/virtualcallstub.cpp).  In Midori, wenever ever ever violated [W^X](https://en.wikipedia.org/wiki/W%5EX); and we avoided mutable runtime data structures,because they inhibit sharing, both in terms of working set, but also amortizing TLB and data cache pressure. --><p>除此之外，调用C#接口是一件很棘手的事情。 现有的系统不像C++那样使用指针调整，因此通常接口的指派需要进行表内搜索。 首先为了获取vtable，需要一层间接的跳转；然后为了获取接口表，又是另一层间接的跳转。 有些系统尝试对单态调用进行callsite缓存，也就是说，缓存最新的调用并期望相同的对象类再次进入该callsite。这种方式需要可变的stub，更不用说一个<a href="https://github.com/dotnet/coreclr/blob/master/src/vm/virtualcallstub.cpp" target="_blank" rel="noopener">异常复杂的thunk和诸如此类的东西的系统</a>。 在Midori，我们从未违反过<a href="https://en.wikipedia.org/wiki/W%5EX" target="_blank" rel="noopener">W^X</a>原则，并且由于其不利于共享也避免了可变的运行时数据结构，使得减小了工作集的大小，同时分摊了TLB和数据缓存的压力。 </p><!-- Our solution took advantage of the memory ordering model earlier.  We used so-called "fat" interface pointers.  A fatinterface pointer was two words: the first, a pointer to the object itself; the second, a pointer to the interfacevtable for that object.  This made conversion to interfaces slightly slower -- because the interface vtable lookup hadto happen -- but for cases where you are invoking it one or more times, it came out a wash or ahead.  Usually,significantly.  Go does something like this, but it's slightly different for two reasons.  First, they generate theinterface tables on the fly, because interfaces are duck typed.  Second, fat interface pointers are subject to tearingand hence can violate memory safety in Go, unlike Midori thanks to our strong concurrency model. --><p>我们的解决方案是更早地利用了内存排序模型，使用了所谓的“胖”接口指针。 胖接口指针由两个单字组成：第一个是指向对象本身的指针，第二个是指向该对象的接口的vtable的指针。 因为必须在接口vtable中进行查找，所以这使得转换到接口的速度变得稍慢。但对于你一次或几次调用的地方，缓存可以有效的解决，并且通常效果是相当显著的。Go做了类似的事情，但由于以下两个原因而略有不同： 首先，由于其接口都是duck typed，因此采用了动态生成接口表的方式；其次，由于Go没有我们Midori强大的并发模型，胖接口指针会遭到破坏从而可能会违反内存安全性。</p><!-- The finally challenge in this category was *generic virtual methods*, or GVMs.  To cut to the chase, we banned them.Even if you NGen an image in .NET, all it takes is a call to the LINQ query `a.Where(...).Select(...)`, and you'repulling in the JIT compiler.  Even in .NET Native, there is considerable runtime data structure creation, lazily, whenthis happens.  In short, there is no known way to AOT compile GVMs in a way that is efficient at runtime.  So, we didn'teven bother offering them.  This was a slightly annoying limitation on the programming model but I'd have done it allover again thanks to the efficiencies that it bought us.  It really is surprising how many GVMs are lurking in .NET. --><p>虚拟调度的最终挑战是<em>泛型虚方法</em>，或者称之为GVM。 我们的方法概括起来就是，禁止了他们的使用。 即使在.NET中使用NGen生成映像文件，所有它需要仅仅是一次LINQ查询<code>a.Where(...).Select(...)</code>的函数调用，然后便进入到了JIT编译器。 另外即使是对于.NET Native，当发生这种情况时，也会惰性地创建了相当多的运行时数据结构。 简而言之，尚未有已知的方法使得AOT以一种在运行时中高效的方式编译GVM，因此我们甚至都不提供GVM。 这对编程模型来说是一个有点烦人的限制，但由于这样的做法确实我们带来了效率，所以我们还是选择这样做。 另外，令人吃惊的是，在.NET中潜伏了大量的GVM。</p><!-- ## Statics --><h2 id="静态值"><a href="#静态值" class="headerlink" title="静态值"></a>静态值</h2><!-- I was astonished the day I learned that 10% of our code size was spent on static initialization checks. --><p>当我知道10%的代码体积用于存储静态初始化检查时，这让我感到十分惊讶。</p><!-- Many people probably don't realize that the [CLI specification](http://www.ecma-international.org/publications/standards/Ecma-335.htm) offers two static initialization modes.  Thereis the default mode and `beforefieldinit`.  The default mode is the same as Java's.  And it's horrible. The staticinitializer will be run just prior to accessing any static field on that type, any static method on that type, anyinstance or virtual method on that type (if it's a value type), or any constructor on that type.  The "when" partdoesn't matter as much as what it takes to make this happen; *all* of those places now need to be guarded with explicitlazy initialization checks in the resulting machine code! --><p>许多人可能没有意识到<a href="http://www.ecma-international.org/publications/standards/Ecma-335.htm" target="_blank" rel="noopener">CLI规范</a>提供了两种静态初始化模式：默认模式和<code>beforefieldinit</code>模式。 默认模式与Java的做法相同，这听起来太可怕了。 静态初始化程序将在访问该类型上的任何静态字段，该类型上的任何静态方法，该类型上的任何实例或虚方法（如果它是值类型）或该类型上的任何构造函数的执行之前运行。 “何时”部分与实现这一目标所需要的一样重要，因为现在<em>所有</em>这些地方都需要在生成的机器代码中进行显式的延迟初始化检查！</p><!-- The `beforefieldinit` relaxation is weaker.  It guarantees the initializer will run sometime before actually accessinga static field on that type.  This gives the compiler a lot of leeway in deciding on this placement.  Thankfully theC# compiler will pick `beforefieldinit` automatically for you should you stick to using field initializers only.  Mostpeople don't realize the incredible cost of choosing instead to use a static constructor, however, especially for valuetypes where suddenly all method calls now incur initialization guards.  It's just the difference between: --><p><code>beforefieldinit</code>的松弛就显得约束比较弱，它保证了初始化程序将在实际访问该类型的静态字段之前运行，这为编译器在决定何时初始时提供了很大的余地。 值得庆幸的是，如果你只坚持使用字段初始化器，C#编译器会自动选择<code>beforefieldinit</code>模式。 然而，大多数人并没有意识到选择使用静态构造函数的不可思议的成本，特别是对于所有方法调用现在都会进行初始化保护的值类型来说。 因此，这是如下两者之间的区别：</p><pre><code>struct S {    static int Field = 42;} </code></pre><!-- and: --><pre><code>struct S {    static int Field;    static S() {        Field = 42;    }} </code></pre><!-- Now imagine the struct has a property: --><p>现假设结构有如下的属性值：</p><!--     struct S {        // As above...        int InstanceField;        public int Property { get { return InstanceField; } }    } --><pre><code>struct S {    // 如上 ...    int InstanceField;    public int Property { get { return InstanceField; } }} </code></pre><!-- Here's the machine code for `Property` if `S` has no static initializer, or uses `beforefieldinit` (automaticallyinjected by C# in the the field initializer example above): --><p>如果<code>S</code>没有静态初始化程序或者使用了beforefieldinit<code>模式（在上面的字段初始化程序示例中由C#自动注入），那么这将是产生如下的</code>Property`的机器代码：</p><!--     ; The struct is one word; move its value into EAX, and return it:    8B C2                mov         eax,edx    C3                   ret --><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">; 结构是一个单字；将其值移入EAX中，并返回：</span><br><span class="line">8B C2                mov         eax,edx</span><br><span class="line">C3                   ret</span><br></pre></td></tr></table></figure><!-- And here's what happens if you add a class constructor: --><p>如果添加类的构造函数，会发生以下情况：</p><!--     ; Big enough to get a frame:    56                   push        rsi    48 83 EC 20          sub         rsp,20h    ; Load the field into ESI:    8B F2                mov         esi,edx    ; Load up the cctor's initialization state:    48 8D 0D 02 D6 FF FF lea         rcx,[1560h]    48 8B 09             mov         rcx,qword ptr [rcx]    BA 03 00 00 00       mov         edx,3    ; Invoke the conditional initialization helper:    E8 DD E0 FF FF       call        2048    ; Move the field from ESI into EAX, and return it:    8B C6                mov         eax,esi    48 83 C4 20          add         rsp,20h    5E                   pop         rsi --><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">; 大到足以装下整个frame：</span><br><span class="line">56                   push        rsi</span><br><span class="line">48 83 EC 20          sub         rsp,20h</span><br><span class="line">; 将字段加载到ESI中：</span><br><span class="line">8B F2                mov         esi,edx</span><br><span class="line">; 加载cctor的初始化状态：</span><br><span class="line">48 8D 0D 02 D6 FF FF lea         rcx,[1560h]</span><br><span class="line">48 8B 09             mov         rcx,qword ptr [rcx]</span><br><span class="line">BA 03 00 00 00       mov         edx,3</span><br><span class="line">; 调用条件初始化函数：</span><br><span class="line">E8 DD E0 FF FF       call        2048</span><br><span class="line">; 将字段从ESI移动到EAX，而后返回：</span><br><span class="line">8B C6                mov         eax,esi</span><br><span class="line">48 83 C4 20          add         rsp,20h</span><br><span class="line">5E                   pop         rsi</span><br></pre></td></tr></table></figure><!-- On every property access! --><p>对于每个属性的访问都是如此！</p><!-- Of course, all static members still incur these checks, even if `beforefieldinit` is applied. --><p>当然，即使应用了<code>beforefieldinit</code>，所有的静态成员仍会执行这些检查。</p><!-- Although C++ doesn't suffer this same problem, it does have mind-bending [initialization ordering semantics](http://en.cppreference.com/w/cpp/language/initialization).  And, like C# statics, C++11 introduced thread-safeinitialization, by way of the ["magic statics" feature](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2660.htm). --><p>尽管C++没有遇到同样的问题，但它确实存在令人费解的<a href="http://en.cppreference.com/w/cpp/language/initialization" target="_blank" rel="noopener">初始化排序语义</a>。 并且就像C#的静态值一样，C++11通过<a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2660.htm" target="_blank" rel="noopener">“魔法静态”功能</a>引入了线程安全的初始化方法。</p><!-- We virtually eliminated this entire mess in Midori. --><p>我们几乎消除了Midori在这个方面的整个混乱局面。</p><!-- I mentioned offhandedly earlier that Midori had no mutable statics.  More accurately, we extended the notion of `const`to cover any kind of object.  This meant that static values were evaluated at compile-time, written to the readonlysegment of the resulting binary image, and shared across all processes.  More importantly for code quality, all runtimeinitialization checks were removed, and all static accesses simply replaced with a constant address. --><p>之前已经提到过，Midori没有可变的静态值。 更准确地说，我们扩展了<code>const</code>的概念以涵盖任何种类的对象， 这意味着静态的值在编译时会进行求值，将结果写入生成的二进制映像文件的只读段，并在所有进程中共享。 而对于代码质量更重要的是，所有运行时初始化检查都被移除，并且所有的静态访问都被固定的地址所替换。</p><!-- There were still mutable statics at the core of the system -- in the kernel, for example -- but these did not make theirway up into user code.  And because they were few and far between, we did not rely on the classical C#-style lazyinitialization checks for them.  They were manually initialized on system startup. --><p>在系统的核心部分，比如内核中，仍然存在可变的静态值，但用户代码中却不存在可变静态值。 因为这样值的数量很少，所以我们未使用它们的经典C#风格的延迟初始化检查，而是在系统启动时手动进行初始化的。</p><!-- As I said earlier, a 10% reduction in code size, and lots of speed improvements.  It's hard to know exactly how muchsaved this was than a standard C# program because by the time we made the change, developers were well aware of theproblems and liberally applied our `[BeforeFieldInit]` attribute all over their types, to avoid some of the overheads.So the 10% number is actually a lower bound on the savings we realized throughout this journey. --><p>正如我之前所提到的，整个镜像代码体积减少了10%，并且速度提升了很多。 不过很难确切知道这比标准C#程序节省了多少空间，因为当我们着手进行更改时，开发者已经很清楚这些问题，并且在他们的类型中自由地使用<code>[BeforeFieldInit]</code>属性以避免开销。 因此，10%实际上是我们在整个过程中实现的代码体积节省的下限值。</p><!-- ## Async model --><h2 id="Async模型"><a href="#Async模型" class="headerlink" title="Async模型"></a>Async模型</h2><!-- I already wrote a lot about [our async model](http://joeduffyblog.com/2015/11/19/asynchronous-everything/).  I won'trehash all of that here.  I will reiterate one point: the compiler was key to making linked stacks work. --><p>我已经写了很多关于<a href="/2018/11/25/midori/3-asynchronous-everything/">异步模型</a>的内容，这里我将不再赘述。 不过我将重申一点：编译器是使链接运行栈运行的关键。</p><!-- In a linked stacks model, the compiler needs to insert probes into the code that check for available stack space.  Inthe event there isn't enough to perform some operation -- make a function call, dynamically allocate on the stack, etc.-- the compiler needs to arrange for a new link to get appended, and to switch to it.  Mostly this amounts to somerange checking, a conditional call to a runtime function, and patching up `RSP`.  A probe looked something like: --><p>在链接栈模型中，编译器需要将探针插入到检查可用栈空间的代码中。如果出现没有足够空间来执行某些操作情况，比如进行函数调用或在栈上动态分配等，编译器则需要安排新的链接附加到内存中，并切换到它的位置。大多数情况下，这相当于一些范围检查，对运行时函数的条件调用以及对<code>RSP</code>的修补。 探针看起来像是如下的代码：</p><!--     ; Check amount of stack space:        lea     rax, [rsp-250h]        cmp     rax, qword ptr gs:[0]        ja      prolog    ; If insufficient stack, link a new segment:        mov     eax, 10029h        call    ?g_LinkNewStackTrampoline    prolog:    ; The real code goes here... --><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">; 检查栈空间使用量：</span><br><span class="line">  lea     rax, [rsp-250h]</span><br><span class="line">  cmp     rax, qword ptr gs:[0]</span><br><span class="line">  ja      prolog</span><br><span class="line">; 如果栈空间不足，则链接到新的段：</span><br><span class="line">  mov     eax, 10029h</span><br><span class="line">  call    ?g_LinkNewStackTrampoline</span><br><span class="line">prolog:</span><br><span class="line">; 真正的运行代码出现在这里 ...</span><br></pre></td></tr></table></figure><!-- Needless to say, you want to probe as little as possible, for two reasons.  First, they incur runtime expense.  Second,they chew up code size.  There are a few techniques we used to eliminate probes. --><p>不用说的是，出于以下两个原因，你希望尽可能少地进行探测操作：首先，它们会产生运行时开销；其次，他们会增加代码体积。因此，我们使用一些技术来消除探针。</p><!-- The compiler of course knew how to compute stack usage of functions.  As a result, it could be smart about the amount ofmemory to probe for.  We incorporated this knowledge into our global analyzer.  We could coalesce checks after doingcode motion and inlining.  We hoisted checks out of loops.  For the most part, we optimized for eliminating checks,sometimes at the expense of using a little more stack. --><p>编译器当然知道如何计算函数运行栈的使用量，因此，对于探测的内存量实际上还可以更聪明一些。 我们将这些知识融入我们的全局分析器中，并可以在代码移动和内联后进行合并检查。 我们将检查外提到循环之外，并在大多数情况下，对消除检查进行了优化，有时甚至以使用更多栈空间作为代价。</p><!-- The most effective technique we used to eliminate probes was to run synchronous code on a classical stack, and to teachour compiler to elide probes altogether for them.  This took advantage of our understanding of async in the type system.Switching between the classical stack and back again again amounted to twiddling `RSP`: --><p>为了消除探针，我们最有效的技术是在经典栈上运行同步代码，并指导我们的编译器完全省略掉探针，这利用了我们对类型系统中异步的理解。而经典的运行栈之间的切换只需简单地更改<code>RSP</code>的值：</p><!--     ; Switch to the classical stack:    move    rsp, qword ptr gs:[10h]    sub     rsp, 20h    ; Do some work (like interop w/ native C/C++ code)...    ; Now switch back:    lea     rsp, [rbp-50h] --><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">; 切换到经典栈：</span><br><span class="line">move    rsp, qword ptr gs:[10h]</span><br><span class="line">sub     rsp, 20h</span><br><span class="line"></span><br><span class="line">; 完成一些任务（比如和原生的C/C++进行互操作）...</span><br><span class="line"></span><br><span class="line">; 再切换回来：</span><br><span class="line">lea     rsp, [rbp-50h]</span><br></pre></td></tr></table></figure><!-- I know Go abandoned linked stacks because of these switches.  At first they were pretty bad for us, however after abouta man year or two of effort, the switching time faded away into the sub-0.5% noise. --><p>我知道由于这样的切换，Go放弃了链接栈。 并且起初对于我们来说，这种方式也是非常糟糕，但经过大约一年或两年的努力，切换时间逐渐降低到低于总开销的0.5%。</p><!-- ## Memory ordering model --><h2 id="内存顺序模型"><a href="#内存顺序模型" class="headerlink" title="内存顺序模型"></a>内存顺序模型</h2><!-- Midori's stance on [safe concurrency](http://joeduffyblog.com/2015/11/03/a-tale-of-three-safeties/) had truly oneamazing benefit: you get a [sequentially consistent](https://en.wikipedia.org/wiki/Sequential_consistency) memoryordering model *for free*.  You may wish to read that again.  Free! --><p>Midori对<a href="/2018/10/24/midori/1-a-tale-of-three-safeties/">安全并发</a>的态度确实有一个惊人的好处：你可以<em>免费</em>获得<a href="https://en.wikipedia.org/wiki/Sequential_consistency" target="_blank" rel="noopener">顺序一致</a>的内存排序模型。现在，你可能希望再次阅读那篇安全并发的文章，那么请随意吧！</p><!-- Why is this so?  First, Midori's [process model](http://joeduffyblog.com/2015/11/19/asynchronous-everything/) ensuredsingle-threaded execution by default.  Second, any fine-grained parallelism inside of a process was governed by a finitenumber of APIs, all of which were race-free.  The lack of races meant we could inject a fence at fork and join points,selectively, without a developer needing to care or know. --><p>为什么会是这样呢？首先，Midori的<a href="/2018/11/25/midori/3-asynchronous-everything/">进程模型</a>在默认情况下确保以单线程方式执行。 其次，进程内部的任何细粒度并行都由有限数量的API控制，所有这些都是无竞争的。 缺少竞争意味着我们可以有选择性地在fork和join位置注入屏障指令，而无需开发者注意或了解其细节。</p><!-- Obviously this had incredible benefits to developer productivity.  The fact that Midori programmers never got bitten bymemory reordering problems was certainly one of my proudest outcomes of the project. --><p>显然，这对开发者的工作效率有着不可思议的好处。 因此，Midori程序员从未被内存重排序问题所困扰，而这也无疑是该项目最值得骄傲的结果之一。</p><!-- But it also meant the compiler was free to make [more aggressive code motion optimizations](https://www.cs.princeton.edu/courses/archive/fall10/cos597C/docs/memory-models.pdf), without any sacrifices to thishighly productive programming model.  In other words, we got the best of both worlds. --><p>同时这也意味着编译器可以自由地进行<a href="https://www.cs.princeton.edu/courses/archive/fall10/cos597C/docs/memory-models.pdf" target="_blank" rel="noopener">更激进的代码移动优化</a>，而不会牺牲编程模型的高效性。换句话说，我们实现了两全其美。</p><!-- A select few kernel developers had to think about the memory ordering model of the underlying machine.  These were thepeople implementing the async model itself.  For that, we eliminated C#'s notion of `volatile` -- which is [utterlybroken anyway](http://joeduffyblog.com/2010/12/04/sayonara-volatile/) -- in favor of something more like C++[`atomic`s](http://en.cppreference.com/w/cpp/atomic).  That model is quite nice for two reasons.  First, what kind offence you need is explicit for every read and write, where it actually matters.  (ences affect the uses of a variable,not its declaration.  Second, the explicit model tells the compiler more information about what optimizations can orcannot take place, again at a specific uses, where it matters most. --><p>不过少数的内核开发者还是不得不考虑底层机器的内存排序模型，因为他们是实现异步模型本身的那群人。 为此，我们消除了C#中<a href="http://joeduffyblog.com/2010/12/04/sayonara-volatile/" target="_blank" rel="noopener">无论如何都会被完全破坏</a>的<code>volatile</code>概念，并支持类似于C++中<a href="http://en.cppreference.com/w/cpp/atomic" target="_blank" rel="noopener"><code>atomic</code></a>的机制。 出于以下两个原因，该模型是非常不错的。 首先，对于每一次读写操作，你需要什么样的内存屏障是明确的，实际上也是重要的（因为这会影响变量的使用，而不是它的声明）；其次，显式模型告诉编译器关于哪些可以优化而哪些不行的更多信息，对于具有特定用途的场景中，则才是最重要的。</p><!-- ## Error model --><h2 id="错误模型"><a href="#错误模型" class="headerlink" title="错误模型"></a>错误模型</h2><!-- Our error model journey was a long one and will be the topic of a future post.  In a nutshell, however, we experimentedwith two ends of the spectrum -- exceptions and return codes -- and lots of points in betweeen. --><p>我们的错误模型之旅很长，并且将成为未来一篇文章的主题。 然而，简而言之，我们试验了错误处理频谱的两个相对的端点——异常和返回错误代码，以及两者之间的许多折衷点。</p><!-- Here is what we found from a code quality perspective. --><p>以下是我们从代码质量角度的一些发现。</p><!-- Return codes are nice because the type system tells you an error can happen.  A developer is thus forced to deal withthem (provided they don't ignore return values).  Return codes are also simple, and require far less "runtime magic"than exceptions or related mechanisms like setjmp/longjmp.  So, lots to like here. --><p>返回错误代码是不错的方式，因为从类型系统就能告知该处代码可能会发生错误，所以开发者被迫对其进行处理（前提是他们不忽略返回值）。 返回错误代码也是一种简单的方法，并且只需要比异常或相关机制（如setjmp/longjmp）少得多的“运行时戏法”。 因此我很喜欢这样的特性。</p><!-- From a code quality persective, however, return codes suck.  They force you to execute instructions in hot paths thatwouldn't have otherwise been executed, including when errors aren't even happening.  You need to return a value fromyour function -- occupying register and/or stack space -- and callers need to perform branches to check the results.Granted, we hope that these are predicted correctly, but the reality is, you're just doing more work. --><p>然而，从代码质量的角度来看，返回代码是相当糟糕，因为它们会强制在热点路径中执行本来不会执行的指令，包括甚至不会发生的错误。 而且还需要从函数返回一个值，导致占用寄存器和/或栈空间，以及调用者需要执行分支来检查结果。 当然，我们希望代码都能正确预测，但事实上后果是开发者需要做更多的工作。</p><!-- Untyped exceptions suck when you're trying to build a reliable system.  Operating systems need to be reliable.  Notknowing that there's a hidden control flow path when you're calling a function is, quite simply, unacceptable.  Theyalso require heavier weight runtime support to unwind stacks, search for handlers, and so on.  It's also a real pain inthe arse to model exceptional control flow in the compiler.  (If you don't believe me, just read through [this mailexchange](http://lists.llvm.org/pipermail/llvm-dev/2015-May/085843.html)).  So, lots to hate here. --><p>当尝试构建可靠的系统时，无类型异常是相当糟糕的，而操作系统需要的就是可靠。 当你调用一个函数时，不知道会有隐藏的控制流路径是非常不可接受的。除此之外，它们还需要更笨重的运行时的支持来展开堆栈，搜索处理程序等。 在编译器中对异常控制流进行建模也是一件真正痛苦的事情（如果你不相信我，只需阅读<a href="http://lists.llvm.org/pipermail/llvm-dev/2015-May/085843.html" target="_blank" rel="noopener">这篇邮件交流的内容</a>）。因此，我很讨厌这样的特性。</p><!-- Typed exceptions -- I got used to not saying checked exceptions for fear of hitting Java nerves -- address some of theseshortcomings, but come with their own challenges.  Again, I'll save detailed analysis for my future post. --><p>我习惯于不会因为害怕遇到Java神经而不检查的异常的有类型异常，解决了其中的一些缺点，但它也遇到了自身的挑战。 我将在未来的文章中再次进行详细分析。</p><!-- From a code quality perspective, exceptions can be nice.  First, you can organize code segments so that the "cold"handlers aren't dirtying your ICACHE on successful pathways.  Second, you don't need to perform any extra work duringthe normal calling convention.  There's no wrapping of values -- so no extra register or stack pressure -- and there'sno branching in callers.  There can be some downsides to exceptions, however.  In an untyped model, you must assumeevery function can throw, which obviously inhibits your ability to move code around. --><p>从代码质量的角度上来看，异常是不错的选择。 首先，可以组织代码段以便“冷”的处理程序不会在成功路径上占用缓存。 其次，在正常的调用约定期间，不需要执行任何额外的工作。 没有返回值的封装，因此没有额外的寄存器或栈空间的压力，并且在调用者中无需有分支。但是，异常也有一些缺点，比如在无类型模型中，必须假设每个函数都可以抛出异常，这显然抑制代码移动的能力。</p><!-- Our model ended up being a hybrid of two things: --><p>我们的模型最终是两者的混合体：</p><!-- * [Fail-fast](http://joeduffyblog.com/2014/10/13/if-youre-going-to-fail-do-it-fast/) for programming bugs.* Typed exceptions for dynamically recoverable errors. --><ul><li>对编程bug采取了<a href="http://joeduffyblog.com/2014/10/13/if-youre-going-to-fail-do-it-fast/" target="_blank" rel="noopener">快速失败（fail-fast）</a>的策略；</li><li>对动态可恢复错误采用了有类型的异常。</li></ul><!-- I'd say the ratio of fail-fast to typed exceptions usage ended up being 10:1.  Exceptions were generally used for I/Oand things that dealt with user data, like the shell and parsers.  Contracts were the biggest source of fail-fast. --><p>我可以说，快速失败与有类型异常之间使用的最终比率为10:1， 异常通常用于I/O和处理用户数据，例如shell和解析器，而合约是快速失败方式最大的来源。</p><!-- The result was the best possible configuration of the above code quality attributes: --><p>最终得到的是上述代码质量属性的最佳可能配置：</p><!-- * No calling convention impact.* No peanut butter associated with wrapping return values and caller branching.* All throwing functions were known in the type system, enabling more flexible code motion.* All throwing functions were known in the type system, giving us novel EH optimizations, like turning try/finally  blocks into straightline code when the try could not throw. --><ul><li>没有对调用约定带来影响；</li><li>封装的返回值和调用分支之间没有关联的胶水代码；</li><li>所有抛出异常的函数在类型系统中都是已知的，从而实现更灵活的代码移动；</li><li>所有抛出异常的函数在类型系统中都是已知的，为我们提供了精巧的EH优化，例如在try不会抛出异常时将try/finally块转换为直接代码。</li></ul><!-- A nice accident of our model was that we could have compiled it with either return codes or exceptions.  Thanks to this,we actually did the experiment, to see what the impact was to our system's size and speed.  The exceptions-based systemended up being roughly 7% smaller and 4% faster on some key benchmarks. --><p>我们模型一个不错的意外收获是，编译时可以选择使用返回代码返回或异常机制。 多亏了这一点，实际上我们做了实验来观察这对我们系统的体积和速度有什么影响， 基于异常的系统最终在某些关键基准测试中体积缩小了约7%，速度提高了4%。</p><!-- At the end, what we ended up with was the most robust error model I've ever used, and certainly the most performant one. --><p>最后，我们最终得到的是我使用过的最强大的错误模型，当然也是性能最好的模型。</p><!-- ## Contracts --><h2 id="合约"><a href="#合约" class="headerlink" title="合约"></a>合约</h2><!-- As implied above, Midori's programming language had first class contracts: --><p>如上所述，Midori的编程语言有作为一等公民的合约机制：</p><pre><code>void Push(T element)    requires element != null    ensures this.Count == old.Count + 1{        ...} </code></pre><!-- The model was simple: --><p>其模型很简单：</p><!-- * By default, all contracts are checked at runtime.* The compiler was free to prove contracts false, and issue compile-time errors.* The compiler was free to prove contracts true, and remove these runtime checks. --><ul><li>默认情况下，运行时检查所有合约；</li><li>编译器可以自由地证明合同是一定有误的，并发出编译时错误；</li><li>编译器可以自由地证明合约是一定无误的，并删除这些运行时的检查。</li></ul><!-- We had conditional compilation modes, however I will skip these for now.  Look for an upcoming post on our language. --><p>我们有条件编译的模式，但是我现在要跳过它们，这部分内容请关注即将发布的关于我们语言的文章。</p><!-- In the early days, we experimented with contract analyzers like MSR's [Clousot](http://research.microsoft.com/pubs/138696/Main.pdf), to prove contracts.  For compile-time reasons, however, we had toabandon this approach.  It turns out compilers are already very good at doing simple constraint solving and propagation.So eventually we just modeled contracts as facts that the compiler knew about, and let it insert the checks wherevernecessary. --><p>在早期，我们尝试使用像MSR的<a href="http://research.microsoft.com/pubs/138696/Main.pdf" target="_blank" rel="noopener">Clousot</a>这样的合约分析器来证明合约，然而，出于编译时的原因，我们不得不放弃这种方法。事实证明，编译器已经非常擅长于简单的约束求解和传播。 因此，最终我们只是将合约建模为编译器已知的事实，并让它在必要的时候插入检查代码。</p><!-- For example, the loop optimizer complete with range information above can already leverage checks like this: --><p>例如，完成上述范围信息的循环优化器已经可以利用如下的检查：</p><pre><code>void M(int[] array, int index) {    if (index &gt;= 0 &amp;&amp; index &lt; array.Length) {        int v = array[index];        ...    }} </code></pre><!-- to eliminate the redundant bounds check inside the guarded if statement.  So why not also do the same thing here? --><p>为了消除在防御性if语句中的冗余边界检查，为什么不在这里做同样的事情呢？</p><pre><code>void M(int[] array, int index)        requires index &gt;= 0 &amp;&amp; index &lt; array.Length {    int v = array[index];    ...} </code></pre><!-- These facts were special, however, when it comes to separate compilation.  A contract is part of a method's signature,and our system ensured proper [subtyping substitution](https://en.wikipedia.org/wiki/Liskov_substitution_principle),letting the compiler do more aggressive optimizations at separately compiled boundaries.  And it could do theseoptimizations faster because they didn't depend on global analysis. --><p>然而，当涉及独立的编译时，这些事实是特殊的。 合约是方法签名的一部分，我们的系统确保了合适的<a href="https://en.wikipedia.org/wiki/Liskov_substitution_principle" target="_blank" rel="noopener">子类型替换</a>，让编译器在独立编译的边界上进行更激进的优化。 它可以更快地完成这些优化，因为它们不依赖于全局分析。</p><!-- ## Objects and allocation --><h2 id="对象和分配"><a href="#对象和分配" class="headerlink" title="对象和分配"></a>对象和分配</h2><!-- In a future post, I'll describe in great detail our war with the garbage collector.  One technique that helped us win,however, was to aggressively reduce the size and quantity of objects a well-behaving program allocated on the heap.This helped with overall working set and hence made programs smaller and faster. --><p>在以后的文章中，我将详细描述我们与垃圾收集器之间的博弈， 然而，帮助我们取胜的技术是积极地减少行为良好的程序在堆上分配对象的大小和数量。这有助于优化整体工作集，从而使程序更小更快。</p><!-- The first technique here was to shrink object sizes. --><p>第一种技术是减少对象大小。</p><!-- In C# and most Java VMs, objects have headers.  A standard size is a single word, that is, 4 bytes on 32-bitarchitectures and 8 bytes on 64-bit.  This is in addition to the vtable pointer.  It's typically used by the GC to markobjects and, in .NET, is used for random stuff, like COM interop, locking, memozation of hash codes, and more.  (Even[the source code calls it the "kitchen sink"](https://github.com/dotnet/coreclr/blob/master/src/vm/syncblk.h#L29).) --><p>在C#和大多数Java VM中说，所有的对象都有对象头。 对象头的标准大小是单字（word），即32位体系结构上的4字节和64位上的8字节。作为vtable指针的扩充，它通常用于标记对象，在.NET中用于随机内容，如COM互操作，加锁，哈希值的记忆化等等（<a href="https://github.com/dotnet/coreclr/blob/master/src/vm/syncblk.h#L27" target="_blank" rel="noopener">甚至在源代码它被为“厨房的水槽”</a>）。</p><!-- Well, we ditched both. --><p>好吧，对于两者，我们最终都抛弃了。</p><!-- We didn't have COM interop.  There was no unsafe free-threading so there was no locking (and [locking on random objectsis a bad idea anyway](http://joeduffyblog.com/2006/10/26/concurrency-and-the-impact-on-reusable-libraries/)).  Our`Object` didn't define a `GetHashCode`.  Etc.  This saved a word per object with no discernable loss in the programmingmodel (actually, to the contrary, it was improved), which is nothing to shake a stick at. --><p>我们没有COM互操作，没有不安全的自由线程，因此没有加锁操作（任何情况下，<a href="http://joeduffyblog.com/2006/10/26/concurrency-and-the-impact-on-reusable-libraries/" target="_blank" rel="noopener">给任意对象上锁</a>都是一个坏主意）。 我们的<code>Object</code>也没有定义<code>GetHashCode</code>等等。这为每个对象节省了一个单字的空间大小，并且在编程模型中没有明显的开销（实际上恰恰相反的是，还得到了某些改进），因此也就没什么可说的了。</p><!-- At that point, the only overhead per object was the vtable pointer.  For structs, of course there wasn't one (unlessthey were boxed).  And we did our best to eliminate all of them.  Sadly, due to RTTI, it was difficult to be aggressive.I think this is another area where I'd go back and entirely upend the C# type system, to follow a more C, C++, or evenmaybe Go-like, model.  In the end, however, I think we did get to be fairly competitive with your average C++ program. --><p>此时，每个对象的唯一开销就是vtable指针。 对于结构来说，当然是没有vtable指针的（除非它们采用了box的方式）。我们尽力消除所有这些开销，但可悲的是，由于RTTI方式的使用，很难将优化变得激进。我认为这是另一个我想回去重新解决，并完全颠覆C#类型系统的领域，使得其能效仿更多的C，C++甚至是类似Go的模型。 不过最后，我认为我们确实取得了与普通C++程序比拟的竞争力。</p><!-- There were padding challenges.  Switching the `struct` layout from C#'s current default of `sequential`, to ourpreferred default of `auto`, certainly helped.  As did optimizations like the well-known C++ [empty base optimization](http://en.cppreference.com/w/cpp/language/ebo). --><p>这里充满了挑战。正如著名的C++<a href="http://en.cppreference.com/w/cpp/language/ebo" target="_blank" rel="noopener">空基优化</a>一样，将<code>struct</code>布局从C#的当前默认的<code>sequential</code>切换到我们首选的默认<code>auto</code>方式，肯定会有所帮助。</p><!-- We also did aggressive escape analysis in order to more efficiently allocate objects.  If an object was found to bestack-confined, it was allocated on the stack instead of the heap.  Our initial implementation of this moved somewherein the neighborhood of 10% static allocations from the heap to the stack, and let us be far more aggressive aboutpruning back the size of objects, eliminating vtable pointers and entire unused fields.  Given how conservative thisanalysis had to be, I was pretty happy with these results. --><p>我们还进行了激进的逃逸分析，以便更有效地分配对象，如果发现一个对象是能够在栈上分配，则它将被分配在栈而不是堆上。 我们的初始实现能够将10%左右的静态分配从堆移动到到栈上，这激励我们更加激进地减少对象的大小，消除vtable指针和整个未使用的字段。 考虑到这种分析是如此的保守，我对这样的结果非常满意。</p><!-- We offered a hybrid between C++ references and Rust borrowing if developers wanted to give the compiler a hint while atthe same time semantically enforcing some level of containment.  For example, say I wanted to allocate a little array toshare with a callee, but know for sure the callee does not remember a reference to it.  This was as simple as saying: --><p>如果开发者想要给编译器提供提示信息，同时在语义上强制执行某种程度的包含，我们也提供了C++引用和Rust借用之间的混合。 例如，假设我想分配一个小型数组与被调用者进行共享，但确定被调用者不存在对它的引用，其做法也就像下面这样简单：</p><!--     void Caller() {        Callee(new[] { 0, 1, ..., 9 });    }    void Callee(int[]& a) {        ... guaranteed that `a` does not escape ...    } --><pre><code>void Caller() {    Callee(new[] { 0, 1, ..., 9 });}void Callee(int[]&amp; a) {    ... 保证a不会逃逸 ...} </code></pre><!-- The compiler used the `int[]&` information to stack allocate the array and, often, eliminate the vtable for itentirely.  Coupled with the sophisticated elimination of bounds checking, this gave us something far closer to Cperformance. --><p>编译器使用<code>int[]&amp;</code>信息在栈上分配数组，并且通常完全消除了vtable的生成， 再加上复杂的边界检查消除算法，这一切给了我们更接近C性能的程序。</p><!-- Lambdas/delegates in our system were also structs, so did not require heap allocation.  The captured display frame wassubject to all of the above, so frequently we could stack allocate them.  As a result, the following code was heapallocation-free; in fact, thanks to some early optimizations, if the callee was inlined, it ran as though the actuallambda body was merely expanded as a sequence of instructions, with no call over head either! --><p>系统中的Lambda/delegate也是结构，因此无需堆的分配。 捕获的显示框架受上述所有问题的影响，因此我们经常可以在栈上分配它们，造成的结果是下面的代码是完全无需堆分配的。 事实上，由于一些早期的优化，如果被调用者被内联优化，那么它就像通过实际的lambda函数体被扩展为一系列指令一样，而没有产生任何实际的函数调用！</p><!--     void Caller() {        Callee(() => ... do something ... );    }    void Callee(Action& callback) {        callback();    } --><pre><code>void Caller() {    Callee(() =&gt; ... 完成某些任务... );}void Callee(Action&amp; callback) {    callback();} </code></pre><!-- In my opinion, this really was the killer use case for the borrowing system.  Developers avoided lambda-based APIs inthe early days before we had this feature for fear of allocations and inefficiency.  After doing this feature, on theother hand, a vibrant ecosystem of expressive lambda-based APIs flourished. --><p>在我看来，这确实是借用系统的杀手级用例。 由于担心分配和效率低下的问题，开发者在我们拥有此功能之前的早期就避免使用基于lambda的API。 而另一方面，在完成此功能之后，充满活力的基于lambda的API生态系统又重新蓬勃发展。</p><!-- # Throughput --><h1 id="吞吐量"><a href="#吞吐量" class="headerlink" title="吞吐量"></a>吞吐量</h1><!-- All of the above have to do with code quality; that is, the size and speed of the resulting code.  Another importantdimension of compiler performance, however, is *throughput*; that is, how quickly you can compile the code.  Here tooa language like C# comes with some of its own challenges. --><p>以上所有的内容都与代码质量，也就是说，与生成代码的大小和执行速度有关。 然而，编译器性能的另一个重要方面是吞吐量，也就是说代码编译的速度能有多快。 对此，像C#这样的语言也存在一些自身的挑战。</p><!-- The biggest challenge we encountered has less to do with the inherently safe nature of a language, and more to do withone very powerful feature: parametric polymorphism.  Or, said less pretentiously, generics. --><p>我们遇到的最大挑战与语言固有的安全性无关，更多的是与一个非常强大的功能有关：参数多态，或者不那么自负的说法——泛型。</p><!-- I already mentioned earlier that generics are just a convenient copy-and-paste mechanism.  And I mentioned somechallenges this poses for code size.  It also poses a problem for throughput, however.  If a `List<T>` instantiationcreates 28 types, each with its own handful of methods, that's just more code for the compiler to deal with.  Separatecompilation helps, however as also noted earlier, generics often flow across module boundaries.  As a result, there'slikely to be a non-trivial impact to compile time.  Indeed, there was. --><p>我之前已经提到过，泛型只是一种方便的复制粘贴机制。 并在本文中提到了它对代码大小带来的一些挑战，然而，其也对编译吞吐量也带来了问题。 如果<code>List&lt;T&gt;</code>的实例创建了28种类型，每种类型都有自己的少数几种方法，那编译器将需要处理更多代码。 独立编译会有所帮助，但是如前所述，泛型通常是跨越模块边界的方式使用。因此，其可能会对编译时间产生的影响，而事实上也是如此。</p><!-- In fact, this is not very different from where most C++ compilers spend the bulk of their time.  In C++, it's templates.More modern C++ code-bases have similar problems, due to heavy use of templated abstractions, like STL, smart pointers,and the like.  Many C++ code-bases are still just "C with classes" and suffer this problem less. --><p>实际上，这与大多数C++编译器花费大量时间的地方没有太大差别。 在C++中，泛型叫做模板，由于大量使用模板化抽象（如STL，智能指针等），更现代的C++代码库具有与上述相类似的问题。 为了避免这个问题带来的影响，许多C++代码库仍然只是“带有类的C”的形式。</p><!-- As I mentioned earlier, I wish we had banished RTTI.  That would have lessened the generics problem.  But I would guessgenerics still would have remained our biggest throughput challenge at the end of the day. --><p>正如我之前提到的，我希望我们放弃RTTI，这么做会减少泛型的问题。但我认为即便如此，泛型仍然是我们在编译吞吐量上最大的挑战。</p><!-- The funny thing -- in a not-so-funny kind of way -- is that you can try to do analysis to prune the set of generics and,though it is effective, this analysis takes time.  The very thing you're trying to save. --><p>有一种看起来不是，但实际确实有趣的方式是，你可以尝试进行分析以裁剪一些泛型。虽然这种方式是有效的，但分析也需要时间，而这就是需要节省的地方。</p><!-- A metric we got in the habit of tracking was how much slower AOT compiling a program was than simply C# compiling it.This was a totally unfair comparison, because the C# compiler just needs to lower to MSIL whereas an AOT compler needsto produce machine code.  It'd have been fairer to compare AOT compiling to JIT compiling.  But no matter, doing a greatjob on throughput is especially important for a C# audience.  The expectation of productivity was quite high.  This wastherefore the key metric we felt customers would judge us on, and so we laser-focused on it. --><p>我们习惯跟踪的一个指标是AOT编译程序的速度比仅仅进行C#编译要慢多少。 而这是一个完全不公平的比较方式，因为C#编译器只需要生成MSIL，而AOT编译器则需要生成机器代码。而将AOT编译与JIT编译进行比较是一个比较公平的方式，但无论如何，在吞吐量方面做得好对于C#的使用者来讲尤其重要，因为对开发生产效率的期望至始至终是相当高的， 因此，这是我们认为客户对我们评价的关键指标，所以我们也专注于此。</p><!-- In the early days, the number was ridiculously bad.  I remember it being 40x slower.  After about a year and half withintense focus we got it down to *3x for debug builds* and *5x for optimized builds*.  I was very happy with this! --><p>在早期，这个指标是非常糟糕的，我记得是它慢了40倍。 经过大约一年半的重点优化，我们将<em>debug版本降低到3倍左右</em>，而<em>optimized版本降低到5倍左右</em>。 对于这样的成绩我很高兴！</p><!-- There was no one secret to achieving this.  Mostly it had to do with just making the compiler faster like you would anyprogram.  Since we built the compiler using Midori's toolchain, however -- and compiled it using itself -- often thiswas done by first making Midori better, which then made the the compiler faster.  It was a nice virtuous loop.  We hadreal problems with string allocations which informed what to do with strings in our programming model.  We found crazygenerics instantiation closures which forced us to eliminate them and build tools to help find them proactively.  Etc. --><p>实现这一点没有任何秘密可言。 大多数情况下，它只需要像任何其他程序一样优化使得将编译器更快。 因为我们使用Midori的工具链构建了编译器，再使用它编译Midori自身，所以通常这是通过首先使Midori变得更好，然后再使编译器更快来达到目的的，而这是一个很不错的良性循环。 我们遇到了字符串分配的实际问题，这些问题告诉我们在编程模型中如何处理字符串，我们也发现了，疯狂的泛型实例化闭包迫使我们消除并构建工具来主动找到它们等等问题。</p><!-- # Culture --><h1 id="文化"><a href="#文化" class="headerlink" title="文化"></a>文化</h1><!-- A final word before wrapping up.  Culture was the most important aspect of what we did.  Without the culture, such anamazing team wouldn't have self-selected, and wouldn't have relentlessly pursued all of the above achievements.  I'lldevote an entire post to this.  However, in the context of compilers, two things helped: --><p>结束前的最后一节我想说的是，文化是我们所做的最重要一方面的工作。 如果没有文化，这样一支出色的团队也就不会进行自我选择，也不会狠狠地追求上述所有这些成绩，我会花一整篇文章谈论这方面的内容。但是，回到关于编译器的内容，有两件事是有所帮助的：</p><!-- 1. We measured everything in the lab.  "If it's not in the lab, it's dead to me."2. We reviewed progress early and often.  Even in areas where no progress was made.  We were habitually self-critical. --><ol><li>我们在试验环境中测量所有指标。 “如果它不在实验室里，那对我来说它已经死了”。</li><li>我们提前和经常性审查进展情况，即使在没有取得进展的地方也是如此，而且我们也习惯性地进行自我批判性思考。</li></ol><!-- Every sprint, we had a so-called "CQ Review" (where CQ stands for "code quality").  The compiler team prepared for a fewdays, by reviewing every benchmark -- ranging from the lowest of microbenchmarks to compiling and booting all of Windows-- and investigating any changes.  All expected wins were confirmed (we called this "confirming your kill"), anyunexpected regressions were root cause analyzed (and bugs filed), and any wins that didn't materialize were alsoanalyzed and reported on so that we could learn from it.  We even stared at numbers that didn't change, and askedourselves, why didn't they change.  Was it expected?  Do we feel bad about it and, if so, how will we change nextsprint?  We reviewed our competitors' latest compiler drops and monitored their rate of change.  And so on. --><p>对于每个开发周期（sprint），我们都有一个所谓的“CQ Review”（其中CQ代表“代码质量”）过程。 通过审查每个基准测试，编译团队准备了好几天的时间，从最低层次的微基准测试到编译和启动所有Windows，并在此过程中分析任何变化。 所有预期的胜利都得到了确认（我们称之为“确认你的毙敌”），任何意料之外的退化都进过根本原因的分析（并提交bug），任何未实现的进展也会被分析和形成报告，以便我们可以从中学习。 我们甚至盯着那些没有改变的数字，并问自己，为什么他们没有改变？这是预期的吗？ 我们是否对其感觉不好，如果是这样，我们将如何在下个开发周期中进行改变？我们审查了竞争对手的最新编译器的进展并监控其变化率等等。</p><!-- This process was enormously healthy.  Everyone was encouraged to be self-critical.  This was not a "witch hunt"; it wasan opportunity to learn as a team how to do better at achieving our goals. --><p>这个过程非常的健康，每个人都被鼓励进行自我批判性思考。这不是所谓的“猎巫”运动，而是一个作为团队学习如何更好地实现目标的机会。</p><!-- Post-Midori, I have kept this process.  I've been surprised at how contentious this can be with some folks.  They getthreatened and worry their lack of progress makes them look bad.  They use "the numbers aren't changing because that'snot our focus right now" as justification for getting out of the rhythm.  In my experience, so long as the code ischanging, the numbers are changing.  It's best to keep your eye on them lest you get caught with your pants around yourankles many months later when it suddenly matters most.  The discipline and constant drumbeat are the most importantparts of these reviews, so skipping even just one can be detrimental, and hence was verboten. --><p>在经历了Midori之后的，我保留了这个过程。 我对一些成员对此的争议感到惊讶，他们受到威胁并担心缺乏进展会使他们看起来很糟糕。 他们使用“数字没有变是因为它们现在不是我们关注的焦点”作为摆脱节奏的理由。 根据我的经验，只要代码发生变化，数字就会发生变化，因此最好密切关注它们，以免在几个月后当它突然成为最重要的任务时，陷入尴尬的境地。 处罚和持续的鼓声是这些评论中最重要的部分，因此即使只忽略掉其中一个也可能是有害的，所以也是不被允许的。</p><!-- This process was as much our secret sauce as anything else was. --><p>该过程和其他所有东西一样都是我们的秘密武器。</p><!-- # Wrapping Up --><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><!-- Whew, that was a lot of ground to cover.  I hope at the very least it was interesting, and I hope for the incredibleteam who built all of this that I did it at least a fraction of justice.  (I know I didn't.) --><p>哇噢，有如此多需要被提到的内容。我希望这些内容至少很有意思，并希望能够建立所有这一切令人难以置信内容的团队，至少做到一小部分的正义（尽管我知道我没有做到）。</p><!-- This journey took us over a decade, particularly if you account for the fact that both Bartok and Phoenix had existedfor many years even before Midori formed.  Merely AOT compiling C#, and doing it well, would have netted us many of thebenefits above.  But to truly achieve the magical native-like performance, and indeed even exceed it in certain areas,required some key "whole system" architectural bets.  I hope that some day we can deliver safety into the world at thislevel of performance.  Given the state of security all-up in the industry, mankind seriously needs it. --><p>这段旅程花了我们十多年的时间，特别是如果当你考虑到Bartok和Phoenix在Midori成立之前已存在多年的事实。 仅仅通过AOT的方式编译C#并且做得很好，会让我们收获上述许多的好处。 但要真正获得神奇的原生性能，甚至在某些领域超过它，需要一些关键的“整个系统”结构上的赌注。我希望有一天我们能够在这个性能水平上为世界提供安全性的保障。 鉴于业界的安全状况，全世界真的都需要它。</p><!-- I've now touched on our programming language enough that I need to go deep on it.  Tune in next time! --><p>到目前为止，我已经简要提及到我们的编程语言，使得我需要进一步对其深入下去。那么，下次再见！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- 
In my [first Midori post](http://joeduffyblog.com/2015/11/03/a-tale-of-three-safeties/), I described how safety was the
foundation of 
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="操作系统" scheme="https://blog.zhangpf.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="安全" scheme="https://blog.zhangpf.com/tags/%E5%AE%89%E5%85%A8/"/>
    
      <category term="Midori" scheme="https://blog.zhangpf.com/tags/Midori/"/>
    
      <category term="翻译" scheme="https://blog.zhangpf.com/tags/%E7%BF%BB%E8%AF%91/"/>
    
      <category term="编译" scheme="https://blog.zhangpf.com/tags/%E7%BC%96%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>Midori博客系列翻译（3）——一切皆异步</title>
    <link href="https://blog.zhangpf.com/2018/11/25/midori/3-asynchronous-everything/"/>
    <id>https://blog.zhangpf.com/2018/11/25/midori/3-asynchronous-everything/</id>
    <published>2018-11-25T10:03:00.000Z</published>
    <updated>2020-01-29T02:19:38.928Z</updated>
    
    <content type="html"><![CDATA[<!-- Midori was built out of many ultra-lightweight, fine-grained processes, connected through strongly typed message passinginterfaces.  It was common to see programs that'd've classically been single, monolithic processes -- perhaps with someinternal multithreading -- expressed instead as dozens of small processes, resulting in natural, safe, and largelyautomatic parallelism.  Synchronous blocking was flat-out disallowed.  This meant that literally everything wasasynchronous: all file and network IO, all message passing, and any "synchronization" activities like rendezvousingwith other asynchronous work.  The resulting system was highly concurrent, responsive to user input, and scaled like thedickens.  But as you can imagine, it also came with some fascinating challenges. --><p>Midori由大量通过强类型消息传递接口相互连接的，超轻量级细粒度进程构建而成。我们传统上常见的程序是可能带有一些内部线程的单一宏进程。而在Midori中，进程则由数十个小型进程所表示，从而实现自然，安全和大部分的自动并行化。同时，在Midori中，显然同步阻塞是不允许的，这意味着包括所有文件和网络I/O，消息传递，以及诸如与其他异步任务进行会合等任何“同步”活动，在字面上的一切都是异步的。这样的设计使得所实现的系统可高度并发，且能快速响应用户的输入，以及灵活的扩展。但也正如你想象的那样，这在设计上也带来了一些具有吸引力的挑战。</p><!-- ## Asynchronous Programming Model --><h2 id="异步编程模型"><a href="#异步编程模型" class="headerlink" title="异步编程模型"></a>异步编程模型</h2><!-- The asynchronous programming model looked a lot like C#'s async/await on the surface. --><p>乍一眼看，异步编程模型很像C#的async/await。</p><!-- That's not a coincidence.  I was the architect and lead developer on [.NET tasks](https://en.wikipedia.org/wiki/Parallel_Extensions).  As the concurrency architect on Midori, coming off just shippingthe .NET release, I admit I had a bit of a bias.  Even I knew what we had wouldn't work as-is for Midori, however, sowe embarked upon a multi-year journey.  But as we went, we worked closely with the C# team to bring some of Midori'sapproaches back to the shipping language, and had been using a variant of the async/await model for about a year when C#began looking into it.  We didn't bring all the Midori goodness to .NET, but some of it certainly showed up, mostly inthe area of performance.  It still kills me that I can't go back in time and make .NET's task a struct. --><p>而这并不是巧合，因为我也是<a href="https://en.wikipedia.org/wiki/Parallel_Extensions" target="_blank" rel="noopener">.NET task</a>的架构师和开发主管。作为Midori的并发设计者，对于即将发布的.NET新版本，我必须承认我对异步编程模型存有偏爱。 因此，即使知道对于Midori，它可能不会像预期的那样发展，但我们依旧开始了数年的开发旅程。而当离开时，我们与C#团队密切合作，将一些在Midori上形成的方法带回到C#中，并在C#开始探索异步模型时，也使用了async/await模型的变体大约一年之久。虽然我们未能将Midori的所有优点带到.NET中，但也确实实现了其中的一部分，而这部分主要是在性能方面。 而对于无法回到过去将.NET的task变成结构体，这一点至今让我感到遗憾。</p><!-- But I'm getting ahead of myself.  The journey to get to this point was a long one, and we should start at the beginning. --><p>但我却逐步超越了我自己。到达这一步的旅程是漫长的，让我们从头讲起。</p><h2 id="Promises"><a href="#Promises" class="headerlink" title="Promises"></a>Promises</h2><!-- At the core of our asynchronous model was a technology called [promises](https://en.wikipedia.org/wiki/Futures_and_promises).  These days, the idea is ubiquitous.  The way we used promises,however, was more interesting, as we'll start to see soon.  We were heavily influenced by the [E system](https://en.wikipedia.org/wiki/E_(programming_language)).  Perhaps the biggest difference compared to popularasynchronous frameworks these days is there was no cheating.  There wasn't a single synchronous API available. --><p>我们使用的异步模型的核心是一项名为<a href="https://en.wikipedia.org/wiki/Futures_and_promises" target="_blank" rel="noopener">Promise</a>的技术。虽然在如今，该想法已是无处不在，但是，你很快将会看到，我们所使用Promise的方式将更加有趣。受到<a href="http://t.cn/EV8S8gf" target="_blank" rel="noopener">E语言系统</a>的强烈影响，使得与流行的异步框架相比，我们最大的不同在于做到了完全的异步——例如，在我们的系统中，没有一个同步的API。</p><!-- The first cut at the model used explicit callbacks.  This'll be familiar to anybody who's done Node.js programming.  Theidea is you get a `Promise<T>` for any operation that will eventually yield a `T` (or fail).  The operation producingthat may be running asynchronously within the process or even remotely somewhere else.  The consumer doesn't need toknow or care.  They just deal with the `Promise<T>` as a first class value and, when the `T` is sought, must rendezvous. --><p>该模型的最重要的方式是使用了显式的回调，而这对于任何使用过Node.js编程的人都非常熟悉。 这里的想法是为任何最终会产生<code>T</code>（或失败）的操作产生一个<code>Promise&lt;T&gt;</code>。 产生的操作可以在进程内异步运行，甚至可以远程执行。结果的使用者无需知道具体运行的位置，他们只需要将<code>Promise&lt;T&gt;</code>作为一等类型值来处理，也就是说当需要获取<code>T</code>值时，必须进行会合（rendezvous）操作。</p><!-- The basic callback model started something like this: --><p>其基本的回调模型如下：</p><!--     Promise<T> p = ... some operation ...;    ... optionally do some things concurrent with that operation ...;    Promise<U> u = Promise.When(        p,        (T t) => { ... the T is available ... },        (Exception e) => { ... a failure occurred ... }    ); --><pre><code>Promise&lt;T&gt; p = ... 一些操作 ...;... 可选地，与该操作并发地完成其他操作 ...;Promise&lt;U&gt; u = Promise.When(    p,    (T t) =&gt; { ... 当T变得可用时 ... },    (Exception e) =&gt; { ... 产生失败时 ... }); </code></pre><!-- Eventually we switched over from static to instance methods: --><p>最终，我们从静态方法切换到实例方法：<!--     Promise<U> u = p.WhenResolved(        (T t) => { ... the T is available ... },        (Exception e) => { ... a failure occurred ... }    ); --></p><pre><code>Promise&lt;U&gt; u = p.WhenResolved(    (T t) =&gt; { ... T变得可用时 ... },    (Exception e) =&gt; { ... 产生失败时 ... }); </code></pre><!-- Notice that the promises chain.  The operation's callbacks are expected to return a value of type `U` or throw anexception, as appropriate.  Then the recipient of the `u` promise does the same, and so on, and so forth. --><p>请注意这里的Promise链：操作的回调返回类型为<code>U</code>的值或者根据需要抛出异常。然后，值为<code>u</code>的Promise使用者也将如此操作，依此类推进行下去。</p><!-- This is [concurrent](https://en.wikipedia.org/wiki/Concurrent_computing#Concurrent_programming_languages) [dataflow](https://en.wikipedia.org/wiki/Dataflow_programming) programming.  It is nice because the true dependencies of operationsgovern the scheduling of activity in the system.  A classical system often results in work stoppage not because of truedependencies, but [false dependencies](https://en.wikipedia.org/wiki/Data_dependency), like the programmer justhappening to issue a synchronous IO call deep down in the callstack, unbeknownst to the caller. --><p>这是一种<a href="https://en.wikipedia.org/wiki/Concurrent_computing#Concurrent_programming_languages" target="_blank" rel="noopener">并发</a>的<a href="https://en.wikipedia.org/wiki/Dataflow_programming" target="_blank" rel="noopener">数据流</a>编程方式，其优点是操作的正确依赖关系控制着系统中活动的调度。 经典系统经常停止工作，不是因为正确的依赖关系，而是由于<a href="https://en.wikipedia.org/wiki/Data_dependency" target="_blank" rel="noopener">错误的依赖关系</a>所导致，例如在调用堆栈的底层中发出同步I/O调用，而上层的调用者却对此一无所知。</p><!-- In fact, this is one of the reasons your screen bleaches so often on Windows.  I'll never forget a paper a few yearsback finding one of the leading causes of hangs in Outlook.  A commonly used API would occasionally enumerate Postscriptfonts by attempting to talk to the printer over the network.  It cached fonts so it only needed to go to the printeronce in a while, at unpredictable times.  As a result, the "good" behavior led developers to think it safe to call fromthe UI thread.  Nothing bad happened during testing (where, presumably, the developers worked on expensive computerswith near-perfect networks).  Sadly, when the network flaked out, the result was 10 second hangs with spinning donutsand bleachy white screens.  To this day, we still have this problem in every OS that I use. --><p>事实上，这也是Windows系统上经常出现白屏的原因之一。 对此，我依然记得几年前的一篇论文，它指出了Outlook中出现挂起的主要原因：某个常用的API偶尔会尝试通过网络，与打印机进行通信来枚举Postscript字体。 请求所花费的时间虽然不可预测，但由于系统缓存了字体，所以只需要偶尔真正向打印机发出请求。 因此，这种表面“良好”的行为使开发人员误以为从UI线程进行调用是安全的。 测试期间（开发人员在造价昂贵的计算机上使用近乎完美的网络时）没有任何不良的后果产生， 但遗憾的是，当网络状况恶化时，其造成的后果是与旋转的“甜甜圈”（光标）和白屏相伴的10秒钟系统挂起。而到目前为止，在我所使用的所有操作系统中，此问题依然存在。</p><!-- The issue in this example is the possibility for high latency wasn't apparent to developers calling the API.  It waseven less apparent because the call was buried deep in a callstack, masked by virtual function calls, and so on.  InMidori, where all asynchrony is expressed in the type system, this wouldn't happen because such an API wouldnecessarily return a promise.  It's true, a developer can still do something ridiculous (like an infinite loop on theUI thread), but it's a lot harder to shoot yourself in the foot.  Especially when it came to IO. --><p>上述举例中产生问题的原因是，调用API的开发人员不清楚可能会产生高延迟。这种延迟甚至非常隐蔽，因为其深埋于调用栈中，被虚函数调用所掩盖。 在Midori中，所有的异步都在类型系统中表示，上述的例子将不会发生，因为这样的API将返回一个Promise类型。 没错，开发人员虽然仍可以做一些荒谬的事情（比如在UI线程上产生死循环），但像这种搬起石头砸自己脚的事情将会变得困难得多，特别是当涉及到I/O操作时。</p><!-- What if you didn't want to continue the dataflow chain?  No problem. --><p>如果想停止链式数据流怎么办？这也没有问题。<!--     p.WhenResolved(        ... as above ...    ).Ignore(); --></p><pre><code>p.WhenResolved(    ... 如上 ...).Ignore(); </code></pre><!-- This turns out to be a bit of an anti-pattern.  It's usually a sign that you're mutating shared state. --><p>结果证明这是一种反模式，它通常表明你正在改变共享状态。</p><!-- The `Ignore` warrants a quick explanation.  Our language didn't let you ignore return values without being explicitabout doing so.  This specific `Ignore` method also addded some diagnostics to help debug situations where youaccidentally ignored something important (and lost, for example, an exception). --><p>这里的<code>Ignore</code>需要一点解释是——除非你显式地这样做，编程语言不允许忽略其返回值。 同时，这种特定的<code>Ignore</code>使用还添加了一些诊断功能，以帮助调试你意外忽略的重要事项（和异常等失败情况）。</p><!-- Eventually we added a bunch of helper overloads and APIs for common patterns: --><p>最后，我们为常见的模式添加了一些作为辅助的重载和API：</p><!--     // Just respond to success, and propagate the error automatically:    Promise<U> u = p.WhenResolved((T t) => { ... the T is available ... });    // Use a finally-like construct:    Promise<U> u = p.WhenResolved(        (T t) => { ... the T is available ... },        (Exception e) => { ... a failure occurred ... },        () => { ... unconditionally executes ... }    );    // Perform various kinds of loops:    Promise<U> u = Async.For(0, 10, (int i) => { ... the loop body ... });    Promise<U> u = Async.While(() => ... predicate, () => { ... the loop body ... });    // And so on. --><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 只响应成功情况，并自动抛出错误：</span><br><span class="line">Promise&lt;U&gt; u = p.WhenResolved((T t) =&gt; &#123; ... T变得可用... &#125;);</span><br><span class="line"></span><br><span class="line">// 使用类似finally的结构：</span><br><span class="line">Promise&lt;U&gt; u = p.WhenResolved(</span><br><span class="line">    (T t) =&gt; &#123; ... T可用 ... &#125;,</span><br><span class="line">    (Exception e) =&gt; &#123; ... 产生失败时 ... &#125;,</span><br><span class="line">    () =&gt; &#123; ... 无条件执行... &#125;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">// 执行各种循环：</span><br><span class="line">Promise&lt;U&gt; u = Async.For(0, 10, (int i) =&gt; &#123; ... 循环体 ... &#125;);</span><br><span class="line">Promise&lt;U&gt; u = Async.While(() =&gt; ... predicate, () =&gt; &#123; ... 循环体 ... &#125;);</span><br><span class="line"></span><br><span class="line">// 等等。</span><br></pre></td></tr></table></figure><!-- This idea is most certainly not even close to new.  [Joule](https://en.wikipedia.org/wiki/Joule_(programming_language))and [Alice](https://en.wikipedia.org/wiki/Alice_(programming_language)) even have nice built-in syntax to make theotherwise clumsy callback passing shown above more tolerable. --><p>基本上可以确定的是，这并不能算的上是新颖的想法。 <a href="http://t.cn/EV8ovNL" target="_blank" rel="noopener">Joule语言</a>和<a href="http://t.cn/EV8o4DO" target="_blank" rel="noopener">Alice语言</a>甚至都已有了良好的内置语法支持，使上述繁琐笨拙的回调传递方法变得更容易使用。</p><!-- But it was not tolerable.  The model tossed out decades of familiar programming language constructs, like loops. --><p>但无法容忍的是，该模型抛弃了数十年以来熟悉的编程语言结构，例如循环。</p><!-- It got really bad.  Like really, really.  It led to callback soups, often nested many levels deep, and often in somereally important code to get right.  For example, imagine you're in the middle of a disk driver, and you see code like: --><p>这一点真的真的很糟糕，因为它往往会导致代码遭遇回调困境，通常出现在多层次的嵌套和一些非常重要故而必须正确的代码中。 例如，假设你在磁盘驱动程序中看到如下代码：</p><pre><code>Promise&lt;void&gt; DoSomething(Promise&lt;string&gt; cmd) {    return cmd.WhenResolved(        s =&gt; {            if (s == &quot;...&quot;) {                return DoSomethingElse(...).WhenResolved(                    v =&gt; {                        return ...;                    },                    e =&gt; {                        Log(e);                        throw e;                    }                );            }            else {                return ...;            }        },        e =&gt; {            Log(e);            throw e;        }    );}</code></pre><!-- It's just impossible to follow what's going on here.  It's hard to tell where the various returns return to, whatthrow is unhandled, and it's easy to duplicate code (such as the error cases), because classical block scoping isn'tavailable.  God forbid you need to do a loop.  And it's a disk driver -- this stuff needs to be reliable! --><p>所以根本不可能在这样的代码中搞清楚所有的逻辑，因为这很难判断所有return的返回位置和所有未处理异常，并且很容易出现重复的代码（例如错误处理），因为经典的块作用域不再适用。上帝禁止你使用循环，但这里是磁盘驱动程序，它需要可靠性的保证！</p><!-- ## Enter Async and Await--><h2 id="进入到Async和Await的世界"><a href="#进入到Async和Await的世界" class="headerlink" title="进入到Async和Await的世界"></a>进入到Async和Await的世界</h2><!-- [Almost](https://msdn.microsoft.com/en-us/library/hh156528.aspx) [every](http://tc39.github.io/ecmascript-asyncawait/)[major](https://www.python.org/dev/peps/pep-0492/) [language](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4134.pdf) now features async and/or await-like constructs.We began wide-scale use sometime in 2009.  And when I say wide-scale, I mean it. --><p><a href="https://msdn.microsoft.com/en-us/library/hh156528.aspx" target="_blank" rel="noopener">几乎</a><a href="http://tc39.github.io/ecmascript-asyncawait/" target="_blank" rel="noopener">所有</a>的<a href="https://www.python.org/dev/peps/pep-0492/" target="_blank" rel="noopener">主要</a><a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4134.pdf" target="_blank" rel="noopener">编程语言</a>都已具有类似async和/或await的数据结构，我们在2009年时，也已经开始对其大规模使用。这里我说大规模使用时，我是认真的。</p><!-- The async/await approach let us keep the non-blocking nature of the system and yet clean up some of the above usabilitymess.  In hindsight, it's pretty obvious, but remember back then the most mainstream language with await used at scalewas F# with its [asynchronous workflows](http://blogs.msdn.com/b/dsyme/archive/2007/10/11/introducing-f-asynchronous-workflows.aspx) (also see [this paper](http://research.microsoft.com/apps/pubs/default.aspx?id=147194)).  And despite the boon to usability and productivity,it was also enormously controversial on the team.  More on that later. --><p>async/await的方法让我们的系统保持了非阻塞的性质，并消除了上述可用性方面的混乱， 事后看来，这种优势是非常明显的，但是请不要忘了，在当时，大规模使用的最主流语言还是F#及其<a href="http://blogs.msdn.com/b/dsyme/archive/2007/10/11/introducing-f-asynchronous-workflows.aspx" target="_blank" rel="noopener">异步工作流</a> （另见<a href="http://research.microsoft.com/apps/pubs/default.aspx?id=147194" target="_blank" rel="noopener">此文</a>）。 尽管async/await对可用性和生产力方面带来了提升，但我们团队对其也存在巨大争议，对此，我将在稍后详细介绍。</p><!-- What we had was a bit different from what you'll find in C# and .NET.  Let's walk through the progression from thepromises model above to this new async/await-based one.  As we go, I'll point out the differences. --><p>我们所设计的async/await的与C#和.NET中的略有不同。 所以让我们来看看从上面的Promise模型演进到新的基于async/await模型的过程，并且在抽丝剥茧的过程中，逐渐指出其中的差异。</p><!-- We first renamed `Promise<T>` to `AsyncResult<T>`, and made it a struct.  (This is similar to .NET's `Task<T>`, howeverfocuses more on the "data" than the "computation.")  A family of related types were born:--><p>我们首先将<code>Promise&lt;T&gt;</code>重命名为<code>AsyncResult&lt;T&gt;</code>，并将其作为结构体 （这一点类似于.NET的<code>Task&lt;T&gt;</code>，但其更多地关注于“数据”而不是“计算”）。据此产生了如下的一系列相关的类型：</p><!-- * `T`: the result of a prompt, synchronous computation that cannot fail.* `Async<T>`: the result of an asynchronous computation that cannot fail.* `Result<T>`: the result of a a prompt, synchronous computation that might fail.* `AsyncResult<T>`: the result of an asynchronous computation that might fail. --><ul><li><code>T</code>：即时同步计算的结果，并且不会导致失败</li><li><code>Async&lt;T&gt;</code>：异步计算的结果，并且不会导致失败</li><li><code>Result&lt;T&gt;</code>：可能导致失败的即时同步计算的结果值</li><li><code>AsyncResult&lt;T&gt;</code>：可能导致失败的异步计算的结果值</li></ul><!-- That last one was really just a shortcut for `Async<Result<T>>`. --><p>最后一个实际上是<code>Async&lt;Result&lt;T&gt;&gt;</code>的别名。</p><!-- The distinction between things that can fail and things that cannot fail is a topic for another day.  In summary,however, our type system guaranteed these properties for us. --><p>可能失败的值和不会导致失败的值之间的区别又将是另外一个主题。但总之，我们的类型系统为我们保证了它们的属性。</p><!--Along with this, we added the `await` and `async` keywords.  A method could be marked `async`: --><p>同时增加了<code>await</code>和<code>async</code>关键字，如果一个方法被标记为<code>async</code>：</p><pre><code>async int Foo() { ... }</code></pre><!-- All this meant was that it was allowed to `await` inside of it: --><p>那么意味着该方法的内部允许存在<code>await</code>关键字：</p><pre><code>async int Bar() {    int x = await Foo();    ...    return x * x;}</code></pre><!-- Originally this was merely syntactic sugar for all the callback goop above, like it is in C#.  Eventually, however, wewent way beyond this, in the name of performance, and added lightweight coroutines and linked stacks.  More below. --><p>正如它们在C#中的那样，<code>async</code>/<code>await</code>仅仅作为上述回调方法的语法糖而存在。 但最终，我们从性能的角度出发，使得其存在意义更加广泛，并在其基础上添加了轻量级协程和链接栈。 其有关的更多内容如下。</p><!-- A caller invoking an `async` method was forced to choose: use `await` and wait for its result, or use `async` andlaunch an asynchronous operation.  All asynchrony in the system was thus explicit: --><p>调用<code>async</code>方法的调用者必须进行如下的二选一：使用<code>await</code>并等待其结果，或使用<code>async</code>并启动异步的操作。 因此，系统中的所有异步操作都将显式进行：</p><!--     int x = await Bar();        // Invoke Bar, but wait for its result.    Async<int> y = async Bar(); // Invoke Bar asynchronously; I'll wait later.    int z = await y;            // ...like now.  This waits for Bar to finish. --><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int x = await Bar();        // 调用Bar，并在等待其结果返回。</span><br><span class="line">Async&lt;int&gt; y = async Bar(); // 异步调用Bar，并在未来某个时刻处理。</span><br><span class="line">int z = await y;            // ... 正如即使计算，它将等待Bar操作完成。</span><br></pre></td></tr></table></figure><!-- This also gave us a very important, but subtle, property that we didn't realize until much later.  Because in Midori theonly way to "wait" for something was to use the asynchronous model, and there was no hidden blocking, our type systemtold us the full set of things that could "wait."  More importantly, it told us the full set of things that could notwait, which told us what was pure synchronous computation!  This could be used to guarantee no code ever blocked theUI from painting and, as we'll see below, many other powerful capabilities. --><p>直到很久以后我们才意识到，这样的作法给我们带来了非常重要但又微妙的优势。 因为在Midori中，“等待”某个事件的唯一方法是使用异步模型，并且没有任何隐藏的阻塞代码是由类型系统提供的。更重要的是，它告诉了我们所有无法用于等待的情况，并告知我们什么是纯粹的同步计算！因此，这种方式可用于保证没有代码会阻塞UI绘制，也正如将在如下看到的一样，它还包含许多其他强大的功能。</p><!-- Because of the sheer magnitude of asynchronous code in the system, we embellished lots of patterns in the language thatC# still doesn't support.  For example, iterators, for loops, and LINQ queries: --><p>由于系统中异步代码的绝对规模，我们在语言中增加了许多C#仍然不支持的模式，这包括迭代器（iterator），for循环和LINQ查询：</p><pre><code>IAsyncEnumerable&lt;Movie&gt; GetMovies(string url) {    foreach (await var movie in http.Get(url)) {        yield return movie;    }}</code></pre><!-- Or, in LINQ style: --><p>或使用LINQ风格：</p><pre><code>IAsyncEnumerable&lt;Movie&gt; GetMovies(string url) {    return        from await movie in http.Get(url)        ... filters ...        select ... movie ...;}</code></pre><!-- The entire LINQ infrastructure participated in streaming, including resource management and backpressure. --><p>整个LINQ基础架构，包括资源管理和<a href="https://github.com/ReactiveX/RxJava/wiki/Backpressure" target="_blank" rel="noopener">背压（backpressure）</a>，也参与到流式计算中。</p><!-- We converted millions of lines of code from the old callback style to the new async/await one.  We found plenty of bugsalong the way, due to the complex control flow of the explicit callback model.  Especially in loops and error handlinglogic, which could now use the familiar programming language constructs, rather than clumsy API versions of them. --><p>我们将数百万行的代码从旧的回调方式转移到新的async/await模式中。 在显式回调模型的复杂控制流中，我们发现了大量的bug，特别对于循环和错误处理逻辑而言，现在可使用熟悉的编程语言结构，而不是笨拙的API方式加以实现。</p><!-- I mentioned this was controversial.  Most of the team loved the usability improvements.  But it wasn't unanimous. --><p>我已经提到过，对于这点是有争议的：虽然团队中的大多数都乐见其在可用性上的改进，但也并非所有人都持此观点。</p><!-- Maybe the biggest problem was that it encouraged a pull-style of concurrency.  Pull is where a caller awaits a calleebefore proceeding with its own operations.  In this new model, you need to go out of your way to *not* do that.  It'salways possible, of course, thanks to the `async` keyword, but there's certainly a little more friction than the oldmodel. The old, familiar, blocking model of waiting for things is just an `await` keyword away. --><p>也许最大问题是旧的回调模型采用了一种pull风格的并发，在这种方式中，调用者在继续自己的操作之前需等待被调用者。 而在这个新模型中，你<em>不</em>能再这样做，当然，可能总是这归功于<code>async</code>关键字，但这肯定比旧模型带来更多的摩擦。旧式熟悉的阻塞模型变成了一个简单的<code>await</code>关键字。</p><!-- We offered bridges between pull and push, in the form of [reactive](https://rx.codeplex.com/)`IObservable<T>`/`IObserver<T>` adapters.  I wouldn't claim they were very successful, however for side-effectfulactions that didn't employ dataflow, they were useful.  In fact, our entire UI framework was based on the concept of[functional reactive programming](https://en.wikipedia.org/wiki/Functional_reactive_programming), which required aslight divergence from the Reactive Framework in the name of performance.  But alas, this is a post on its own. --><p>我们以<a href="https://rx.codeplex.com/" target="_blank" rel="noopener">反应式</a>的<code>IObservable&lt;T&gt;</code>/<code>IObserver&lt;T&gt;</code>适配器形式提供了pull和push之间的桥接。 虽然不敢宣称它非常成功，但是对于未使用数据流的带有副作用的行为，它们是很有用的。 实际上，我们整个UI框架都基于<a href="https://en.wikipedia.org/wiki/Functional_reactive_programming" target="_blank" rel="noopener">函数式反应式编程</a>的概念。以性能的名义，它与Reactive Framework略有不同， 但对于此问题，需要一篇独立的文章进行描述，本文将不再展开。</p><!-- An interesting consequence was a new difference between a method that awaits before returning a `T`, and one thatreturns an `Async<T>` directly.  This difference didn't exist in the type system previously.  This, quite frankly,annoyed the hell out of me and still does.  For example: --><p>一个有趣的后果是在返回<code>T</code>之前的await和直接返回<code>Async&lt;T&gt;</code>之间产生了差异，而在先前在类型系统中并不存在。 坦率地说，对此我感到非常烦恼直到现在。 比如说：</p><pre><code>async int Bar()  { return await Foo(); }Async&lt;int&gt; Bar() { return async Foo(); }</code></pre><!-- We would like to claim the performance between these two is identical.  But alas, it isn't.  The former blocks and keepsa stack frame alive, whereas the latter does not.  Some compiler cleverness can help address common patterns -- this isreally the moral equivalent to an asynchronous tail call -- however it's not always so cut and dry. --><p>我们想表达这两者的表现形式是等价的，但是事实并非如此。 前者阻塞并保持堆栈帧活跃，而后者却不能。 一些编译器可以巧妙地解决这种常见的模式——这实际上是异步尾调用的精神上的等价物——但事情并不总是这么简单。</p><!-- On its own, this wasn't a killer.  It caused some anti-patterns in important areas like streams, however.  Developerswere prone to awaiting in areas they used to just pass around `Async<T>`s, leading to an accumulation of paused stackframes that really didn't need to be there.  We had good solutions to most patterns, but up to the end of the projectwe struggled with this, especially in the networking stack that was chasing 10GB NIC saturation at wire speed.  We'lldiscuss some of the techniques we employed below. --><p>就其本身而言，这个问题不是非常严重。 然而，它在流式处理等重要领域引起了一些反模式情况。 开发人员倾向于在他们过去传递<code>Async&lt;T&gt;</code>的区域进行await，从而导致不必存在的暂停堆栈帧的大量积累。 我们对大多数的模式都有很好的解决方案，但直到项目结束时我们都在努力解决这个问题，尤其是在追求10Gb网卡传输速度饱和度的网络堆栈中。 我们将在下面讨论所采用的一些技术。</p><!-- But at the end of this journey, this change was well worth it, both in the simplicity and usability of the model, andalso in some of the optimization doors it opened up for us. --><p>但是在这次探索之旅结束时，这样的变化是非常值得的，不仅是在模型的简单性和可用性方面有提升，而且是在为我们后续优化打开了大门。</p><!-- ## The Execution Model --><h2 id="执行模型"><a href="#执行模型" class="headerlink" title="执行模型"></a>执行模型</h2><!-- That brings me to the execution model.  We went through maybe five different models, but landed in a nice place. --><p>这种变化让我首先经历的是执行模型。 我们经历了五种不同的模型，都取得不错的结果。</p><!-- A key to achieving asynchronous everything was ultra-lightweight processes.  This was possible thanks to [softwareisolated processes (SIPs)](http://research.microsoft.com/apps/pubs/default.aspx?id=71996), building upon [the foundationof safety described in an earlier post](http://joeduffyblog.com/2015/11/03/a-tale-of-three-safeties/). --><p>实现“异步皆一切”的关键是超轻量级的进程。这归功于在<a href="/2018/10/24/midori/1-a-tale-of-three-safeties/">一篇早期帖子中描述的安全基础</a>所建立的<a href="http://research.microsoft.com/apps/pubs/default.aspx?id=71996" target="_blank" rel="noopener">软件隔离过程（SIP）</a>机制之上。</p><!-- The absence of shared, mutable static state helped us keep processes small.  It's surprising how much address space isburned in a typical program with tables and mutable static variables.  And how much startup time can be spentinitializing said state.  As I mentioned earlier, we froze most statics as constants that got shared across manyprocesses.  The execution model also resulted in cheaper stacks (more on that below) which was also a key factor.  Thefinal thing here that helped wasn't even technical, but cultural.  We measured process start time and process footprintnightly in our lab and had a "ratcheting" process where every sprint we ensured we got better than last sprint.  A groupof us got in a room every week to look at the numbers and answer the question of why they went up, down, or stayed thesame. We had this culture for performance generally, but in this case, it kept the base of the system light and nimble. --><p>无共享可变静态状态使得将进程保持尽量小。 令人惊讶的是，在具有table和可变静态变量的典型程序中，有大量的地址空间被破坏，同时会花费大量的启动时间来初始化这些共享区域。 正如我之前提到的，我们将大多数的静态函数固化为在许多进程中共享的常量。执行模型使得堆栈的开销变得更小（更多细节将在下文中提到），这也是一个关键因素。 但是最后的贡献因素甚至却都不是技术，而是文化。 我们在实验环境中每天测量进程的启动时间和资源的使用量，并在优化的过程，确保我们每次冲刺都比上一次有所提高。 我们中的一群人每周会进入一个房间，对着各种数字并回答关于性能为什么上升/下降或保持不变的问题。 对于这种关于性能的文化我们是普遍存在的，并通过这种方式，保持了系统基础的轻量和灵活。</p><!-- Code running inside processes could not block.  Inside the kernel, blocking was permitted in select areas, but rememberno user code ever ran in the kernel, so this was an implementation detail.  And when I say "no blocking," I really meanit: Midori did not have [demand paging](https://en.wikipedia.org/wiki/Demand_paging) which, in a classical system, meansthat touching a piece of memory may physically block to perform IO.  I have to say, the lack of page thrashing was sucha welcome that, to this day, the first thing I do on a new Windows sytem is disable paging.  I would much rather havethe OS kill programs when it is close to the limit, and continue running reliably, than to deal with a paging madness. --><p>在进程内部运行的代码无法被阻塞， 而在内核中，代码允许在指定区域中被阻塞，但是请记住在内核中没有运行任何用户态的代码，因此（内核代码在指定区域中被阻塞）仅是一个实现的细节。这里当我说“没有阻塞”时，我是认真的：Midori没有<a href="https://en.wikipedia.org/wiki/Demand_paging" target="_blank" rel="noopener">按需换页</a>的机制。在传统的操作系统中，按需换页意味着对内存的访问可能会物理阻塞以执行I/O操作。我不得不说的是，没有按需换页所带来的页面抖动是如此的受欢迎，以至于直到了今天，我在新的Windows系统上所做的第一件事就是禁用分页。与其愚蠢地进行换页操作，我更希望操作系统在内存不足时杀死进程，并继续可靠地运行。</p><!-- C#'s implementation of async/await is entirely a front-end compiler trick.  If you've ever run ildasm on the resultingassembly, you know: it lifts captured variables into fields on an object, rewrites the method's body into a statemachine, and uses Task's continuation passing machinery to keep the iterator-like object advancing through the states. --><p>C#的async/await实现完全是一个前端的编译技巧。 如果你曾在生成的汇编上运行过ildasm，那么你就知道：它将捕获的变量提升到对象的字段中，将方法的主体重写为状态机形式，并使用Task的继续传递机制来保持像迭代器这类的对象向前通过状态。</p><!-- We began this way and shared some of our key optimizations with the C# and .NET teams.  Unfortunately, the result atthe scale of what we had in Midori simply didn't work. --><p>我们以这种方式开始并与C#和.NET团队分享了一些关键的优化方法。 但不幸的是，在Midori的规模上，这种方法根本无法工作。</p><!-- First, remember, Midori was an entire OS written to use garbage collected memory.  We learned some key lessons that werenecessary for this to perform adequately.  But I'd say the prime directive was to avoid superfluous allocations like theplague.  Even short-lived ones.  There is a mantra that permeated .NET in the early days: Gen0 collections are free.Unfortunately, this shaped a lot of .NET's library code, and is utter hogwash.  Gen0 collections introduce pauses, dirtythe cache, and introduce [beat frequency issues](https://en.wikipedia.org/wiki/Beat_(acoustics)) in a highly concurrentsystem.  I will point out, however, one of the tricks to garbage collection working at the scale of Midori was preciselythe fine-grained process model, where each process had a distinct heap that was independently collectible.  I'll have anentire article devoted to how we got good behavior out of our garbage collector, but this was the most importantarchitectural characteristic. --><p>首先，请记住，Midori是一个为使用内存垃圾回收而编写的完整操作系统，对于充分发挥这一功能，我们收获一些必要的关键教训。 但我要说的是，主要方向是即使是短寿命的，也需避免像瘟疫一样多余的内存分配。 在早期有一个贯穿.NET的口头禅：Gen0代回收是免费的。 不幸的是，这种理念已经塑造了很多.NET库代码，并且是彻头彻尾的贯穿。Gen0的回收导致暂停和脏缓存，并在高度并发的系统中导致<a href="http://t.cn/EV8oCxL" target="_blank" rel="noopener">拍频问题</a>。 然而，我要指出的是，在Midori规模上进行垃圾收集工作的一个技巧恰恰是细粒度的进程模型，因为其中每个进程都有一个独立的堆，可以独立地进行回收。 我将有一篇专门介绍我们如何通过垃圾回收器获得良好表现的文章，但这是最重要的架构上的特征。</p><!-- The first key optimization, therefore, is that an async method that doesn't await shouldn't allocate anything. --><p>因此，首要的关键优化是：async方法不应该分配任何内存。</p><!-- We were able to share this experience with .NET in time for C#'s await to ship.  Sadly, by then, .NET's Task hadalready been made a class.  Since .NET requires async method return types to be Tasks, they cannot be zero-allocationunless you go out of your way to use clumsy patterns like caching singleton Task objects. --><p>我们及时与.NET分享了这一经验，以便于C#中await的发布。 可惜的是，在那时，.NET的Task已经成为了一个类。 由于.NET要求异步方法的返回类型为Task，使得它们无法零分配，除非你不遗余力地使用类似缓存单例Task对象之类的笨拙模式。</p><!-- The second key optimization was to ensure that async methods that awaited allocated as little as possible. --><p>第二个关键优化是：确保await所分配的async方法尽可能少。</p><!-- In Midori, it was very common for one async method to invoke another, which invoked another ... and so on.  If you thinkabout what happens in the state machine model, a leaf method that blocks triggers a cascade of O(K) allocations, where Kis the depth of the stack at the time of the await.  This is really unfortunate. --><p>在Midori中，一个async方法被另一个async方法所调用是非常常见的，同时它也可能调用了另一个async方法……依此类推下去。 如果考虑状态机模型中所发生的情况，则阻塞的最末端方法会触发级联分配的复杂度为O(K)，其中K是await发生时堆栈的深度，而这将是非常糟糕的情况。</p><!-- What we ended up with was a model that only allocated when the await happened, and that allocated only once for anentire such chain of calls.  We called this chain an "activity."  The top-most `async` demarcated the boundary of anactivity.  As a result, `async` could cost something, but `await` was free. --><p>我们最终采用的是一个只在await发生时分配的模型，并且只为整个这样的调用链分配一次，我们将此链称为“activity”。最顶层的<code>async</code>划分了activity的边界。 因此，<code>async</code>可能会产生一些开销，但<code>await</code>却是零开销的。</p><!-- Well, that required one additional step.  And this one was the biggie. --><p>因此，这需要一个额外的步骤，而这一点却是最重要的。</p><!-- The final key optimization was to ensure that async methods imposed as little penalty as possible.  This meanteliminating a few sub-optimal aspects of the state machine rewrite model.  Actually, we abandoned it: --><p>最终的关键优化是确保<code>async</code>方法尽可能地减小开销， 这意味着它消除了状态机重写模型的一些不太理想的地方。 但是在实际中，我们最终弃用了它，其原因在于：</p><!-- 1. It completely destroyed code quality.  It impeded simple optimizations like inlining, because few inliners consider   a switch statement with multiple state variables, plus a heap-allocated display frame, with lots of local variable   copying, to be a "simple method."  We were competing with OS's written in native code, so this matters a lot. --><ol><li>它完全破坏了代码质量，阻碍了像内联这样的简单优化。因为很少有内联使用者认为，带有多个状态变量的switch语句，并加上堆分配的显示框架（display frame），再包含大量局部变量的复制的过程，是一个“简单的方法”。由于我们与使用原生代码编写的OS进行竞争，因此这一点很重要。</li></ol><!-- 1. It required changes to the calling convention.  Namely, returns had to be `Async*<T>` objects, much like .NET's   `Task<T>`.  This was a non-starter.  Even though ours were structs -- eliminating the allocation aspect -- they were   multi-words, and required that code fetch out the values with state and type testing.  If my async method returns   an int, I want the generated machine code to be a method that returns an int, goddamnit. --><ol start="2"><li>它需对调用约定进行修改。也就是说，返回的必须是<code>Async*&lt;T&gt;</code>对象，就像.NET的<code>Task&lt;T&gt;</code>一样。 这并非易事，即使我们返回的是结构并消除了分配的问题，而它们返回的是多字且要求代码通过状态和类型测试来获取值。也就是说，如果我的<code>async</code>方法返回的是int值，那么我也希望生成的机器码也TMD返回int值。</li></ol><!-- 1. Finally, it was common for too much heap state to get captured.  We wanted the total space consumed by an awaiting   activity to be as small as possible.  It was common for some processes to end up with hundreds or thousands of them,   in addition to some processes constantly switching between them.  For footprint and cache reasons, it was important   that they remain as small as the most carefully hand-crafted state machine as possible. --><ol start="3"><li>最后，一个普遍的问题是捕获的堆状态过多。而我们希望等待中的activity所消耗的总空间尽可能小，除了在它们之间不断切换的一些进程之外，一些进程通常最终会包含数百或数千个轻量级进程。 出于占用空间和缓存的原因，它们作为人工精心编写的状态机应保持越小越好，而这一点非常重要。</li></ol><!-- The model we built was one where asynchronous activities ran on [linked stacks](https://gcc.gnu.org/wiki/SplitStacks).These links could start as small as 128 bytes and grow as needed.  After much experimentation, we landed on a modelwhere link sizes doubled each time; so, the first link would be 128b, then 256b, ..., on up to 8k chunks.  Implementingthis required deep compiler support.  As did getting it to perform well.  The compiler knew to hoist link checks,especially out of loops, and probe for larger amounts when it could predict the size of stack frames (accounting forinlining).  There is a common problem with linking where you can end up relinking frequently, especially at the edge ofa function call within a loop, however most of the above optimizations prevented this from showing up.  And, even ifthey did, our linking code was hand-crafted assembly -- IIRC, it was three instructions to link -- and we kept alookaside of hot link segments we could reuse. --><p>我们构建的模型是异步activity在<a href="https://gcc.gnu.org/wiki/SplitStacks" target="_blank" rel="noopener">链接堆栈（linked stack）</a>上运行。 这些连接从小到128字节开始，并可根据需要进行增长。 经过多次实验，我们采用了一个每次链接大小加倍的模型：也就是说，首个链接大小是128字节，然后是256字节…直到最大为8K大小的块。 对此功能的实现需要编译器的深度支持，它也确实表现良好。 编译器知道何时进行链接检查，特别是在循环之外，并且（考虑到内联）当它可以预测堆栈帧大小时会探测更大数量的链接。 但是链接存在一个常见问题，即最终可能会进行频繁地重新链接，尤其是在循环内函数调用的边缘处。但是上述的大多数优化都阻止了这种情况的出现，而且，即使这种情况发生了，我们的链接代码也是由人工编写的汇编代码——IIRC，用于链接的三个指令，另外我们也一直关注于可以重用的活跃链接段。</p><!-- There was another key innovation.  Remember, I hinted earlier, we knew statically in the type system whether a functionwas asynchronous or not, simply by the presence of the `async` keyword?  That gave us the ability in the compiler toexecute all non-asynchronous code on classical stacks.  The net result was that all synchronous code remainedprobe-free!  Another consequence is the OS kernel could schedule all synchronous code on a set of pooled stacks.  Thesewere always warm, and resembled a classical thread pool, more than an OS scheduler.  Because they never blocked, youdidn't have O(T) stacks, where T is the number of threads active in the entire system.  Instead, you ended up with O(P),where P is the number of processors on the machine.  Remember, eliminating demand paging was also key to achieiving thisoutcome.  So it was really a bunch of "big bets" that added up to something that was seriously game-changing. --><p>除此之外，我们还获得了其他的重要创新。 还记得我之前曾暗示过，仅通过<code>async</code>关键字可以判断出，静态地知道函数在类型系统是否是异步的。这使我们能够在编译器中执行经典堆栈上的所有非异步代码，使得最终结果是所有同步代码都保持无探针状态！ 而另一项创新是OS内核可以在一组堆栈池中调度所有同步代码， 这组堆栈池总是处于活跃状态，这一点类似于经典的线程池，但不局限于OS调度程序。因为线程从未被阻塞，所以不会导致堆栈成为O(T)，其中T是整个系统中活跃的线程数，相反，你最终得到的堆栈大小是O(P)，其中P是机器上的处理器数量。 回想一下，消除按需分页也是达到这一目标的关键，所以这些真是一堆“大赌注”，所有这一切加起来足以形成革命性的成果。</p><!-- ## Message Passing --><h2 id="消息传递"><a href="#消息传递" class="headerlink" title="消息传递"></a>消息传递</h2><!-- A fundamental part of the system has been missing from the conversation: message passing. --><p>到目前为止，系统中尚未提及的基础部分是：消息传递。</p><!-- Not only were processes ultra-lightweight, they were single-threaded in nature.  Each one ran an [event loop](https://en.wikipedia.org/wiki/Event-driven_programming) and that event loop couldn't be blocked, thanks to thenon-blocking nature of the system.  Its job was to execute a piece of non-blocking work until it finished or awaited,and then to fetch the next piece of work, and so on.  An await that was previously waiting and became satisfied wassimply scheduled as another turn of the crank. --><p>进程不仅是超轻量级的，而且在本质上是单线程的。 由于系统的非阻塞特性，每个进程都运行了一个无法被阻塞的<a href="https://en.wikipedia.org/wiki/Event-driven_programming" target="_blank" rel="noopener">事件循环（event loop）</a>。 它的任务是执行一段非阻塞的程序，直到程序结束或通过await挂起等待，然后再获取下一个任务，依此类推。之前处于等待状态的await如果事件被满足，这将被调度为新的转动曲柄。</p><!-- Each such turn of the crank was called, fittingly, a "turn." --><p>曲柄的每次这样的转动被恰如其分地被称为“旋转”。</p><!-- This meant that turns could happen between asynchronous activities and at await points, nowhere else.  As a result,concurrent interleaving only occurred at well-defined points.  This was a giant boon to reasoning about state in theface of concurrency, however it comes with some gotchas, as we explore later. --><p>这意味着在旋转只可能发生在异步activity和await等待点之间，使得并发的交织只可能发生在定义明确的点上。 这对于面向并发的状态推理来讲是一个巨大的福音，但它也带来了一些陷阱，对此我们将稍后进行探讨。</p><!-- The nicest part of this, however, was that processes suffered no shared memory race conditions. --><p>然而，这种方式最大的优势是进程间不存在共享内存的竞争条件。</p><!-- We did have a task and data parallel framework.  It leveraged the concurrency safety features of the languge I'vementioned previously -- [immutability, isolation, and readonly annotations](http://research.microsoft.com/apps/pubs/default.aspx?id=170528) -- to ensure that this data race freedom wasnot violated.  This was used for fine-grained computations that could use the extra compute power.  Most of the system,however, gained its parallel execution through the decomposition into processes connected by message passing. --><p>我们确实开发了一个任务和数据并行框架，其利用了我之前提到的语言的并发安全功能——<a href="http://research.microsoft.com/apps/pubs/default.aspx?id=170528" target="_blank" rel="noopener">不变性，隔离和只读注解</a>——来确保不违反这种数据竞争的自由度，这被用来进行使用额外计算能力的细粒度计算。然而，大多数系统通过分解成通过消息传递连接的进程来进行并行执行。</p><!-- Each process could export an asynchronous interface.  It looked something like this: --><p>每个进程都可导出异步的接口，它看起来类似如下：<!--    async interface ICalculator {        async int Add(int x, int y);        async int Multiply(int x, int y);        // Etc...    }--></p><pre><code>async interface ICalculator {    async int Add(int x, int y);    async int Multiply(int x, int y);    // 等等...}</code></pre><!-- As with most asynchronous RPC systems, from this interface was generated a server stub and client-side proxy.  On theserver, we would implement the interface: --><p>与大多数异步RPC系统一样，从该接口可以生成服务器stub和客户端代理的代码。 于服务器端，我们需实现如下接口：<!--     class MyCalculator : ICalculator {        async int Add(int x, int y) { return x + y; }        async int Multiply(int x, int y) { return x * y; }        // Etc...    } --></p><pre><code>class MyCalculator : ICalculator {    async int Add(int x, int y) { return x + y; }    async int Multiply(int x, int y) { return x * y; }    // 等等...}</code></pre><!-- Each server-side object could also request [capabilities](https://en.wikipedia.org/wiki/Capability-based_security)simply by exposing a constructor, much like the program's main entrypoint could, as I described in [the prior post](http://joeduffyblog.com/2015/11/10/objects-as-secure-capabilities/).  Our application model took care of activating andwiring up the server's programs and services. --><p>正如我在<a href="/2018/11/18/midori/2-objects-as-secure-capabilities/">前一篇文章</a>中所描述的那样，每个服务器端对象也可以，像程序的主入口点那样，仅地通过暴露构造函数的方式来请求获取<a href="https://en.wikipedia.org/wiki/Capability-based_security" target="_blank" rel="noopener">权能</a>。而我们的应用程序模型则会处理好激活和连接服务端程序和服务。</p><!-- A server could also return references to other objects, either in its own process, or a distant one.  The systemmanaged the object lifetime state in coordination with the garbage collector.  So, for example, a tree: --><p>服务器还可以在其自己的进程或远程进程中返回其他对象的引用， 而系统与垃圾回收器一起协调管理对象的生存周期状态。 因此，例如有一个如下的MyTree对象：</p><pre><code>class MyTree : ITree {    async ITree Left() { ... }    async ITree Right() { ... }}</code></pre><!-- As you might guess, the client-side would then get its hands on a proxy object, connected to this server objectrunning in a process.  It's possible the server would be in the same process as the client, however typically theobject was distant, because this is how processes communicated with one another: --><p>你可能已经猜到，客户端将获得代理对象的引用，该对象连接到在进程中运行的此服务器对象。 服务器可能与客户端处于相同的进程中，但通常情况下对象可能物理位置相距很远，这是由进程间通信方式所决定的：</p><pre><code>class MyProgram {    async void Main(IConsole console, ICalculator calc) {        var result = await calc.Add(2, 2);        await console.WriteLine(result);    }}</code></pre><!-- Imagining for a moment that the calculator was a system service, this program would communicate with that systemservice to add two numbers, and then print the result to the console (which itself also could be a different service). --><p>想象一下如果Calculator是系统服务，则该程序将与该系统服务通信来求两个数字之和，而后将结果打印到控制台（该过程本身也可能是另一个不同的服务）。</p><!-- A few key aspects of the system made message passing very efficient.  First, all of the data structures necessary totalk cross-process were in user-mode, so no kernel-mode transitions were needed.  In fact, they were mostly lock-free.Second, the system used a technique called "[pipelining](https://en.wikipedia.org/wiki/Futures_and_promises#Promise_pipelining)" to remove round-trips and synchronizationping-ponging.  Batches of messages could be stuffed into channels before they filled up.  They were delivered in chunksat-a-time.  Finally, a novel technique called "three-party handoff" was used to shorten the communication paths betweenparties engaging in a message passing dialogue.  This cut out middle-men whose jobs in a normal system would have beento simply bucket brigade the messages, adding no value, other than latency and wasted work. --><p>系统的一些关键特征使得消息传递非常高效。 首先，跨进程通信所需的所有数据结构都处于用户空间态，因此不需要到内核模式进行转换，事实上，他们大多是无锁的。 其次，该系统使用一种称为<a href="https://en.wikipedia.org/wiki/Futures_and_promises#Promise_pipelining" target="_blank" rel="noopener">“流水线”</a>的技术来消除消息往返和同步回传。 在通道填充满之前，可以将批量消息填充到通道中，使得消息每次一批一批地传递。 最后，一种称为“三方切换”的新技术被用来缩短参与消息传递对话的各方之间的通信路径，该方法省去了在通常系统中仅负责简单地中转消息得中间人，因为它除了增加延迟和开销之外不具有任何价值。</p><img src="/2018/11/25/midori/3-asynchronous-everything/2015-11-19-asynchronous-everything.pipeline.jpg" title="消息传递示意图"><!-- The only types marshalable across message passing boundaries were: --><p>跨消息传递边界的可编组类型（可消息传递的数据类型）包括：</p><!-- * Primitive types (`int`, `string`, etc).* Custom PODs that didn't contain pointers (explicitly labeled marshalable).* References to streams (see below).* References to other async objects (e.g., our `ICalculator` above).* A special `SharedData` object, which requires a bit more explanation. --><ul><li>基本类型（<code>int</code>，<code>string</code>等）。</li><li>不包含指针（明确标记为可编组）的自定义POD。</li><li>指向流的引用（见下文）。</li><li>指向其他async对象的引用（例如，上文中的<code>ICalculator</code>）。</li><li>特殊的<code>SharedData</code>对象，对此需要更多的解释。</li></ul><!-- Most of these are obvious.  The `SharedData` thing is a little subtle, however.  Midori had a fundamental philosophy of"zero-copy" woven throughout its fabric.  This will be the topic of a future post.  It's the secret sauce that let usout-perform many classical systems on some key benchmarks.  The idea is, however, no byte should be copied if it canbe avoided.  So we don't want to marshal a `byte[]` by copy when sending a message between processes, for example.  The`SharedData` was a automatic ref-counted pointer to some immutable data in a heap shared between processes.  The OSkernel managed this heap memory and reclaimed it when all references dropped to zero.  Because the ref-counts wereautomatic, programs couldn't get it wrong.  This leveraged some new features in our language, like destructors. --><p>上述的大部分内容是显而易见的，然而，<code>SharedData</code>则有点微妙。Midori的整个构造过程都采用了“零拷贝”的基本理念，这是未来其中一篇文章的主题，也是让我们在一些关键的基准测试中超越许多经典系统的秘诀。 但其中主要的思路是，如果可以避免则不应该复制任何数据。 因此，我们不希望在进程之间发送消息时以复制的方式编组<code>byte[]</code>。 <code>SharedData</code>是一个自动重新计数的指针，指向进程之间共享堆中的不可变数据，操作系统内核管理该堆内存并在所有引用计数为零时对其进行回收。因为引用计数是自动完成的，所以保证了程序不会出错，这充分利用了我们语言中的一些新功能，如析构函数等。</p><!-- We also had the notion of "near objects," which went an extra step and let you marshal references to immutable datawithin the same process heap.  This let you marshal rich objects by-reference.  For example: --><p>同时，我们还有“近对象（near objects）”的概念，它采取了额外的步骤，可在同一进程堆中对不可变数据的引用进行编组， 使得可以通过引用来编组富对象。 例如：</p><!--     // An asynchronous object in my heap:    ISpellChecker checker = ...;    // A complex immutable Document in my heap,    // perhaps using piece tables:    immutable Document doc = ...;    // Check the document by sending messages within    // my own process; no copies are necessary:    var results = await checker.Check(doc); --><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 堆中的异步对象：</span><br><span class="line">ISpellChecker checker = ...;</span><br><span class="line"></span><br><span class="line">// 堆中的复杂的不可变的Document对象，可能使用了其他的table：</span><br><span class="line">immutable Document doc = ...;</span><br><span class="line"></span><br><span class="line">// 通过在自身进程中发送消息来对Document进行check，而无需复制：</span><br><span class="line">var results = await checker.Check(doc);</span><br></pre></td></tr></table></figure><!-- As you can guess, all of this was built upon a more fundamental notion of a "channel."  This is similar to what you'llsee in [Occam](https://en.wikipedia.org/wiki/Occam_(programming_language)), [Go](https://en.wikipedia.org/wiki/Go_(programming_language)) and related [CSP](https://en.wikipedia.org/wiki/Communicating_sequential_processes) languages.  I personally found the structure andassociated checking around how messages float around the system more comfortable than coding straight to the channelsthemselves, but your mileage may vary.  The result felt similar to programming with [actors](https://en.wikipedia.org/wiki/Actor_model), with some key differences around the relationship between process andobject identity. --><p>你可以猜到，所有这些都建立在一个更为基本的“通道（channel）”概念之上，这与你在<a href="http://t.cn/EV8oHB7" target="_blank" rel="noopener">Occam</a>，<a href="http://t.cn/RGeyJl7" target="_blank" rel="noopener">Go</a>和相关的CSP语言中看到的相类似。 我个人发现消息是如何在系统周围浮动的结构和相关检查比直接编码到通道本身更舒适，但你的自身体验可能会有所不同。其结果与使用<a href="https://en.wikipedia.org/wiki/Actor_model" target="_blank" rel="noopener">Actor</a>的编程相类似，只是在进程和对象身份之间的关系方面存在一些关键差异。</p><!-- ## Streams --><h2 id="流"><a href="#流" class="headerlink" title="流"></a>流</h2><!-- Our framework had two fundamental stream types: `Stream` held a stream of bytes and `Sequence<T>` held `T`s.  Theywere both forward-only (we had separate seekable classes) and 100% asynchronous. --><p>我们的框架有两种基本的流类型：<code>Stream</code>持有一个字节流，而<code>Sequence &lt;T&gt;</code>持有数据<code>T</code>。 它们都是前向的（我们有其他的可seek类）和100%纯异步的。</p><!-- Why two types, you wonder?  They began as entirely independent things, and eventually converged to be brother andsister, sharing a lot of policy and implementation with one another.  The core reason they remained distinct, however,is that it turns out when you know you're just schlepping raw byte-streams around, you can make a lot of interestingperformance improvements in the implementation, compared to a fully generic version. --><p>你可能会问，为什么有两种类型？它们从完全独立的类开始，最终收敛到一起，彼此共享了许多策略及实现。然而，它们保持完全不同的根本原因是，如果我们知道只是简单地处理原始字节流，那么与完全泛型的版本相比，可以在实现中做出很多有趣的性能改进。</p><!-- For purposes of this discussion, however, just imagine that `Stream` and `Sequence<byte>` are isomorphic. --><p>但是，为了讨论方便，这里你需认为<code>Stream</code>和<code>Sequence&lt;byte&gt;</code>是相互同构的。</p><!-- As hinted at earlier, we also had `IAsyncEnumerable<T>` and `IAsyncEnumerator<T>` types.  These were the most generalpurpose interfaces you'd code against when wanting to consume something.  Developers could, of course, implement theirown stream types, especially since we had asynchronous iterators in the language.  A full set of asynchronous LINQoperators worked over these interfaces, so LINQ worked nicely for consuming and composing streams and sequences. --><p>如前所述，我们还有<code>IAsyncEnumerable&lt;T&gt;</code>和<code>IAsyncEnumerator&lt;T&gt;</code>类型。这些是代码中想要对流进行消费时，需要进行编码的最通用接口。当然，开发人员可以实现自己的流类型，特别是由于我们在编程语言中使用了异步的迭代器。 一整套的异步LINQ运算符可以在这些接口上工作，因此LINQ可以很好地用于消费和组合流和序列。</p><!-- In addition to the enumerable-based consumption techniques, all the standard peeking and batch-based APIs wereavailable.  It's important to point out, however, that the entire streams framework built atop the zero-copycapabilities of the kernel, to avoid copying.  Every time I see an API in .NET that deals with streams in terms of`byte[]`s makes me shed a tear.  The result is our streams were actually used in very fundamental areas of the system,like the network stack itself, the filesystem the web servers, and more. --><p>除了基于可枚举的流消费技术之外，还提供了所有标准<em>瞥见</em>和批处理的API。 然而，需要重点指出的是，整个流框架是构建在内核的零拷贝基础之上，以避免不必要的复制。 每当我在.NET中看到一个用<code>byte[]</code>来流处理的API时，我就会流下眼泪。 最终的结果是我们的流实际用于系统非常基础的区域，如网络堆栈本身，文件系统，Web服务器等。</p><!-- As hinted at earlier, we supported both push and pull-style concurrency in the streaming APIs.  For example, wesupported generators, which could either style: --><p>如前所述，我们支持流API中的推（push）式和拉（pull）式并发，例如，我们支持采用以下方式的生成器：</p><!--     // Push:    var s = new Stream(g => {        var item = ... do some work ...;        g.Push(item);    });    // Pull:    var s = new Stream(g => {        var item = await ... do some work ...;        yield return item;    }); --><pre><code>// 推：var s = new Stream(g =&gt; {    var item = ... 做一些事情 ...;    g.Push(item);});// 拉：var s = new Stream(g =&gt; {    var item = await ... 做一些事情 ；...;    yield return item;}); </code></pre><!-- The streaming implementation handled gory details of batching and generally ensuring streaming was as efficient aspossible.  A key technique was [flow control](https://en.wikipedia.org/wiki/Transmission_Control_Protocol), borrowedfrom the world of TCP.  A stream producer and consumer collaborated, entirely under the hood of the abstraction, toensure that the pipeline didn't get too imbalanced.  This worked much like TCP flow control by maintaining a so-called"window" and opening and closing it as availability came and went.  Overall this worked great.  For example, ourrealtime multimedia stack had two asynchronous pipelines, one for processing audio and the other for processing video,and merged them together, to implement [A/V sync](https://en.wikipedia.org/wiki/Audio_to_video_synchronization).  Ingeneral, the built-in flow control mechanisms were able to keep them from dropping frames. --><p>流的实现方法照本宣科地搞定了批处理的细节，并大体上确保了流式传输尽可能的高效。 一个关键技术是借鉴了TCP中的<a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol" target="_blank" rel="noopener">流量控制</a>技术，流的生产者和消费者完全在抽象的框架下进行合作，以确保流水线不会过于的不平衡。在这一点上很像TCP的流量控制，通过维护所谓的“窗口”，随着可用性的来去不断打开和关闭控制阀门。 总的来说，这种方式很有效，例如，我们的实时多媒体栈中有两个异步的流水线，一个用于处理音频，另一个用于处理视频，并通过将它们合并在一起，以实现<a href="https://en.wikipedia.org/wiki/Audio_to_video_synchronization" target="_blank" rel="noopener">A/V同步</a>。 总的来说，内置的流量控制机制能够有效地防止丢帧的情况发生。</p><!-- ## "Grand" Challenges --><h2 id="“重大”的挑战"><a href="#“重大”的挑战" class="headerlink" title="“重大”的挑战"></a>“重大”的挑战</h2><!-- The above was a whirlwind tour.  I've glossed over some key details, but hopefully you get the picture. --><p>以上的内容像是一场匆忙的旅行， 我故意忽略了其中一些关键细节，但希望你能获得整体的轮廓。</p><!-- Along this journey we uncovered several "grand challenges."  I'll never forget them, as they formed the outline of myentire yearly performance review for a good 3 years straight.  I was determined to conquer them.  I can't say that ouranswers were perfect, but we made a gigantic dent in them. --><p>在这次旅程中，我们发现了几个“重大的挑战”。 我永远不会忘记它们，因为它们连续三年组成了我整个年度绩效评估的大体轮廓。 我决心征服他们，虽然不能说我们的成果是完美的，但我们确实在此做出了巨大的改进。</p><!-- ### Cancellation --><h3 id="Cancellation（取消）"><a href="#Cancellation（取消）" class="headerlink" title="Cancellation（取消）"></a>Cancellation（取消）</h3><!-- The need to have cancellable work isn't anything new.  I came up with the [`CancellationToken` abstraction in .NET](http://blogs.msdn.com/b/pfxteam/archive/2009/05/22/9635790.aspx), largely in response to some of the challenges we hadaround ambient authority with prior "implicitly scoped" attempts. --><p>对可取消任务的需求已经不是什么新鲜事， 对此我设计了<a href="http://blogs.msdn.com/b/pfxteam/archive/2009/05/22/9635790.aspx" target="_blank" rel="noopener">.NET中的CancellationToken抽象</a>，其主要目的是解决我们之前围绕带有“隐式范围（implicitly scoped）”的环境权限所面临一些挑战。</p><!-- The difference in Midori was the scale.  Asynchronous work was everywhere.  It sprawled out across processes and,sometimes, even machines.  It was incredibly difficult to chase down run-away work.  My simple use-case was how toimplement the browser's "cancel" button reliably.  Simply rendering a webpage involved a handful of the browser's ownprocesses, plus the various networking processes -- including the NIC's device driver -- along with the UI stack, andmore.  Having the ability to instantly and reliably cancel all of this work was not just appealing, it was required.--><p>Midori与.NET的不同之处在于其具有的规模，使得其异步的任务无处不在，不仅跨越多个进程，甚至是还是跨主机之间的。 追踪失控的任务是一件非常困难的是，我的一个简单例子是，如何可靠地实现浏览器的“取消”按钮。 由于简单地渲染网页便涉及数个浏览器自身的进程，以及各种网络进程——这包括NIC的设备驱动程序，以及UI堆栈等等。 能够立即可靠地对所有这些工作执行取消操作不仅具有吸引力，并且也是必需的。</p><!-- The solution ended up building atop the foundation of `CancellationToken`. --><p>其解决方案最终建立在<code>CancellationToken</code>的基础之上。</p><!-- They key innovation was first to rebuild the idea of `CancellationToken` on top of our overall message passing model,and then to weave it throughout in all the right places.  For example: --><p>其关键创新在于首次在我们的整体消息传递模型上重建了<code>CancellationToken</code>的思想，并在所有正确的位置使用它。 例如：</p><!-- * CancellationTokens could extend their reach across processes.* Whole async objects could be wrapped in a CancellationToken, and used to trigger [revocation](  http://c2.com/cgi/wiki?RevokableCapabilities).* Whole async functions could be invoked with a CancellationToken, such that cancelling propagated downward.* Areas like storage needed to manually check to ensure that state was kept consistent. --><ul><li>CancellationToken可以扩展它们的范围实现跨进程；</li><li>整个async对象可包含在CancellationToken中，并用于触发<a href="http://c2.com/cgi/wiki?RevokableCapabilities" target="_blank" rel="noopener">撤销操作</a>；</li><li>可使用CancellationToken调用整个async函数，以便取消其向下传播；</li><li>存储像这样的区域需进行手动检查以确保状态保持一致。</li></ul><!-- In summary, we took a "whole system" approach to the way cancellation was plumbed throughout the system, includingextending the reach of cancellation across processes.  I was happy with where we landedon this one. --><p>总之，我们采用了“整体系统”的方法来解决整个系统中的取消问题，包括扩展跨进程取消的范围。 我对我们采用的这种思路非常满意。</p><!-- ### State Management --><h3 id="状态管理"><a href="#状态管理" class="headerlink" title="状态管理"></a>状态管理</h3><!-- The ever-problematic "state management" problem can be illustrated with a simple example: --><p>可以通过一个简单的例子来说明可能会出问题的“状态管理”：</p><pre><code>async void M(State s) {    int x = s.x;    await ... something ...;    assert(x == s.x);}</code></pre><!-- The question here is, can the assertion fire? --><p>那么问题是，断言会触发吗？</p><!-- The answer is obviously yes.  Even without concurrency, reentrancy is a problem.  Depending on what I do in "...something ...", the `State` object pointed to by `s` might change before returning back to us. --><p>答案显然是肯定的。 即使没有并发，重入也会是一个问题。 根据我在“await … something …”语句中所做的事情，<code>s</code>所指向的<code>State</code>对象可能会在返回之前发生变化。</p><!-- But somewhat subtly, even if "... something ..." doesn't change the object, we may find that the assertion fires.Consider a caller: --><p>但有些微妙的是，即使“await … something …”没有改变对象，断言可能也会触发，考虑如下的一个调用：</p><pre><code>State s = ...;Async&lt;void&gt; a = async M(s);s.x++;await a;</code></pre><!-- The caller retains an alias to the same object.  If M's awaiting operation must wait, control is resumed to the caller.The caller here then increments `x` before awaiting M's completion.  Unfortunately, when M resumes, it will discoverthat the value of `x` no longer matches `s.x`. --><p>调用者保留同一对象的别名。 如果等待中操作M需要等待，则控制权将重新交还给调用者， 而后，调用者在等待M完成之前将x递增。 不幸的是，当M恢复时，它会发现x的值不再与s.x的值相匹配。</p><!-- This problem manifests in other more devious ways.  For example, imagine one of those server objects earlier: --><p>这个问题以其他更加隐蔽的方式表现出来。 例如，回想一下之前的那些服务对象：</p><!--     class StatefulActor : ISomething {        int state;        async void A() {            // Use state        }        async void B() {            // Use state        }    } --><pre><code>class StatefulActor : ISomething {    int state;    async void A() {        // 使用状态A    }    async void B() {        // 使用状态B    }} </code></pre><!-- Imagining that both A and B contain awaits, they can now interleave with one another, in addition to interleaving withmultiple activations of themselves.  If you're thinking this smells like a race condition, you're right.  In fact,saying that message passing systems don't have race conditions is an outright lie.  There have even been papersdiscussing this [in the context of Erlang](https://www.it.uu.se/research/group/hipe/dialyzer/publications/races.pdf).It's more correct to say our system didn't have *data race* conditions. --><p>设想一下，A和B都包含await操作，除了多次自我激活的交错之外，它们还可相互交错。 如果你认为它们看起来像是一对竞争条件，那你猜对了。 事实上，如果说消息传递系统没有竞争条件是一个彻头彻尾的谎言，甚至有一些论文在<a href="https://www.it.uu.se/research/group/hipe/dialyzer/publications/races.pdf" target="_blank" rel="noopener">Erlang的背景下</a>讨论该问题。 因此，说我们的系统没有数据竞争条件更为正确。</p><!-- Anyway, there be dragons here. --><p>无论如何，这里是有竞争冒险存在的。</p><!-- The solution is to steal a page from classical synchronization, and apply one of many techniques: --><p>其解决方案也从经典的同步方法中得到启发，并应用以下多种技术之一：</p><!-- * Isolation.* Standard synchronization techniques (prevent write-write or read-write hazards).* Transactions. --><ul><li>隔离</li><li>标准同步技术（防止写后写或读后写冒险）。</li><li>事务处理</li></ul><!-- By far, we preferred isolation.  It turns out web frameworks offer good lessons to learn from here.  Most of the time,a server object is part of a "session" and should not be aliased across multiple concurrent clients.  It tended to beeasy to partition state into sub-objects, and have dialogues using those.  Our language annotations around mutabilityhelped to guide this process. --><p>直至今日，我们更喜欢隔离。 事实证明，Web框架提供了很好的，可供学习的教训： 大多数情况下，服务器对象是“会话”的一部分，不应该跨多个并发客户端使用。 它往往很容易将状态划分为子对象，并使用它们进行会话。我们围绕可变性的语言注释有助于指导这一过程。</p><!-- A lesser regarded technique was to apply synchronization.  Thankfully in our language, we knew which operations readversus wrote, and so we could use that to block dispatching messages intelligently, using standard reader-writer locktechniques.  This was comfy and cozy and whatnot, but could lead to deadlocks if done incorrectly (which we did ourbest to detect).  As you can see, once you start down this path, the world is less elegant, so we discouraged it. --><p>三者中较少考虑的技术是同步。值得庆幸的是，在我们的语言中，我们知道哪些操作存在读与写的竞争，因此可以利用它使用标准读写器加锁技术智能地阻止调度消息。这看起来是个很不错的方法，但如果做得不正确可能导致死锁（对此我们将尽力检测）。 正如你所看到的，一旦你采用了这个方法，世界就不那么优雅了，所以我们不鼓励使用它。</p><!-- Finally, transactions.  We didn't go there.  [Distributed transactions are evil](http://c2.com/cgi/wiki?DistributedTransactionsAreEvil). --><p>最后，对于事务处理，我们根本没有对其进行实践过，因为<a href="http://c2.com/cgi/wiki?DistributedTransactionsAreEvil" target="_blank" rel="noopener">分布式事务是邪恶的</a>。</p><!-- In general, we tried to learn from the web, and apply architectures that worked for large-scale distributed systems.Statelessness was by far the easiest pattern.  Isolation was a close second.  Everything else was just a little dirty. --><p>总的来说，我们尝试从互联网上学习，并应用适用于大规模分布式系统的架构。 无状态的方法是迄今为止最简单的模式，而隔离紧随其后，其他一切方式都显得有点脏。</p><!-- P.S.  I will be sure to have an entire post dedicated to the language annotations. --><p>备注：未来我一定会写一篇专门讨论语言注释的文章。</p><!-- ### Ordering --><h3 id="有序性"><a href="#有序性" class="headerlink" title="有序性"></a>有序性</h3><!-- In a distributed system, things get unordered unless you go out of your way to preserve order.  And going out of yourway to preserve order removes concurrency from the system, adds book-keeping, and a ton of complexity.  My biggestlesson learned here was: distributed systems are unordered.  It sucks.  Don't fight it.  You'll regret trying. --><p>在分布式系统中，除非你不遗余力地保持有序状态，否则事物会是无序的。 并且不遗余力地保持有序状态会消弱系统的并发性，增加状态的记录开销和大量复杂性。 于此我学到的最大教训是：分布式系统是无序的。 这糟糕透了，不过不要去试图打败它，否则你会后悔尝试的。</p><!-- Leslie Lamport has a classic must-read paper on the topic: [Time, Clocks, and the Ordering of Events in a DistributedSystem](http://amturing.acm.org/p558-lamport.pdf). --><p>Leslie Lamport有一篇关于该主题的经典必读文章：<a href="http://amturing.acm.org/p558-lamport.pdf" target="_blank" rel="noopener">Time, Clocks, and the Ordering of Events in a Distributed System</a>。</p><!-- But unordered events surprise developers.  A good example is as follows: --><p>但无序的事件却让开发人员感到意外。 下面是一个很好的例子：</p><!--     // Three asynchronous objects:    IA a = ...;    IB b = ...;    IC c = ...;    // Tell b to talk to a:    var req1 = async b.TalkTo(a);    // Tell c to talk to b:    var req2 = async c.TalkTo(a);    await Async.Join(req1, req2); --><pre><code>// 三个异步对象：IA a = ...;IB b = ...;IC c = ...;// 让b与a交互：var req1 = async b.TalkTo(a);// 让c与a交互：var req2 = async c.TalkTo(a);await Async.Join(req1, req2); </code></pre><!-- If you expected that `b` is guaranteed to talk with `a` before `c` talks with `a`, you're in for a bad day. --><p>如果你认为<code>b</code>在<code>c</code>与<code>a</code>交互之前理应与<code>a</code>交互，那么你将陷入非常糟糕的一天。</p><!-- We offered facilities for controlling order.  For example, you could flush all the messages from a channel, and awaittheir delivery.  You could, of course, always await the individual operations, but this introduces some amount ofunnecessary latency due to round-tripping.  We also had a "flow" abstraction that let you guarantee a sequence ofasynchronous messages were delivered in order, but in the most efficient way possible. --><p>我们提供控制顺序的机制。 例如，你可以刷新通道中的所有消息，并等待消息回传。 当然，你随时可以await某个操作，但由于往返操作会引入一些不必要的延迟。我们还有一个关于“流”的抽象，采用最有效的方式，保证一系列异步消息按顺序传递。</p><!-- As with state management, we found that an abundance of ordering problems was often indicative of a design problem. --><p>与状态管理层一样，我们发现大量的有序性问题的存在往往表明设计上出了问题。</p><!-- ### Debugging --><h3 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h3><!-- With so much work flying around in the system, debugging was a challenge in the early days. --><p>在早期，由于在系统中存在大量的任务，调试也是一项挑战。</p><!-- The solution, as with many such challenges, was tooling.  We taught our tools that activities were as first class asthreads.  We introduced causality IDs that flowed with messages across processes, so if you broke into a messagedispatch in one process, you could trace back to the origin in potentially some other distant process.  The defaultbehavior for a crash was to gather this cross-process stack trace, to help figure out how you go to where you were. --><p>与许多此类挑战一样，解决方案是使用工具。 在我们的工具中，activity和线程一样作为一等公民而存在。 同时，我们引入了跨进程的消息传递的因果ID，因此如果从一个进程中进入消息调度，则可能会在其他远程进程中追溯到消息源。 进程崩溃的默认行为是收集此跨进程堆栈追踪（stack trace），以帮助开发者找到目标位置。</p><!-- Another enormous benefit of our improved execution model was that stacks were back!  Yes, you actually got stack tracesfor asynchronous activities awaiting multiple levels deep at no extra expense.  Many systems like .NET's have to go outof their way to piece together a stack trace from disparate hunks of stack-like objects.  We had that challenge acrossprocesses, but within a single process, all activities had normal stack traces with variables that were in a good state. --><p>我们改进后的执行模型的另一个巨大好处是堆栈又回来了，没错，你实际上有异步activity的堆栈跟踪等待多个级别，而无需额外开销。像.NET这样的许多系统都不得不竭尽全力将来自不同类似堆栈对象的堆栈跟踪拼凑在一起。虽然我们在跨进程中也遇到了该挑战，但在单个进程内部，所有activity都有正常的堆栈跟踪并且变量处于良好的状态。</p><!-- ### Resource Management --><h3 id="资源管理"><a href="#资源管理" class="headerlink" title="资源管理"></a>资源管理</h3><!-- At some point, I had a key realization.  Blocking in a classical system acts as a natural throttle on the amount of workthat can be offered up to the system.  Your average program doesn't express all of its latent concurrency andparallelism by default.  But ours did!  Although that sounds like a good thing -- and indeed it was -- it came with adark side.  How the heck do you manage resources and schedule all that work intelligently, in the face of so much of it? --><p>在某种程度上，我收获了一个重要认识，传统系统中阻塞方法可作为提供给系统所有任务的自然节流措施。默认情况下，你的普通程序并不能表达其所有潜在的并发性和并行性，但我们的系统的确做到了，虽然这听起来像是一件好事，事实上也确实如此，但它带来了暗黑的一面。在面对如此多的情况时，你到底如何管理资源并智能地调度所有任务？</p><!-- This was a loooooooong, winding road.  I won't claim we solved it.  I won't claim we even came close.  I will claim wetackled it enough that the problem was less disastrous to the stability of the system than it would have otherwise been. --><p>这是一条蜿蜒曲折的道路，我不会声称我们已经彻底解决， 甚至都不会声称靠近这个目标。 我只会说，在这个问题对于系统的稳定性来说比其他情况所造成的灾难性都要小。</p><!-- An analogous problem that I've faced in the past is with thread pools in both Windows and the .NET Framework.  Giventhat work items might block in the thread pool, how do you decide the number of threads to keep active at once?  Thereare always imperfect heuristics applied, and I would say we did no worse.  If anything, we erred on the side of usingmore of the latent parallelism to saturate the available resources.  It was pretty common to be running the Midorisystem at 100% CPU utilization, because it was doing useful stuff, which is pretty rare on PCs and traditional apps. --><p>我过去在Windows和.NET Framework的线程池上遇到了类似的问题，鉴于任务项可能会在线程池中阻塞，那么将如何一次性确定保持活跃状态的线程数？总有不完美的启发式方法可以使用，那么我可以说我们的方法并没有变得更糟。 如果真的有的话，那么错误地使用较多的潜在并行性，可用资源变得饱和可以算作一个。 以100%的CPU利用率运行Midori系统是很常见的，因为它正在运行有意义的任务，而这在PC和传统应用程序中非常罕见。</p><!-- But the scale of our problem was much worse than anything I'd ever seen.  Everything was asynchronous.  Imagine an apptraversing the entire filesystem, and performing a series of asynchronous operations for each file on disk.  In Midori,the app, filesystem, disk drivers, etc., are all different asynchronous processes.  It's easy to envision the resulting[fork bomb](https://en.wikipedia.org/wiki/Fork_bomb)-like problem that results. --><p>但我们问题的规模比我见过的任何系统都要复杂得多，因为所有的一切都是异步的。 设想一下，如果某个应用程序遍历整个文件系统，并为磁盘上的每个文件执行一系列异步操作。 在Midori中，应用程序，文件系统和磁盘驱动程序等都是不同的异步进程。 那么很容易想到会产生类似<a href="https://en.wikipedia.org/wiki/Fork_bomb" target="_blank" rel="noopener">fork炸弹</a>的问题。</p><!-- The solution here broke down into a two-pronged defense: --><p>因此，这里的解决方案可分解为双管齐下的防御措施：</p><!-- 1. Self-control: async code knows that it could flood the system with work, and explicitly tries not to.2. Automatic resource management: no matter what the user-written code does, the system can throttle automatically. --><ol><li>自我控制：异步代码明白它可能会使系统充满任务，并明确地尝试避免这样做；</li><li>自动资源管理：无论用户编写什么样的代码，系统都可以自动加以控制。</li></ol><!-- For obvious reasons, we preferred automatic resource management. --><p>出于显而易见的原因，我们倾向于自动资源管理</p><!-- This took the form of the OS scheduler making decisions about which processes to visit, which turns to let run, and, insome cases, techniques like flow control as we saw above with streams.  This is the area we had the most "open ended"and "unresolved" research.  We tried out many really cool ideas.  This included attempting to model the expectedresource usage of asynchronous activities (similar to [this paper on convex optimization](https://www.usenix.org/legacy/event/hotpar11/tech/final_files/Bird.pdf)).  That turned out to be very difficult, butcertainly shows some interesting long turn promise if you can couple it with adaptive techniques.  Perhaps surprisingly,our most promising results came from adapting [advertisement bidding algorithms](http://research.microsoft.com/en-us/um/people/nikdev/pubs/rtb-perf.pdf) to resource allocation.  Coupled with an elementof [game theory](https://en.wikipedia.org/wiki/Game_theory), this approach gets very interesting.  If the system chargesa market value for all system resources, and all agents in the system have a finite amount of "purchasing power," wecan expect they will purchase those resources that benefit themselves the most based on the market prices available. --><p>这采用了决定要访问哪些进程，决定哪些进程运行的OS调度程序的形式，以及在某些情况下，像流控制这样的技术，就像我们在上面看到的流一样。这是我们最“开放式”和“未解决”的研究领域。我们尝试了许多非常酷的想法，这包括尝试对异步activity的预期资源使用进行建模（类似于<a href="https://www.usenix.org/legacy/event/hotpar11/tech/final_files/Bird.pdf" target="_blank" rel="noopener">关于凸优化的论文</a>）。事实证明这非常困难，但如果你能将它与自适应技术相结合，肯定会获得一些有趣的长期回报。也许令人惊讶的是，我们最有希望的结果来自于使<a href="http://research.microsoft.com/en-us/um/people/nikdev/pubs/rtb-perf.pdf" target="_blank" rel="noopener">广告竞价算法</a>适用于资源分配，再加上<a href="https://en.wikipedia.org/wiki/Game_theory" target="_blank" rel="noopener">博弈论</a>的要素，使得这种方法变得非常有趣。如果系统收取所有系统资源的市场价值，并且系统中的所有代理商都具有有限的“购买力”，我们可以预期他们将根据可用的市场价格购买那些最有利于自己的资源。</p><!-- But automatic management wasn't always perfect.  That's where self-control came in.  A programmer could also help us outby capping the maximum number of outstanding activities, using simple techniques like "wide-loops."  A wide-loop was anasynchronous loop where the developer specified the maximum outstanding iterations.  The system ensured it launched nomore than this count at once.  It always felt a little cheesy but, coupled with resource management, did the trick. --><p>但自动管理并不总是完美的，这就是自我控制发挥作用的地方。程序员也可以通过使用诸如“wide-loops”之类的简单技术，来限制未完成的activity的最大数量。wide-loops作为一个异步循环，其中开发人员指定了最大的未完成迭代数量，系统确保它不会立刻超过此计数的启动值。 虽然总觉得有点俗气，但加上资源管理，就足够了。</p><!-- I would say we didn't die from this one.  We really thought we would die from this one.  I would also say it was solvedto my least satisfaction out of the bunch, however.  It remains fertile ground for innovative systems research. --><p>我可以说我们并没有栽在这里，虽然我们真的以为我们会栽于此。 但我也不得不说这是所有问题的解决方案中我最不满意的一个，不过这方面仍然是创新性系统研究的沃土。</p><!-- ## Winding Down --><h2 id="放松一下"><a href="#放松一下" class="headerlink" title="放松一下"></a>放松一下</h2><!-- That was a lot to fit into one post.  As you can see, we took "asynchronous everywhere" to quite the extreme. --><p>确实有太多的内容适合放进这篇文章中。正如你所看到的，我们将“无处不在的异步”发挥到了极致。</p><!-- In the meantime, the world has come a long way, much closer to this model than when we began.  In Windows 8, a largefocus was the introduction of asynchronous APIs, and, like with adding await to C#, we gave them our own lessons learnedat the time.  A little bit of what we were doing rubbed off, but certainly nothing to the level of what's above. --><p>与此同时，世界也经过漫长的发展，比我们开始时更接近这个模型。 在Windows 8的开发中，异步API的引入是其中一个重点，就像在C#中添加await一样，我们在当时提供给了他们，我们自己的经验和教训。我们正所做事情对其造成了一点影响，但肯定没有达到上面所描述的水平。</p><!-- The resulting system was automatically parallel in a very different way than the standard meaning.  Tons of tinyprocesses and lots of asynchronous messages ensured the system kept making forward progress, even in the face ofvariable latency operations like networking.  My favorite demo we ever gave, to Steve Ballmer, was a mockimplementation of Skype on our own multimedia stack that wouldn't hang even if you tried your hardest to force it. --><p>所产生的系统以与标准含义完全不同的方式自动地进行并行。 大量的微小进程和大量异步消息确保了系统不断前进，即使面对网络等可变延迟操作也是如此。 我最喜欢的一个演示，是向史蒂夫·鲍尔默展示我们在自己的多媒体栈上模拟Skype的实现，即使再怎么尝试它也不会出现挂起现象。</p><!-- As much as I'd like to keep going on architecture and programming model topics, I think I need to take a step back.  Ourcompiler keeps coming up and, in many ways, it was our secret sauce.  The techniques we used there enabled us to achieveall of these larger goals.  Without that foundation, we'd never have had the safety or been on the same playing groundas native code.  See you next time, when we nerd out a bit on compilers. --><p>尽管我想继续研究架构和编程模型相关的主题，但我却认为需要退后一步， 因为我们的编译器在前文中的很多方面不断出现，它也是我们的秘密武器。 我们在编译器上所使用的技术使我们能够实现所有这些更宏大的目标， 没有这个基础，我们就永远不会同时实现安全性和原生代码。下次当我们深入地探讨编译器时再见！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- 
Midori was built out of many ultra-lightweight, fine-grained processes, connected through strongly typed message passing
interfaces.  
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="操作系统" scheme="https://blog.zhangpf.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="并发" scheme="https://blog.zhangpf.com/tags/%E5%B9%B6%E5%8F%91/"/>
    
      <category term="Midori" scheme="https://blog.zhangpf.com/tags/Midori/"/>
    
      <category term="翻译" scheme="https://blog.zhangpf.com/tags/%E7%BF%BB%E8%AF%91/"/>
    
      <category term="异步" scheme="https://blog.zhangpf.com/tags/%E5%BC%82%E6%AD%A5/"/>
    
  </entry>
  
  <entry>
    <title>Midori博客系列翻译（2）——对象即安全权能</title>
    <link href="https://blog.zhangpf.com/2018/11/18/midori/2-objects-as-secure-capabilities/"/>
    <id>https://blog.zhangpf.com/2018/11/18/midori/2-objects-as-secure-capabilities/</id>
    <published>2018-11-18T05:15:00.000Z</published>
    <updated>2020-01-29T02:19:38.926Z</updated>
    
    <content type="html"><![CDATA[<!-- [Last time](http://joeduffyblog.com/2015/11/03/a-tale-of-three-safeties/), wesaw how Midori built on a foundation of type, memory, and concurrency safety.This time, we will see how this enabled some novel approaches to security.Namely, it let our system eliminate [ambient authority and access control](https://en.wikipedia.org/wiki/Ambient_authority) in favor of [capabilities](https://en.wikipedia.org/wiki/Capability-based_security) woven into the fabricof the system and its code.  As with many of our other principles, theguarantees were delivered "by-construction" via the programming language and itstype system. --><p><a href="/2018/10/24/midori/1-a-tale-of-three-safeties/">在上一篇博客中</a>，我们已经看到Midori是如何建立在类型，内存和并发安全的基础之上的。 在本文中，我们将看到它们又是使一些新颖的方法来实现安全变得可能，也就是说，这些方法让我们的系统消除了<a href="https://en.wikipedia.org/wiki/Ambient_authority" target="_blank" rel="noopener">环境权限和访问控制</a>问题，有利于编织到系统及其代码的结构中的<a href="https://en.wikipedia.org/wiki/Capability-based_security" target="_blank" rel="noopener">权能</a>之上。 与我们的许多其他原则一样，这种保证是通过编程语言及其类型系统“从构造”处提供的。</p><!-- # Capabilities --><h1 id="权能"><a href="#权能" class="headerlink" title="权能"></a>权能</h1><!-- First and foremost: what the heck are capabilities? --><p>首要的问题是，权能究竟是什么？</p><!-- In the security systems most of us know and love, i.e. UNIX and Windows,permission to do something is granted based on identity, typically in the formof users and groups.  Certain protected objects like files and system calls haveaccess controls attached to them that restrict which users and groups can usethem.  At runtime, the OS checks that the requested operation is permitted basedon these access controls, using ambient identity like what user is running thecurrent process. --><p>在我们大多数人都知道和喜爱的安全系统中（例如UNIX和Windows），授予做某事的许可是基于身份的，通常以用户（user）和组（group）的形式出现。 某些受保护的对象（如文件和系统调用）具有附加到这些身份的访问控制方法，这些控制限制了哪些用户和组可以使用该对象。在运行时，操作系统使用环境标识，如正在运行当前进程的用户，基于这些访问控制来检查是否允许执行所请求的操作。</p><!-- To illustrate this concept, consider a simple C call to the `open` API:    void main() {        int file = open("filename", O_RDONLY, 0);        // Interact with `file`...    }--><p>为了说明这个概念，考虑对<code>open</code> API进行如下简单的C语言调用：</p><pre><code>void main() {    int file = open(&quot;filename&quot;, O_RDONLY, 0);    // 与`file`进行交互...}</code></pre><!-- Internally, this call is going to look at the identity of the current process,the access control tables for the given file object, and permit or reject thecall accordingly.  There are various mechanisms for impersonating users, like`su` and `setuid` on UNIX and `ImpersonateLoggedOnUser` on Windows.  But theprimary point here is that `open` just "knew" how to inspect some global stateto understand the security implications of the action being asked of it.Another interesting aspect of this is that the `O_RDONLY` flag is passed, askingfor readonly access, which factors into the authorization process too. --><p>在内部，此调用将查看当前进程的标识，给定的文件对象的访问控制表，以及相应地允许或拒绝调用的标识符。 有大量的机制用于模仿用户的各种机制，例如UNIX上的<code>su</code>和<code>setuid</code>操作以及Windows上的<code>ImpersonateLoggedOnUser</code>操作。 但这里的主要问题是<code>open</code>仅仅是“知道”如何检查一些全局状态，以了解所请求操作的的安全含义。另一个有趣的方面是传递了要求进行只读访问的<code>O_RDONLY</code>标志，这也会影响对授权过程产生影响。</p><!--Well, what's wrong with this? --><p>呃，那么这有何问题呢？</p><!-- It's imprecise.  It relies on ambient state that is invisible to the program.You can't easily audit to see the security implications of the operation.  Youjust need to know how `open` works.  And thanks to its imprecision, it's easy toget wrong, and going here wrong means security vulnerabilities.  Specifically,it's easy to trick a program into doing something on behalf of a user that itwas never intended to do.  This is called the ["confused deputy problem"](http://c2.com/cgi/wiki?ConfusedDeputyProblem).  All you need to do is trick theshell or program into impersonating a superuser, and you're home free. --><p>问题在于基于环境权限的访问控制这是不精确的，它依赖于对程序不可见的环境状态，因此无法轻松地对操作存在的安全隐患进行审计。 你只需要知道<code>open</code>是如何工作的，而且正是由于不精确，所以很容易出错，而错误通常导致安全漏洞。 具体来说，它很容易伪装成用户并欺骗程序做一些从未打算做的事情。 这被称为<a href="http://c2.com/cgi/wiki?ConfusedDeputyProblem" target="_blank" rel="noopener">“混淆代理人问题”</a>。你需要做的就是欺骗shell或程序以冒充超级用户，那么就几乎可以做任何特权操作。</p><!-- [Capability-based security](https://en.wikipedia.org/wiki/Capability-based_security),on the other hand, isn't reliant on global authority in this same way.  It usesso-called "unforgeable tokens" to represent the ability to perform privilegedoperations.  No matter how the decision gets made -- there is an entirely complextopic of policy management and granting authority that gets into social and humanbehaviors -- if the software isn't meant to perform some operation, it simplynever receives the token necessary to do said operation.  And because tokens areunforgeable, the program can't even attempt the operation.  In a system likeMidori's, type safety meant that not only could the program not perform theoperation, it would often be caught at compile-time. --><p>相反地，<a href="(https://en.wikipedia.org/wiki/Capability-based_security">基于权能的安全性</a>不以同样的方式依赖于全局权限。 它使用所谓的“不可伪造的令牌”来表示执行特权操作的能力。 无论决策是如何制定的，都存在一个完全复杂的策略管理主题和关于社会和人类行为的授权行为。总的来说，如果软件无意执行某些操作，它根本就不会收到执行这些操作所需的令牌。 并且由于令牌是不可伪造的，程序甚至无法尝试操作。在像Midori这样的系统中，类型安全也意味着程序不仅不能执行未授权的操作，而且通常会在编译期间捕获这些操作。</p><!-- Insecure operations rejected at compile-time, how cool is that! --><p>在编译时拒绝了不安全操作，这有多酷！</p><!-- The hypothetical `open` API from earlier, as you may have guessed, would lookvery different:     void main(File file) {        // Interact with `file`...    }--><p>正如您可能已经猜到的那样，之前假设的<code>open</code> API看起来会非常不同：</p><pre><code>void main(File file) {    // 和`file`交互...}</code></pre><!-- OK, clearly we're not in Kansas anymore.  This is *extremely* different.  AndI've just passed the buck.  *Someone else* has to show up with a File object?How do they get one? --><p>好的，显然我们不再是在基于环境权限的访问控制范畴内了，那么事情将变得<em>非常</em>不同。 我刚刚没有提到的是，这里的<em>其他部分（调用者）</em>必须获得一个File对象，那他们又是如何得到的？</p><!--The trite answer is, who cares, that's up to the caller.  But if they *do* showup with one, they must have been authorized to get it, because object referencesin a type safe system are unforgeable.  The matter of policy and authorizationare now pushed to the source where, arguably, they belong.--><p>老套的回答是，没有人会谁在乎，获得的方式完全取决于调用者。但是如果他们<em>确实</em>持有File句柄，那么它们必须被授权访问File，因为在类型安全的系统中，对象引用是不可伪造的。 策略和授权的问题现在被推到可以说它们本来就属于的源头处。</p><!-- I'm over-simplifying a little bit, since this answer likely raised more questionsthan it actually answered.  Let's keep digging deeper. --><p>我想我可能过度简化了一点，因为这个回答可能会产生更多的问题。 那么让我们继续深入分析。</p><!-- So, again, let's ask the question: how does one get their hands on a File object? --><p>那么，让我们再问一个问题：如何获得File对象？</p><!-- The code above neither knows nor cares whether where it came from.  All it knowsis it is given an object with a File-like API.  It might have been `new`'d up bythe caller.  More likely, it was obtained by consulting a separate entity, likea Filesystem or a Directory, both of which are also capability objects: --><p>上面的代码既不知道也不关心File来自何处。 它只知道给它一个具有类File的API的对象。 它可能是由调用者通过<code>new</code>操作获得， 更有可能的情况是，它是通过调用一个单独的实体获得的，比如文件系统或目录，而这两个实体也都是权能对象：</p><pre><code>Filesystem fs = ...;Directory dir = ... something(fs) ...;File file = ... something(dir) ...;MyProgram(file);</code></pre><!-- You might be getting really angry at me now.  Where did `fs` come from?  How didI get a Directory from `fs`, and how did I get a File from `dir`?  I've justsquished all the interesting topics around, like a water balloon, and answerednothing at all! --><p>你现在可能真的会对我生气了。<code>fs</code>又来自哪里？我又如何从<code>fs</code>获取Directory对象？我又是如何从<code>dir</code>获取File对象的？我刚刚把所有有趣的话题都挤到一起，就像水球一样，但却什么都没回答！</p><!-- The reality is that those are all the interesting questions you encounter nowwhen you try to design a filesystem using capabilities.  You probably don't wantto permit free enumeration of the entire filesystem hierarchy, because if youget access to a Filesystem object -- or the system's root Directory -- you canaccess everything, transitively.  That's the sort of thinking you do when youbegin dealing with capabilities.  You think hard about information encapsulationand exposure, because all you've got are objects to secure your system.Probably, you'll have a way that a program requests access to some statesomewhere on the Filesystem, declaratively, and then the "capability oracle"decides whether to give it to you.  This is the role our application modelplayed, and is how `main` got its hands on the capabilities a program's manifestdemanded that it needs.  From that point onwards it's just objects.  The key isthat nowhere in the entire system will you find the classical kind of ambientauthority, and so none of these abstractions can "cheat" in their construction. --><p>现实情况是，当你尝试使用权能设计文件系统时，这些都是你现在将遇到的所有有趣问题。你应该不希望允许用户自由地在整个文件系统层次结构上进行枚举访问，因为如果用户可以访问Filesystem对象，或系统的根目录，那么它其实可以以向下传递的方式访问所有内容。这就是你开始和权能打交道时所做的那种想法。您认真考虑信息封装和曝光，因为您所拥有的只是保护系统安全的对象。也许，你会有一种方法，一个程序请求在文件系统的某个地方请求访问某个状态，声明，然后“权能母体”决定是否给你。这是我们的应用程序模型所扮演的角色，主要是如何<code>main</code>掌握程序清单所需的权能。从那时起，它只是对象。关键是整个系统中没有任何地方可以找到经典的环境权威，因此这些抽象都不能在其构造中“作弊”。</p><!-- A classic paper, [Protection](http://research.microsoft.com/en-us/um/people/blampson/08-Protection/Acrobat.pdf),by Butler Lampson clearly articulates some of the key underlying principles, likeunforgeable tokens.  In a sense, each object in our system is its own "protectiondomain."  I also love [Capability Myths Demolished](http://srl.cs.jhu.edu/pubs/SRL2003-02.pdf)'s way of comparing and contrastingcapabilities with classical security models, if you want more details (orincorrectly speculate that they might be isomorphic). --><p>Butler Lampson的一篇经典论文<a href="http://research.microsoft.com/en-us/um/people/blampson/08-Protection/Acrobat.pdf" target="_blank" rel="noopener">“Protection”</a>清楚地阐明了一些设计上关键基本原则，例如不可伪造的令牌。 从某种意义上说，我们系统中的每个对象都是它自己的“保护域”。如果想了解更多的细节（或者错误地任务访问控制列表和基于权能的系统是等价的），那么我也推荐<a href="http://srl.cs.jhu.edu/pubs/SRL2003-02.pdf" target="_blank" rel="noopener">“Capability Myths Demolished”</a>中权能与经典安全模型进行比较和对比的方式。</p><!--Midori was by no means the first to build an operating systems with objectcapabilities at its core.  In fact, we drew significant inspiration from[KeyKOS](http://www.cis.upenn.edu/~KeyKOS/NanoKernel/NanoKernel.html) and itssuccessors [EROS](https://en.wikipedia.org/wiki/EROS_(microkernel)) and[Coyotos](http://www.coyotos.org/docs/misc/eros-comparison.html).  Thesesystems, like Midori, leveraged object-orientation to deliver capabilities.  Wewere lucky enough to have some of the original designers of those projects onthe team. --><p>Midori绝不是第一个以对象权能为核心构建操作系统的系统。事实上，我们从<a href="http://www.cis.upenn.edu/~KeyKOS/NanoKernel/NanoKernel.html" target="_blank" rel="noopener">KeyKOS</a>及其后继者<a href="http://t.cn/EVRHoZV" target="_blank" rel="noopener">EROS</a>和<a href="http://www.coyotos.org/docs/misc/eros-comparison.html" target="_blank" rel="noopener">Coyotos</a>中获得了重要的灵感。 这些系统像Midori一样，利用面向对象方式来提供权能，我们很幸运的是能够在团队中拥有这些项目的一些最初设计者。</p><!-- Before moving on, a warning's in order: some systems confusingly use the term"capability" even though aren't true capability systems.  [POSIX defines such asystem](http://c2.com/cgi/wiki?PosixCapabilities) and so [both Linux and Androidinherit it](https://www.kernel.org/pub/linux/libs/security/linux-privs/kernel-2.2/capfaq-0.2.txt).Although POSIX capabilities are nicer than the typical classical ambient stateand access control mechanisms -- enabling finer-grained controls than usual --they are closer to them than the true sort of capability we're discussing here.--><p>在继续讨论之前，按顺序发出警告：即使某些系统不是真正的权能系统，它们也会混淆地使用“capability”这个术语。 例如，<a href="http://c2.com/cgi/wiki?PosixCapabilities" target="_blank" rel="noopener">POSIX定义了这样一个系统</a>，因此<a href="https://www.kernel.org/pub/linux/libs/security/linux-privs/kernel-2.2/capfaq-0.2.txt" target="_blank" rel="noopener">Linux和Android都继承使用了它</a>。 虽然POSIX的“权能”比典型的经典基于环境状态和访问控制机制表现更好，因为它实现了比这些方式更细粒度的控制，但它们确实比我们在这里讨论的真正权能更接近经典模型。</p><!-- # Objects and State --><h1 id="对象和状态"><a href="#对象和状态" class="headerlink" title="对象和状态"></a>对象和状态</h1><!-- A nice thing about capabilities just being objects is that you can apply existingknowledge about object-orientation to the domains of security and authority.--><p>作为对象的权能的一个好处是，你可以将有关面向对象的现有知识应用于安全和权限领域。</p><!-- Since objects represent capabilities, they can be as fine or coarse as youwish.  You can make new ones through composition, or modify existing onesthrough subclassing.  Dependencies are managed just like any dependencies in anobject-oriented system: by encapsulating, sharing, and requesting references toobjects.  You can leverage all sorts of [classic design patterns](https://en.wikipedia.org/wiki/Design_Patterns) suddenly in the domain ofsecurity.  I do have to admit the simplicity of this idea was jarring to some.--><p>由于对象代表着权能，因此它们可以如你所希望的那样进行细粒度或粗粒度控制。您可以通过合成方式创建新的权能，或通过继承方式修改现有的权能。 依赖关系的管理方式与面向对象系统中的任何依赖关系一样：通过封装，共享和请求对象的引用。 因此你可以在安全领域利用各种<a href="https://en.wikipedia.org/wiki/Design_Patterns" target="_blank" rel="noopener">经典的设计模式</a>。 但我不得不承认这个想法过于简单，以至于使某些人感到震惊。</p><!-- One fundamental idea is [revocation](http://c2.com/cgi/wiki?RevokableCapabilities).An object has a type and our system let you substitute one implementation in placeof another.  That means if you ask me for a Clock, I needn't give you access toa clock for all time, or even the real one for that matter.  Instead, I can giveyou my own subclass of a Clock that delegates to the real one, and rejects yourattempts after some event occurs.  You've got to either trust the source of theclock, or explicitly safe-guard yourself against it, if you aren't sure. --><p>一个基本的想法是<a href="http://c2.com/cgi/wiki?RevokableCapabilities" target="_blank" rel="noopener">撤销（revocation）</a>。 对象是具有类型的，我们的系统允许使用另一个实现来替换现有的实现。 这意味着如果你向我请求一个Clock对象，我无需在任何时候都向你授予访问时钟的权限，我甚至都不需向你提供真正的Clock对象。 相反地，我可以向你提供我自己实现的一个Clock子类，它作为真正Clock的代理，并可以在某些事件发生后拒绝你的请求。 因此你必须要么信任时钟源，要么在在不确定的情况下，显式地保护自己免受攻击。</p><!-- Another concept is state.  In our system, we banned mutable statics,by-construction at compile-time, in our programming language.  That's right, notonly could a static field only be written to once, but the entire object graphit referred to was frozen after construction.  It turns out mutable statics arereally just a form of ambient authority, and this approach prevents someonefrom, say, caching a Filesystem object in a global static variable, and sharingit freely, thereby creating something very similar to the classical securitymodels we are seeking to avoid.  It also had many benefits in the area of safeconcurrency and even gave us performance benefits, because statics simply becamerich constant object graphs that could be frozen and shared across binaries. --><p>另一个概念是状态。在我们的系统中，我们通过在编译期间“从构造”的方式，在编程语言中禁掉了可变的静态变量。这是正确的，不仅静态字段只能被写入一次，而且它所引用的整个对象图在构造之后也将被冻结。 事实证明，可变静态变量实际上只是环境权限的一种形式，不可变静态变量可以阻止用户在全局静态变量中缓存Filesystem对象，并自由地共享它，从而构造出一些和经典安全模型非常类似，而且是Midori极力避免的东西。 不可变静态变量在安全并发方面也有很多好处，甚至给我们带来了性能优势，因为这种方式下，静态只是变成了更加丰富的常量对象图，可以在二进制文件中固化和共享。</p><!-- The total elimination of mutable statics had an improvement to our system'sreliability that is difficult to quantify, and difficult to understate.  This isone of the biggest things I miss. --><p>完全消除可变静态变量对Midori系统的可靠性带来了难以量化和低估的改善，而这也是我最怀念的地方之一。</p><!-- Recall my mention of Clock above.  This is an extreme example, however, yes,that's right, there was no global function to read time, like C's `localtime` orC#'s `DateTime.Now`.  To get the time, you must explicitly request a Clockcapability.  This has the effect of eliminating non-determinism from an entireclass of functions.  A static function that doesn't do IO -- [something we canascertain in our type system (think Haskell monads)](http://research.microsoft.com/apps/pubs/default.aspx?id=170528) -- now becomespurely functional, memoizable, and even something we can evaluate atcompile-time (a bit like [`constexpr`](http://en.cppreference.com/w/cpp/language/constexpr) on steroids). --><p>回想一下上面提到的Clock，这是一个极端的例子。但没错的是，它没有诸如C的<code>localtime</code>或C#的<code>DateTime.Now</code>的读取时间的全局函数，而为了获得时间，你必须显式地请求Clock权能，而这具有消除整个类函数中非确定性的效果。一个无需IO，即<a href="http://research.microsoft.com/apps/pubs/default.aspx?id=170528" target="_blank" rel="noopener">可以在类型系统中确定（想想Haskell的monad）</a> 的静态函数，现在变得纯函数化、可记忆化并且甚至可以在编译时进行eval（这有点类似于<a href="http://en.cppreference.com/w/cpp/language/constexpr" target="_blank" rel="noopener"><code>constexpr</code> on steroids</a>）。</p><!-- I'll be the first to admit, there was a maturity process that developers wentthrough, as they learned about the design patterns in an object capabilitysystem.  It was common for "big bags" of capabilities to grow over time, and/orfor capabilities to be requested at an inopportune time.  For example, imaginea Stopwatch API.  It probably needs the Clock.  Do you pass the Clock to everyoperation that needs to access the current time, like Start and Stop?  Or do youconstruct the Stopwatch with a Clock instance up-front, thereby encapsulatingthe Stopwatch's use of the time, making it easier to pass to others (recognizing,importantly, that this essentially grants the capability to read the time to therecipient).  Another example, if your abstraction requires 15 distinctcapabilities to get its job done, does its constructor take a flat list of 15objects?  What an unwieldy, annoying constructor!  Instead, a better approach isto logically group these capabilities into separate objects, and maybe even usecontextual storage like parents and children to make fetching them easier. --><p>我将首先承认，这将存在一个逐渐成熟的过程，是开发者需要面对的，正如他们学习了对象权能系统中的设计模式。 权能的“大袋子”随着时间的推移而增长，以及在不合时宜的时候请求权能是很常见的。例如，设想存在一个秒表Stopwatch的API，它可能会需要Clock，那么你是否需要将Clock传递给需要访问当前时间的每个操作，例如Start和Stop？或者你是否预先构建了一个带有Clock实例的Stopwatch，从而将秒表对时间的使用进行封装，使其更容易传递给其他对象（重要的是，意识到这基本上向接收者赋予了读取时间的权能）。另一个例子，如果你的抽象需要15个不同的权能才能完成它的工作，那么它的构造函也需要使用15个参数的列表？这将是多么笨重和烦人的构造函数！相反，更好的方法是将这些权能逻辑地分组到单独的对象中，甚至可以使用父类和子类等上下文存储来简化它们的获取。</p><!-- The weaknesses of classical object-oriented systems also rear their ugly heads.Downcasting, for example, means you cannot entirely trust subclassing as a meansof [information hiding](https://en.wikipedia.org/wiki/Information_hiding).  Ifyou ask for a File, and I supply my own CloudFile that derives from File and addsits own public cloud-like functions to it, you might sneakily downcast toCloudFile and do things I didn't intend.  We addressed this with severerestrictions on casting and by putting the most sensitive capabilities on anentirely different plan altogether... --><p>经典的面向对象系统的缺陷也给这种方式带来了弱点。 例如，向下类型转换（downcasting）意味着你不能完全信任继承作为<a href="https://en.wikipedia.org/wiki/Information_hiding" target="_blank" rel="noopener">信息隐藏</a>的手段。 如果你请求一个File，而我提供了派生自File的CloudFile类，它向其添加了自己的类似公有云的功能，那么你可能会悄悄地向下转换为CloudFile并执行我不想要的操作。 我们通过对类型转换的严格限制，以及将最敏感的权能放在另一个完全不同的计划上来解决此问题……</p><!-- # Distributed Capabilities --><h1 id="分布式权能"><a href="#分布式权能" class="headerlink" title="分布式权能"></a>分布式权能</h1><!-- I'll briefly touch on an area that warrants a lot more coverage in a futurepost: our asynchronous programming model.  This model formed the foundation ofhow we did concurrent, distributed computing; how we performed IO; and, mostrelevant to this discussion, how capabilities could extend their reach acrossthese critical domains. --><p>我将简要介绍在未来的帖子中需要进一步涉及的领域：我们的异步编程模型。 该模型构成了我们如何进行并发和分布式计算的基础，和我们执行IO的方式，以及与本文最相关的是，权能是如何扩展它们在这些关键域中的覆盖范围的。</p><!-- In the Filesystem example above, our system often hosted the real object behindthat Filesystem reference in a different process altogether.  That's right,invoking a method actually dispatched a remote call to another process, whichserviced the call.  So, in practice, most, but not all, capabilities wereasynchronous objects; or, more precisely, unforgeable tokens that permit one totalk with them, something we called an "eventual" capability.  The Clock was acounter-example to this.  It was something we called a "prompt" capability:something that wrapped a system call, rather than a remote call.  But mostsecurity-related capabilities tended to be remote, because most interestingthings that require authority bottom out on some kind of IO.  It's rare you needauthority to simply perform a computation.  In fact, the filesystem, networkstack, device drivers, graphics surfaces, and a whole lot more took the form ofeventual capabilities. --><p>在上面的Filesystem示例中，我们的系统通常在不同的进程中托管该Filesystem后面引用的真实对象。 这种方式下，调用一个方法实际上是将一个远程调用分派到另一个进程上，而进程为该调用提供相应的服务。因此，在实践中，大多数（但不是全部的）权能都是异步对象，或者更确切地说，是允许与服务交互的不可伪造的令牌，我们称之为“最终（eventual）权能”。而Clock却是一个反例，它是我们称之为“提示（prompt）权能”——它包含系统调用而不是远程调用。 但是大多数与安全相关的权能往往是远程的，因为大多数需要权限的操作通常会最终触及到IO上，而很少需要权限来仅仅执行简单地计算。 而实际上，文件系统、网络堆栈、设备驱动程序、图形界面以及更多的子系统都采用了最终权能的形式。</p><!-- This unification of overall security in the OS and how we built distributed, andhighly concurrent, secure systems, was one of our largest, innovative, and mostimportant accomplishments. --><p>这种操作系统整体安全性的统一以及我们构建分布式的，高度并发的安全系统的方式，是我们最显著，最具创新性和最重要的成就之一。</p><!-- I should note, like the idea of capabilities in general, similar ideas werepioneered well before Midori.  Although we didn't use the languages directly,the ideas from the [Joule](https://en.wikipedia.org/wiki/Joule_(programming_language)) language and, later,[E](https://en.wikipedia.org/wiki/E_(programming_language)), laid some verypowerful foundations for us to build upon.  [Mark Miller's 2006 PhD thesis](http://www.erights.org/talks/thesis/markm-thesis.pdf) is a treasure trove ofinsights in this entire area.  We had the privilege of working closely with oneof the brightest minds I've ever worked alongside, who happened to have been achief designer of both systems. --><p>我应该指出的是，就像通用权能的想法一样，类似的想法在Midori之前就已经存在。虽然我们没有直接使用，但是来自于<a href="http://t.cn/EV8ovNL" target="_blank" rel="noopener">Joule语言</a>和后来的<a href="http://t.cn/EV8S8gf" target="_blank" rel="noopener">E语言</a>的想法为我们提供了非常强大的构建基础。<a href="http://www.erights.org/talks/thesis/markm-thesis.pdf" target="_blank" rel="noopener">Mark Miller在2006年的博士论文</a>是这整个领域的重要财富。 我们有幸与我曾合作过的最聪明的人之一密切合作，而他恰好是这两个系统的首席设计师。</p><!-- # Wrapping Up --><h1 id="封装起来"><a href="#封装起来" class="headerlink" title="封装起来"></a>封装起来</h1><!-- There is so much to say about the benefits of capabilities.  The foundation oftype safety let us make some bold leaps forward.  It led to a very differentsystem architecture than is commonplace with ambient authority and accesscontrols.  This system brought secure, distributed computing to the forefront ina way that I've never witnessed before.  The design patterns that emerged reallyembraced object-orientation to its fullest, leveraging all sorts of designpatterns that suddenly seemed more relevant than ever before. --><p>关于权能的优点还有太多地方可以说的。总之，类型安全的基础让我们大踏步前进，它产生了一种与环境权限和访问控制相比非常不同的系统架构。该系统以前所未有的方式将安全的分布式计算带到了最前沿。出现的设计模式确实充分利用了面向对象，也充分利用了各种看起来比以往更加重要的设计模式。</p><!--We never did get much real-world exposure on this model.  The user-facingaspects were under-explored compared to the architectural ones, like policymanagement.  For example, I doubt we'd want to ask my mom if she wants to letthe program use a Clock.  Most likely we'd want some capabilities to be grantedautomatically (like the Clock), and others to be grouped, through composition,into related ones.  Capabilities-as-objects thankfully gives us a plethora ofknown design patterns for doing this.  We did have a few honey pots, and noneever got hacked (well, at least, we didn't know if we did), but I cannot attestfor sure about the quantifiable security of the resulting system.  QualitativelyI can say we felt better having the belts-and-suspenders security at many layersof the system's construction, but we didn't get a chance to prove it at scale.--><p>我们从未在该模型上进行较多的曝光。与策略管理等体系结构方面相比，面向用户的层面还未得到充分研究。例如，我很怀疑我们是否想在程序界面中提示我的母亲，她是否想让程序使用Clock。最有可能的方式是，我们希望自动授予某些权能（如时钟），并将其他权能通过组合的方式分组为相关的权能，而作为对象的权能幸运地为我们提供了大量已知的设计模式。我们的系统中确实有一些蜜罐，却没有一个被黑客攻击（好吧，至少我们尚不知道有被攻击成功过），但我无法确定最终系统的可量化安全性。因此我可以定性地说，在系统结构的许多层面上感觉具有更好的冗余安全性，但我们却没有机会大规模地加以证明。</p><!-- In the next article, we'll dig deeper into the asynchronous model that ran deepthroughout the system.  These days, asynchronous programming is a hot topic,with `await` showing up in [C#](https://msdn.microsoft.com/en-us/library/hh156528.aspx), [ECMAScript7](http://tc39.github.io/ecmascript-asyncawait/), [Python](https://www.python.org/dev/peps/pep-0492/), [C++](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4134.pdf), and more.This plus the fine-grained decomposition into lightweight processes connected bymessage passing were able to deliver a highly concurrent, reliable, andperformant system, with asynchrony that was as easy to use as in all of thoselanguages.  See you next time!--><p>在下一篇博客中，我们将深入研究贯穿于整个系统的异步模型。异步编程在当前是一个热门话题，例如<code>await</code>出现在<a href="https://msdn.microsoft.com/en-us/library/hh156528.aspx" target="_blank" rel="noopener">C#</a>，<a href="http://tc39.github.io/ecmascript-asyncawait/" target="_blank" rel="noopener">ECMAScript7</a>，以及<a href="https://www.python.org/dev/peps/pep-0492/" target="_blank" rel="noopener">Python</a>和<a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4134.pdf" target="_blank" rel="noopener">C++</a>等语言中。同时加上通过消息传递方式连接的细粒度分解的轻量级进程，可实现高度并发，可靠且高性能的系统，其异步性与所有这些语言一样易于使用。下篇博客见！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- 
[Last time](http://joeduffyblog.com/2015/11/03/a-tale-of-three-safeties/), we
saw how Midori built on a foundation of type, memory, an
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="操作系统" scheme="https://blog.zhangpf.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="安全" scheme="https://blog.zhangpf.com/tags/%E5%AE%89%E5%85%A8/"/>
    
      <category term="Midori" scheme="https://blog.zhangpf.com/tags/Midori/"/>
    
      <category term="翻译" scheme="https://blog.zhangpf.com/tags/%E7%BF%BB%E8%AF%91/"/>
    
      <category term="权能" scheme="https://blog.zhangpf.com/tags/%E6%9D%83%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Midori博客系列翻译（1）——三类安全性的故事</title>
    <link href="https://blog.zhangpf.com/2018/10/24/midori/1-a-tale-of-three-safeties/"/>
    <id>https://blog.zhangpf.com/2018/10/24/midori/1-a-tale-of-three-safeties/</id>
    <published>2018-10-24T05:02:00.000Z</published>
    <updated>2020-01-29T02:19:38.925Z</updated>
    
    <content type="html"><![CDATA[<!-- [Midori](/2015/11/03/blogging-about-midori) was built on a foundation of threekinds of safety: type, memory, and concurrency-safety.  These safeties eliminatedwhole classes of bugs "by-construction" and delivered significant improvements inareas like reliability, security, and developer productivity.  They also fundamentallyallowed us to depend on the type system in new and powerful ways, to deliver newabstractions, perform novel compiler optimizations, and more.  As I look back,the biggest contribution of our project was proof that an entire operatingsystem and its ecosystem of services, applications, and libraries could indeedbe written in safe code, without loss of performance, and with some quantumleaps forward in several important dimensions. --><p><a href="/2018/10/20/midori/0-blogging-about-midori/">Midori</a>建立在三类安全性的基础之上，这包括：类型安全、内存安全和并发安全。 它们“从构造”上消除了的各类错误，并在可靠性、安全性和开发人员生产力等方面取得了重大改进。 另外，它们还从根本上允许我们以新颖强大的方式，依赖类型系统来提供新的抽象、执行最新的编译器优化等。 回顾过去，我们项目的最大贡献就是证明了整个操作系统及其服务、应用程序和库的生态系统确实可以使用安全的代码编写，同时不会损失性能，并且在数个重要维度上取得了一些重大突破。</p><!-- First, let us define the three safeties, in foundational order: --><p>首先，让我们按照基本顺序定义三类安全性：</p><!-- * [Memory Safety](https://en.wikipedia.org/wiki/Memory_safety) prohibits access  to invalid regions of memory.  Numerous flaws arise when memory safety is  violated, including buffer overflow, use after free, and double frees.   Generally speaking, violation of memory safety is a critical error that can  lead to exploits such as code injection. --><ul><li><a href="https://en.wikipedia.org/wiki/Memory_safety" target="_blank" rel="noopener">内存安全</a>禁止访问无效的内存区域。当破坏内存安全性时会产生多种缺陷，这包括缓冲区溢出，释放后使用（use after free，UAF）和双重释放（double free）等。一般来讲，违反内存安全性是严重的错误，可能导致代码注入等漏洞。</li></ul><!-- * [Type Safety](https://en.wikipedia.org/wiki/Type_safety) prohibits use of  memory that is at odds with the type allocated within that memory.  Numerous  flaws arise when type safety is violated, including type confusion, casting  errors, and uninitialized variables.  Although generally less severe than  memory safety violations, type safety violations can lead to exploits,  particularly when exposing pathways to memory safety holes. --><ul><li><a href="https://en.wikipedia.org/wiki/Type_safety" target="_blank" rel="noopener">类型安全</a>禁止以与内存分配时类型不一致的类型方式访问该内存。当违背类型安全时也会导致多种缺陷，这包括类型混淆，数据类型转换错误和未初始化的变量等。 虽然通常不如内存安全严重，但类型安全被破坏也可能导致漏洞，特别是当间接导致内存安全漏洞时。</li></ul><!-- * [Concurrency Safety](https://en.wikipedia.org/wiki/Thread_safety) prohibits  unsafe concurrent use of shared memory.  These concurrency hazards are widely  known in the form of [data races](  https://en.wikipedia.org/wiki/Race_condition), or read-write, write-read, and  write-write hazards.   Generally speaking, if concurrency safety is violated,  it can frequently lead to type, and therefore memory, safety being violated.  These exploits are often quite subtle -- like tearing memory -- and we often  said that concurrency vulnerabilities are the "next frontier" of exploitable  security holes. --><ul><li><a href="https://en.wikipedia.org/wiki/Thread_safety" target="_blank" rel="noopener">并发安全</a>禁止以不安全的方式并发使用共享内存。这些并发冒险以<a href="https://en.wikipedia.org/wiki/Race_condition" target="_blank" rel="noopener">数据竞争</a>，或读后写、写后读和写后写冒险的形式广为人知。一般来说，如果违背了并发安全性，常常会导致类型安全，进而内存安全被破坏。 并发安全的漏洞通常非常微妙，例如内存撕裂（memory tearing）等，因此我们认为并发性漏洞是可利用安全漏洞的“下一个热点领域”。</li></ul><!-- Many approaches exist to establish one or more of these safeties, and/orsafeguard against violations. --><p>存在多种方法来构建上述三种安全性中的一个或多个，以及防止被破坏的安全手段。</p><!-- [Software fault isolation](http://www.cs.cmu.edu/~srini/15-829/readings/sfi.pdf)establishes memory safety as a backstop against the most severe exploits.  Thiscomes at some runtime cost, although [proof carrying code](https://en.wikipedia.org/wiki/Proof-carrying_code) can lessen it.  Thesetechniques don't deliver all the added benefits of type and concurrency safety. --><p><a href="http://www.cs.cmu.edu/~srini/15-829/readings/sfi.pdf" target="_blank" rel="noopener">软件故障隔离（Software fault isolation）</a>构建内存安全，作为对抗最严重攻击的措施，但这种方式也带来了一些运行时开销。尽管<a href="https://en.wikipedia.org/wiki/Proof-carrying_code" target="_blank" rel="noopener">携带证明的代码</a>可以减少开销，但这些技术也无法提供类型安全和并发安全。</p><!-- Language-based safety, on the other hand, is done through a system of typesystem rules and local checks (as opposed to global) that, inductively, ensurecertain operations do not occur, plus optional runtime checks (like array boundschecking in the absence of a more capable [dependent type system](https://en.wikipedia.org/wiki/Dependent_type)).  The benefits of this approachare often a more productive approach to stopping safety holes because a developerfinds them while writing his or her code, rather than at runtime.  But if you cantrick the type system into permitting an illegal operation, you're screwed,because there is no backstop to prevent hackers from violating memory safety inorder to running arbitrary code, for example. --><p>另一方面，基于语言的安全，则是通过类型系统规则和局部检查（与全局相反的）体系来完成，通过推导的方式，该体系可确保不发生违反安全性的操作，再加上可选的运行时检查（例如在缺乏更强大的<a href="https://en.wikipedia.org/wiki/Dependent_type" target="_blank" rel="noopener">依赖类型系统</a>时的数组边界检查）。 这种方法的好处是它采取一种通常更有效的方法来阻止安全漏洞，因为开发人员不是在软件运行时，而在编写代码时便可发现这些漏洞。 但是，如果你采取欺骗的方式使得类型系统允许非法得操作，你就完蛋了，因为没有后备措施可以阻止黑客们违反内存安全性以运行任意代码。</p><!-- Multiple techniques are frequently used in conjunction with another, somethingcalled "defense in depth," in order to deliver the best of all of thesetechniques. --><p>多种技术经常被结合一起使用，以获得这些技术的所有优点，这也被称为“深度防御”。</p><!-- Good examples of the runtime approach to safety include [Google's C++ sanitizers](https://github.com/google/sanitizers) and [Microsoft's "/guard" feature](http://blogs.msdn.com/b/vcblog/archive/2014/12/08/visual-studio-2015-preview-work-in-progress-security-feature.aspx).Good examples of the language approach include C#, Java, most functionallanguages, Go, etc.  We can see some cracks already, however, since C# has the`unsafe` keyword which permits unsafe regions that violate safety. --><p>采用运行时安全的优秀案例包括<a href="https://github.com/google/sanitizers" target="_blank" rel="noopener">Google的C++ sanitizer</a>和<a href="http://blogs.msdn.com/b/vcblog/archive/2014/12/08/visual-studio-2015-preview-work-in-progress-security-feature.aspx" target="_blank" rel="noopener">微软的“/guard”功能</a>。 而采用基于语言的安全的不错的例子则包括C#、Java、大多数函数式语言和Go等。但是，我们也看到这种方式的不完备，因为C#有“unsafe”关键字允许违反安全性的不安全区域的存在。</p><!-- So, anyway, how do you build an operating system, whose central purpose is tocontrol hardware resources, buffers, services and applications running inparallel, and so on, all of which are pretty damn unsafe things, using a safeprogramming environment?  Great question. --><p>那么，到底应该如何构建一个操作系统，其核心目的是，使用安全的编程环境来控制并行运行的硬件资源、缓冲区、服务和应用程序等所有可能造成不安全后果的例程？ 这是一个不错的问题。</p><!-- The answer is surprisingly simple: layers. --><p>答案非常简单：分层。</p><!-- There was of course _some_ unsafe code in the system.  Each unsafe component wasresponsible for "encapsulating" its unsafety.  This is easier said than done,and was certainly the hardest part of the system to get right.  Which is whythis so-called [trusted computing base](https://en.wikipedia.org/wiki/Trusted_computing_base) (TCB) always remained assmall as we could make it.  Nothing above the OS kernel and runtime was meant toemploy unsafe code, and very little above the microkernel did.  Yes, our OSscheduler and memory manager was written in safe code.  And all application-level and library code was most certainly 100% safe, like our entire web browser. --><p>当然，系统中会有<em>一些</em>不安全的代码，而每个不安全组件都需要负责“安全封装”它自身的不安全性。 这说起来容易做起来难，而且肯定是系统中最难实现的部分。 这就是为什么所谓的<a href="https://en.wikipedia.org/wiki/Trusted_computing_base" target="_blank" rel="noopener">可信计算基（TCB）</a>要始终保持尽可能小的原因。因此，不安全代码不应存在于操作系统内核和运行时之上的任何部件中，而应存在于微内核之上的极少部分。 没错，Midori的操作系统调度程序和内存管理器皆是用由安全代码编写而成的，并且所有应用级和库代码也肯定是100%安全，就像我们的Web浏览器一样安全。</p><!-- One interesting aspect of relying on type safety was that [your compiler](https://en.wikipedia.org/wiki/Bartok_(compiler)) becomes part of your TCB.Although our compiler was written in safe code, it emitted instructions for theprocessor to execute.  The risk here can be remedied slightly by techniques likeproof-carrying code and [typed assembly language](https://en.wikipedia.org/wiki/Typed_assembly_language) (TAL).  Added runtimechecks, a la software fault isolation, can also lessen some of this risk. --><p>有趣的是，依靠类型安全的方法中，<a href="http://t.cn/EVXZ53S" target="_blank" rel="noopener">编译器</a>将成为TCB的一部分，因为虽然编译器是用安全代码编写的，但它仍需输出指令供处理器直接执行。 但这里产生风险可以通过携带证明的代码和<a href="https://en.wikipedia.org/wiki/Typed_assembly_language" target="_blank" rel="noopener">类型汇编语言（TAL）</a>等技术稍作补救，另外，添加运行时检查和软件故障隔离等方法，也可以减少部分风险。</p><!-- A nice consequence of our approach was that the system was built upon itself.This was a key principle we took to an extreme.  I covered it a bit [in a priorarticle](http://joeduffyblog.com/2014/09/10/software-leadership-7-codevelopment-is-a-powerful-thing/).But when you've got an OS kernel, filesystem, networking stack, device drivers,UI and graphics stack, web browser, web server, multimedia stack, ..., and eventhe compiler itself, all written in your safe programming model, you can bepretty sure it will work for mostly anything you can throw at it. --><p>我们的分层方法的一个很好的后果是系统构建在自己的基础之上，这将我们的关键原则发挥到了极致，而在我<a href="http://joeduffyblog.com/2014/09/10/software-leadership-7-codevelopment-is-a-powerful-thing/" target="_blank" rel="noopener">前面的一篇文章</a>中对此也进行了一些介绍。 当你的操作系统内核、文件系统、网络栈、设备驱动程序、用户界面、图形堆栈、网页浏览器、网络服务器和多媒体堆栈……甚至编译器本身都是用你的设计的安全编程模型编写而成，那么可以非常肯定这种安全模型适用于你系统中的大部分的部件。</p><!-- You may be wondering what all this safety cost.  Simply put, there are thingsyou can't do without pointer arithmetic, data races, and the like.  Much ofwhat we did went into minimizing these added costs.  And I'm happy to say, in theend, we did end up with a competitive system.  Building the system on itself waskey to keeping us honest.  It turns out architectural decisions like no blockingIO, lightweight processes, fine grained concurrency, asynchronous messagepassing, and more, far outweighed the "minor" costs incurred by requiring safetyup and down the stack. --><p>所以，你可能考虑所有这些安全性带来的开销有多大。简单的说，系统中总会存在那些如果没有指针运算和数据竞争等不安全操作就无法完成的例程。而我们所做的大部分工作都是为了尽量减少这些增加的开销。 我可以很高兴地告诉你，我们最终得到了一个具有竞争力的系统，在自身的基础上构建系统是保持这种竞争力的关键。事实证明，诸如无阻塞I/O、轻量级进程、细粒度并发、异步消息传递等架构决策带来的好处，远远超过了需要在全部堆栈保持安全性所带来的“较小”的开销。</p><!-- For example, we did have certain types that were just buckets of bits.  But thesewere just [PODs](https://en.wikipedia.org/wiki/Passive_data_structure).  Thisallowed us to parse bits out of byte buffers -- and casting to and fro betweendifferent wholly differnt "types" -- efficiently and without loss of safety.We had a first class slicing type that permit us to form safe, checked windowsover buffers, and unify the way we accessed all memory in the system([the slice type](https://github.com/joeduffy/slice.net) we're adding to .NETwas inspired by this). --><p>例如，我们确实有某些类型只是存放数据的桶结构，但这些只是<a href="https://en.wikipedia.org/wiki/Passive_data_structure" target="_blank" rel="noopener">被动数据结构（POD）</a>。 它们的存在使我们能够以高效且不会损失安全性的方式，从字节缓冲区中解析数据，以及在完全不同的类型之间来回转换。 例如，我们有作为一等公民的切片（slice）类型，它允许在缓冲区上形成安全和校检的访问窗口，并形成统一访问所有系统内存的安全方式（我们正在添加至.NET的<a href="https://github.com/joeduffy/slice.net" target="_blank" rel="noopener">切片类型</a>的灵感也来源于此）。</p><!-- You might also wonder about the [RTTI](https://en.wikipedia.org/wiki/Run-time_type_information) overheads required tosupport type safety.  Well, thanks to PODs, and proper support for [discriminated unions](https://en.wikipedia.org/wiki/Tagged_union), we didn'tneed to cast things all that much.  And anywhere we did, the compiler optimizedthe hell out of the structures.  The net result wasn't much more than what atypical C++ program has just to support virtual dispatch (never mind casting). --><p>你可能还会考虑支持类型安全所需的<a href="https://en.wikipedia.org/wiki/Run-time_type_information" target="_blank" rel="noopener">运行时类型信息（RTTI）</a>的开销有多大。 好吧，多亏了POD，以及对<a href="https://en.wikipedia.org/wiki/Tagged_union" target="_blank" rel="noopener">可辨识联合</a>合适的支持，使得我们无需进行太多的类型转换。而即使在我们进行类型任何地方，编译器都对结构进行了优化，因此最终结果并不比只支持虚拟调度（virtual dispatch）的典型C++程序要差（所以不要担心类型转换和它的开销）。</p><!-- A general theme that ran throughout this journey is that compiler technology hasadvanced tremeodusly in the past 20 years.  In most cases, safety overheads canbe optimized very aggressively.  That's not to say they drop to zero, but we wereable to get them within the noise for most interesting programs.  And --surprisingly -- we found plenty of cases where safety _enabled_ new, noveloptimization techniques!  For example, having immutability in the type systempermit us to share pages more aggressively across multiple heaps and programs;teaching the optimizer about [contracts](https://en.wikipedia.org/wiki/Design_by_contract) let us more aggressively hoisttype safety checks; and so on. --><p>贯穿于整个过程的通用主题是，编译器技术在过去20年中已经发展得非常好。 在大多数情况下，安全性带来的额外开销可在很大程度上被优化掉，虽然这并不表示开销可以降到零，但在大多数程序中，我们能够让其控制在可接受的范围内。 并且，<em>令人惊讶的是</em>，我们发现了大量由于安全性所导致的新奇的优化方法！例如，类型系统中的不变性使得我们可以在多个堆和程序之间采用更积极的方式共享页面，以及采用<a href="https://en.wikipedia.org/wiki/Design_by_contract" target="_blank" rel="noopener">合约</a>的方式使优化器更激进地提升类型安全检查等。</p><!--Another controversial area was concurrency safety.  Especially given that thestart of the project overlapped with the heady multicore days of the late 2000s.What, no parallelism, you ask? --><p>而另一个有争议的方面是并发安全，特别是考虑到Midori项目开始时正好与2000年代后期令人兴奋的多核的发展相重叠。 你说什么，Midori没有并行性？</p><!-- Note that I didn't say we banned concurrency altogether, just that we banned_unsafe_ concurrency.  First, most concurrency in the system was expressed usingmessage passing between lightweight [software isolated processes](http://research.microsoft.com/apps/pubs/default.aspx?id=71996).  Second, withina process, we formalized the rules of safe shared memory parallelism, enforcedthrough type system and programming model rules.  The net result was that youcouldn't write a shared memory data race. --><p>需要注意的使，我未说我们完全禁止并发，只是我们禁止了<em>不安全的</em>并发。 首先，系统中的大多数并发采用在轻量级的<a href="http://research.microsoft.com/apps/pubs/default.aspx?id=71996" target="_blank" rel="noopener">软件隔离进程</a>之间消息传递的方式进行表达。 其次，在一个进程中，我们通过强制类型系统和编程模型规则的方式来形式化安全共享内存的并行规则，其最终结果很自然地就禁止了编写具有共享内存数据冒险的代码。</p><!-- They key insight driving the formalism here was that no two "threads" sharing anaddress space were permitted to see the same object as mutable at the same time.Many could read from the same memory at once, and one could write, but multiplecould not write at once.  A few details were discussed in [our OOPSLA paper](http://research.microsoft.com/apps/pubs/default.aspx?id=170528), and Rustachieved a similar outcome [and documented it nicely](http://blog.rust-lang.org/2015/04/10/Fearless-Concurrency.html).  It worked wellenough for many uses of fine-grained parallelism, like our multimedia stack. --><p>这里推动形式化的关键内因是，不允许共享地址空间的两个“线程”同时看到同一个对象是可变的，也就是说多个线程可同时读取同一内存中，或者仅单个线程可写入，但不允许多线程同时写入。 <a href="http://research.microsoft.com/apps/pubs/default.aspx?id=170528" target="_blank" rel="noopener">我们的OOPSLA论文</a>讨论了一些细节，Rust也取得了类似的成果<a href="https://blog.rust-lang.org/2015/04/10/Fearless-Concurrency.html" target="_blank" rel="noopener">并进行了很好地描述</a>。 在多种细粒度并行的场景中，例如Midori的多媒体栈中，它都工作的很好。</p><!-- Since Midori, I've been working to bring some of our key lessons about how toachieve simultaneous safety and performance to both .NET and C++.  Perhaps themost visible artifact are the [safety profiles](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#S-profile)we recently launched as part of the C++ Core Guidelines effort.  I expect moreto show up in C# 7 and the C# AOT work we're doing right now as we take .NETcross-platform.  Midori was greenfield, whereas these environments requiredelicate compromises, which has been fun, but slowed down some of the transferof these ideas into production.  I'm happy to finally start seeing some of itbearing fruit. --><p>在Midori以后，我一直致力于提供对于.Net以及C++而言如何同时实现安全性和性能的重要经验。其中最显著的产品是我们最近作为C++核心指南的一部分推出的<a href="https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#S-profile" target="_blank" rel="noopener">安全配置（safety profiles）</a>。 因为我们将.Net变成了跨平台的项目，所以我希望能在C# 7和我们正在进行的C# AOT的项目中展示更多内容。与Midori宛如真空的安全环境不同，其他这些环境（.Net和C++）则需要进行微妙的妥协，这虽然很有趣，但也减缓我们将这些想法实现到产品中的步伐，但我也很高兴开始看到在Midori的安全性上的工作终于接出了果实。</p><!-- The combination of memory, type, and concurrency safety gave us a powerfulfoundation to stand on.  Most of all, it delivered a heightened level ofdeveloper productivity and let us move fast.  The extremely costly bufferoverflows, data races, deadlocks, and so on, simply did not happen.Someday all operating systems will be written this way. --><p>内存安全、类型安全和并发安全的结合为我们的开发提供了强大的基础。 最重要的是，它提高了开发人员的工作效率，使我们能够快速演进，因为导致严重后果的缓冲区溢出、数据冒险和死锁等安全漏洞在Midori中根本就不会发生。因此，（我相信）总有一天，所有的操作系统都会采用这种方式编写。</p><!--In the next article in this series, we'll look at how this foundational safetylet us deliver a [capability-based security model](https://en.wikipedia.org/wiki/Capability-based_security) that was first class inthe programming model and type system, and brought the same "by-construction"solution to eliminating [ambient authority](https://en.wikipedia.org/wiki/Ambient_authority) and enabling the [principle of least privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege)everywhere, by default, in a big way.  See you next time. --><p>在本系列的下一篇文章中，我们将看到这种基本的安全性是如何提供了在编程模型和类型系统中担任一等公民的<a href="https://en.wikipedia.org/wiki/Capability-based_security" target="_blank" rel="noopener">基于权能的安全模型（capability-based security model）</a>的，以及“从构造”上消除<a href="https://en.wikipedia.org/wiki/Ambient_authority" target="_blank" rel="noopener">环境权限（ambient authority）</a>和默认在所有地方启用<a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege" target="_blank" rel="noopener">最小权限原则</a>。我们下次再见！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- 
[Midori](/2015/11/03/blogging-about-midori) was built on a foundation of three
kinds of safety: type, memory, and concurrency-safety. 
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="操作系统" scheme="https://blog.zhangpf.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="安全" scheme="https://blog.zhangpf.com/tags/%E5%AE%89%E5%85%A8/"/>
    
      <category term="Midori" scheme="https://blog.zhangpf.com/tags/Midori/"/>
    
      <category term="翻译" scheme="https://blog.zhangpf.com/tags/%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>Midori博客系列翻译（0）——介绍</title>
    <link href="https://blog.zhangpf.com/2018/10/20/midori/0-blogging-about-midori/"/>
    <id>https://blog.zhangpf.com/2018/10/20/midori/0-blogging-about-midori/</id>
    <published>2018-10-20T06:55:07.000Z</published>
    <updated>2020-01-29T02:19:38.924Z</updated>
    
    <content type="html"><![CDATA[<!-- Enough time has passed that I feel safe blogging about my prior project here atMicrosoft, "Midori."  In the months to come, I'll publish a dozen-or-so articlescovering the most interesting aspects of this project, and my key take-aways. --><p>已经离开了足够长的时间，因此我觉得在博客中谈论以前在微软的“Midori”项目是安全的。在接下来的几个月里，我将发表十余篇文章，以涵盖这个项目最有趣的方面，以及我认为的主要教训。</p><!-- Midori was a research/incubation project to explore ways of innovatingthroughout Microsoft's software stack.  This spanned all aspects, including theprogramming language, compilers, OS, its services, applications, and the overallprogramming models.  We had a heavy bias towards cloud, concurrency, and safety.The project included novel "cultural" approaches too, being 100% developers andvery code-focused, looking more like the Microsoft of today and hopefullytomorrow, than it did the Microsoft of 8 years ago when the project began. --><p>Midori是一个研究/孵化项目，它的目标是在整个微软软件栈上的探索可能的创新。其涵盖了包括编程语言、编译器、操作系统及其服务、应用程序和整体编程模型在内的所有方面。在该项目中，我们侧重于对云计算、并发和安全的考虑。同时，该项目包含了新颖的“文化”方法——全员开发以及非常专注于代码，因此它看起来更像是今天微软的样子，以及对微软未来所期望的模样，而不是8年前项目开始时的微软。<!-- I worked on Midori from 2009 until we transitioned the teams to their respectivenew homes during 2012-2014.  I led the groups focusing on the developerexperience: language, compilers, core frameworks, concurrency models, andIDEs/tools.  And I wrote lots of code the whole time. -->我于2009年开始在Midori项目工作，直到2012至2014年期间，团队中的各个成员相继离开而去了各自新团队。在这期间，我带领团队专注于面向开发者的体验，这包括编程语言、编译器、核心框架、并发模型和IDE工具等，同时也写了不少的代码。</p><!-- Although we started with C# and .NET, we were forced to radically depart in thename of security, reliability, and performance.  Now, I am helping to bring manyof those lessons learned back to the shipping products including, perhapssurprisingly, C++.  Most of my blog entries will focus on the key lessons thatwe're now trying to apply back to the products, like asynchrony everywhere,zero-copy IO, dispelling the false dichotomy between safety and performance,capability-based security, safe concurrency, establishing a culture of technicaldebate, and more. --><p>虽然起初我们从C#和.NET技术开始，但在离开时Midori最终走向了对安全性、可靠性和性能的追求。而现在，我正在帮助将Midori的许多经验教训带回到交付的产品中，这也包括可能会令人惊讶的C++。因此，我的大多数博客文章都将重点关注于那些我们正尝试用于改善现有其他产品的关键教训，例如，无处不在的异步、零拷贝I/O、对安全和性能不可调和性的消除、基于功能的（capability-based）安全、安全并发、建立关于技术的辩论文化等。</p><!-- I'll be the first to admit, none of us knew how Midori would turn out.  That'soften the case with research.  My biggest regret is that we didn't OSS it fromthe start, where the meritocracy of the Internet could judge its piecesappropriately.  As with all big corporations, decisions around the destiny ofMidori's core technology weren't entirely technology-driven, and sadly, not evenentirely business-driven.  But therein lies some important lessons too.  Mysecond biggest regret is that we didn't publish more papers.  This blog seriesmay help to recitify some of this. --><p>我得首先承认，于初大家都不知道Midori会怎么样发展，因为研究通常就是这样。而我最大的遗憾是，从一开始就没有将它开源，因为开源可以很好地使互联网的各类优秀开发者对其进行评判。与所有其他大公司一样，围绕Midori核心技术命运的决策并非完全由技术驱动，并且可悲的是，甚至不完全由业务所驱动，于此也有一些重要的教训。我的第二大遗憾是我们没有发表更多关于Midori的论文，但该系列博客可能有助于重新阐述其中的部分内容。</p><!-- I shall update this list as new articles are published: --><p>在我发布新文章时，也将同时更新此列表：</p><!-- 1. [A Tale of Three Safeties](/2015/11/03/a-tale-of-three-safeties/)2. [Objects as Secure Capabilities](/2015/11/10/objects-as-secure-capabilities/)3. [Asynchronous Everything](/2015/11/19/asynchronous-everything/)4. [Safe Native Code](/2015/12/19/safe-native-code)5. [The Error Model](/2016/02/07/the-error-model)6. [Performance Culture](/2016/04/10/performance-culture)7. [15 Years of Concurrency](/2016/11/30/15-years-of-concurrency/) --><ol><li><a href="/2018/10/24/midori/1-a-tale-of-three-safeties/">三类安全性的故事</a></li><li><a href="/2018/11/18/midori/2-objects-as-secure-capabilities/">对象即安全权能</a></li><li><a href="/2018/11/25/midori/3-asynchronous-everything/">一切皆异步</a></li><li><a href="/2019/02/17/midori/4-safe-native-code/">安全的原生代码</a></li><li><a href="/2019/03/09/midori/5-the-error-model/">错误处理模型</a></li><li><a href="/2019/03/13/midori/6-performance-culture/">性能文化</a></li><li>关于并发的15年</li></ol><!-- Midori was a fascinating journey, and the most fun I've had in my careerto-date.  I look forward to sharing some of that journey with you.--><p>Midori是一段迷人的旅程，也是我职业生涯中迄今为止最有趣的事情，因此期待与您分享这一旅程。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- 
Enough time has passed that I feel safe blogging about my prior project here at
Microsoft, &quot;Midori.&quot;  In the months to come, I&#39;ll publ
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="操作系统" scheme="https://blog.zhangpf.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="Midori" scheme="https://blog.zhangpf.com/tags/Midori/"/>
    
      <category term="翻译" scheme="https://blog.zhangpf.com/tags/%E7%BF%BB%E8%AF%91/"/>
    
      <category term="微软" scheme="https://blog.zhangpf.com/tags/%E5%BE%AE%E8%BD%AF/"/>
    
  </entry>
  
  <entry>
    <title>微内核的回归</title>
    <link href="https://blog.zhangpf.com/2018/09/04/The-raise-up-of-microkernel/"/>
    <id>https://blog.zhangpf.com/2018/09/04/The-raise-up-of-microkernel/</id>
    <published>2018-09-04T08:11:48.000Z</published>
    <updated>2020-01-29T02:19:30.129Z</updated>
    
    <content type="html"><![CDATA[<p><em>（本文写于2018年初，但由于原博客不再维护，原始的Markdown文件也已丢失，于是我重新整理并添加了一些新的内容，再次发布到本博客中）</em></p><p>最近调研了操作系统近几年的发展，特别是<a href="https://zh.wikipedia.org/wiki/%E5%BE%AE%E5%85%A7%E6%A0%B8" target="_blank" rel="noopener">微内核</a>操作系统的发展，我越来越清晰地归纳出一个结论，<strong>如果操作系统未来还能有突破性发展的话，那也许是发生在微内核上</strong>。</p><p>首先要解释一下微内核和与之相对的<a href="https://zh.wikipedia.org/wiki/%E6%95%B4%E5%A1%8A%E6%80%A7%E6%A0%B8%E5%BF%83" target="_blank" rel="noopener">宏内核</a>是什么，以及两者的优缺点。微内核是指操作系统的最底层是一个包含最基本功能的kernel（内核），这个kernel通常只负责最基本的最底层的任务，如上下文切换、中断处理、进程间通信（IPC）和时钟处理等，而其他的系统任务，如硬件驱动，文件系统和内存管理都以用户态进程（即ring3）的形式运行，并且相互之间通过IPC进行通信；宏内核则与之相反，所有的系统任务均在内核态（ring0）处理，系统模块之间通过函数调用方式进行交互。这两个不同架构模式的操作系统概念自提出已经过去了三十年（最早的微内核系统是Mach，于1985年被提出，之前的操作系统都是宏内核，如Unix），工业界和学界普遍认为微内核的优势在于kernel很轻（通常C代码在10000行左右），因此攻击面和代码出错的可能性更低，而且大部分任务是以进程的方式运行，一旦出错只会影响到这个进程本身，稳定性更强，而宏内核与之相反，一个驱动的微小错误很容易导致整个系统崩溃。但是微内核最被诟病的是它的性能，因为一个简单的系统调用可能会涉及到多个系统任务，以及大量的IPC和相对应的上下文切换，这样的开销是巨大的，而在宏内核中，一次系统调用只需要两次上下文切换。关于宏内核和微内核有著名的“<a href="https://www.oreilly.com/openbook/opensources/book/appa.html" target="_blank" rel="noopener">Linus Tanenbaum debate</a>”，可以算的上操作系统设计和研究最佳的材料，值得反复阅读。</p><p>由于历史和性能的原因，我们日常接触到的操作系统，90%以上都是宏内核（包括类Unix和Windows，Mac OS X最初是基于微内核，但是最后也加了很多宏内核的方式），微内核一度仅仅局限于研究目的，除了QNX这种车载娱乐等专业领域上的操作系统之外。但是最近几年的微内核的发展以及不断涌现的新操作系统，可以说是给人耳目一新的感觉，这里简单介绍三个项目：</p><ol><li><p><a href="https://sel4.systems/" target="_blank" rel="noopener">seL4</a>： 提到seL4，不得不提到<a href="https://zh.wikipedia.org/wiki/L4%E5%BE%AE%E5%86%85%E6%A0%B8%E7%B3%BB%E5%88%97" target="_blank" rel="noopener">L4</a>微内核操作系统家族。因为Mach的IPC简直就是性能灾难，所以Jochen Liedtke提出了L3和后续的L4结构，对IPC的性能改进很大，甚至相比较Unix都有很大的性能优势。而seL4是在L4操作系统上，运用形式化验证（formal verfication）的方法来证明kernel在模型上的正确性。在其SOSP09的论文中宣称这是一个”bug free”的内核，它付出的代价也是巨大的，总共花了11person year的工作量进行形式化验证。但是这样的付出对于一个严肃和safety-critical的系统是值得的，而显然这对于宏内核是完全不可能的事情。</p></li><li><p><a href="http://fuchsia.googlesource.com/" target="_blank" rel="noopener">Fuchsia</a>：这是Google在2016年公开的一个项目，尽管Google一直没有对外公布它的目的和计划，但是它普遍被认为将用于取代Android系统，以求彻底解决Android及其底层的Linux kernel的各种历史问题。Fuchsia的底层Zircon本身是基于lk，而lk原本是一个嵌入式微控制器系统，最具有最基本的任务调度，同步原语等功能。但是Zircon在lk基础上构建各种系统服务，而Fuchsia又在Zircon上构建了图像化服务和其他应用程序，从最近的[测试视频][<a href="https://www.youtube.com/watch?v=LY9DhA2vt9A" target="_blank" rel="noopener">Fuchsia-demo</a>上看，Fuchsia已经达到了基本可用的状态。</p></li><li><p><a href="https://www.redox-os.org/" target="_blank" rel="noopener">Redox OS</a>：这是一个从2015年开始的开源微内核操作系统项目，它最大的特点在于完全使用rust（和少量必需的汇编）作为内核开发的语言，显示了rust在保持强大的表述能力的同时，对底层资源的操作也能灵活自如。但是让我最惊讶不已的是，Redox在仅两名核心开发人员（加上一些GSoc参与者）的条件下，用了不到三年的时间，已经差不多宣称要实现<a href="https://changelog.com/podcast/280" target="_blank" rel="noopener">self-hosting</a>，并具有比较完整的图形化子系统。这也从某种程度上显示了rust生态的威力。</p></li></ol><p>当再一次从头比较微内核和宏内核的几个关键争论点，我惊讶的发现它们或多或少已经发生了改变，其一是性能问题，硬件和处理器得到了很大的发展，摩尔定律和多核技术使得硬件能力基本处于过剩状态，现在的大部分性能问题可以说是软件造成的（我还清晰地记得本科计算机原理课老师经常要把写软件的人拿出来批判一番……），而且L4在微内核本身的IPC性能已经有了长足的发展，就更没有理由认为微内核比宏内核性能更差。其二，越来越多的新场景，包括IoT，自动驾驶和区块链等，对操作系统的稳定性和可靠性提出了更高的要求，从CVE的<a href="https://www.cvedetails.com/product/47/Linux-Linux-Kernel.html?vendor_id=33" target="_blank" rel="noopener">统计</a>中我们可以看出，Linux内核的漏洞数量基本呈逐年上升的趋势，虽然微内核并不能从根本上解决所有漏洞，但seL4给我们的启示是，通过形式化验证或模型检验的方式可以消除大部分kernel中的bug，而这些方法巨大的工作量，只可能在微内核上才可能被接受。另外，seL4和fuchsia中基于capabilities的权限验证，redox中rust提供的内存安全性等，都是提高系统安全性的有效方式，而这些方式似乎已经很难加入到现有的操作系统。</p><p>至于为什么没有一款真正的微内核操作系统进入大部分人的生活，我想这是因为既有的软件生态和强大的惯性。Linux和Windows在大部分时间里已经just work，我们没有理由也没有可能在现有场景下重复造轮子，更没法从零构建出如此庞大的生态。但是历史的车轮也在向前，新生事物也是在不断批判旧事物的基础上产生的，当系统需求和使用场景发生变化时也将相应的对软件设计提出新的要求。如部署于自动驾驶中枢位置的操作系统，对安全性和稳定性的要求将大于兼容性和灵活性，也行将是微内核发挥自己长处的地方，而我也期待着见证这个历史过程。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;（本文写于2018年初，但由于原博客不再维护，原始的Markdown文件也已丢失，于是我重新整理并添加了一些新的内容，再次发布到本博客中）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;最近调研了操作系统近几年的发展，特别是&lt;a href=&quot;https://zh.wikipedia.or
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="操作系统" scheme="https://blog.zhangpf.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="微内核" scheme="https://blog.zhangpf.com/tags/%E5%BE%AE%E5%86%85%E6%A0%B8/"/>
    
      <category term="安全" scheme="https://blog.zhangpf.com/tags/%E5%AE%89%E5%85%A8/"/>
    
  </entry>
  
  <entry>
    <title>自动驾驶需要什么样的软件平台</title>
    <link href="https://blog.zhangpf.com/2018/09/04/The-software-platform-of-autonomous-vehicle/"/>
    <id>https://blog.zhangpf.com/2018/09/04/The-software-platform-of-autonomous-vehicle/</id>
    <published>2018-09-04T07:52:55.000Z</published>
    <updated>2020-01-29T02:19:30.131Z</updated>
    
    <content type="html"><![CDATA[<p><em>（本文写于2018年初，但由于原博客不再维护，原始的Markdown文件也已丢失，于是我重新整理并添加了一些新的内容，再次发布到本博客中）</em></p><p>从2014年开始，互联网公司、传统车企以及大批创业公司纷纷进入自动驾驶领域。因为自动驾驶的5级中（L1-L5从易到难），实现L4/L5级别的难度不是一家公司可以搞得定的，所以在我看来，他们的切入点可以分为两个不同层次，传统车企和少部分创业公司，以及大量的传感器制造商走的是底层的硬件制造和设计路线，即如何改进（或重新设计）整车（或部分硬件模块）以适应驾驶无人化，因为他们具有多年的硬件经验，所以这是他们的强项；互联网公司和大部分创业公司，则走的是高层感知决策规划的路线，这些玩家要么手持大量数据，要么聚集大量算法（以当前最热门的深度学习和人工智能为代表的）人才。当然还有两种路线通吃的玩家，如特斯拉，自己不但设计和制造电动汽车还在此基础了开发了autopilot辅助驾驶套件，但是，特斯拉当前提供的确实辅助驾驶的L2技术，因此不在本文的考虑之列。</p><p>虽然自动驾驶是软硬件结合的平台，而且呈现出高度专业化的特点：即使有一天自动驾驶真的成为现实，也很难说需要成百上千万的自动驾驶开发人才，所以这使得它和传统服务器、移动互联网开发有很大的不同。但是我认为它依然遵循经典软件架构的基本模式，即：从计算机发展的历史上看，经典的系统架构，大体上都可以在垂直方向上分成三个不同层次，对于桌面和服务器系统，它们是硬件、系统软件和应用软件；对于移动和手持设备，它们是硬件、移动操作系统（Android，iOS等）和App；甚至对于云计算，我们都可以将其分为服务器、IaaS+PaaS（虽然IaaS和PaaS最初是上下两层，但从最近几年的云服务提供商发展趋势上看，两者的界限已经模糊）和SaaS。在自动驾驶的三层中，上下两层的玩家很多，中间软件平台层当前却不怎么欣欣向荣（其实也有，我未来会从技术角度一一点评），但是软件平台确也是不可或缺的一层。因此，在本文我想重点谈谈在自动驾驶领域，我们到底需要怎样的软件平台。</p><p>为了回答这个问题，我先提出另一个问题，对于自动驾驶，我最关心的是什么？毫无疑问，是它的安全性。一辆以超过100km/h的速度行驶在高速公路上的无人车，即使它的机动性能再好，内饰再豪华，娱乐系统再丰富，但是它无法保证安全性，我想也是没有人敢乘坐，因为对安全感的追求已经早已融入到每种动物的基因里，而人类也毫无例外。同时，我们也可以从另外一些侧面说明问题：想想Waymo号称已经路测500万公里，但是为何依然不能落地无人租车服务；打开任何一个自动驾驶的媒体公众号，稍微统计一下里面和安全性有关的文章数量，也许你就能明白。因此从安全性，这个第一性原理出发，不难推导出自动驾驶对软件平台的需求方向。虽说安全保证不能缺少在来自上下游的整车硬件和感知规划算法的配合，但本文中我们仅具体聚焦到软件平台上，我这里将其具体归纳为以下四个方面：</p><ol><li><p><strong>可靠性</strong>：当我们坐在一辆没有司机的汽车中，等于说我们将生命将完全托付给了它。传统软件中的bug可能只会让我们不开心，但是控制汽车的软件中的bug后果真的会要了我们的命。因此无人车上对软件的bug容忍度基本趋近于零，传统软件设计和测试中的鲁棒性理论将在这里发挥巨大的作用，SMT、形式化验证（Formal verification）、模型验证（Model checker），覆盖性测试等方法都在此将发挥作用。</p></li><li><p><strong>实时性</strong>：来外部的突发事件（如突然钻出的行人）需要被平台及时捕获并得到处理，否则也是及其危险的，这里就需要软件在设计上体现其实时处理的特征。毫无疑问，实时系统（Real-time）对于自动驾驶平台也是极端重要的，而且这里的实时不是指面向用户流畅体验的软实时，而指的是“all or nothing”的硬实时，因此在操作系统层就不能使用Linux这样的无实时保证的操作系统。另外黑莓的QNX在智能手机上未能发展壮大，而自动驾驶对于它来讲可能是更适合的场景（最近百度和黑莓关于QNX的合作也印证了这一点）。</p></li><li><p><strong>抗攻击性</strong>：自动驾驶软件平台本质上还是软件系统，一旦联网后也同样面临如何防御来自外部的恶意入侵。恶意入侵在无人车领域也不是什么新鲜事，而特斯拉已经被曝出能够被黑客远程启动。如何防漏洞，在我看来是一项系统工程，而且必须贯穿设计、实现、调试和维护等各个环节。我不是这个领域的专家，不过也会不断学习这方面的内容，并且分享我的心得和观点。</p></li><li><p><strong>互联性</strong>：这里我将自动驾驶软件平台的互联互通分为以下三个角度:</p><ul><li>软件平台和远程服务的互联：平台和远程服务，特别是云计算和大数据平台，已经被大家所熟知，具有代表性是利用远程的地图数据实现路径规划，和接入租车平台参与调度等，这需要平台提供完整的网络协议栈以支持，以及高效的网络处理能力，特别对5G网络和NB-IoT等物联网协议的支持，以后应该是平台支持的重点。另外，选择如DDS等有实时性保证的协议和标准，在其他的无人车平台的实践上已经有所探索，而这也是形成端到端的实时的必须的部分。</li><li>无人车和其他物理实体的互联；这其实是物联网的典型应用。无人车为了和其他物理实体，如道路、收费站和其他车辆，进行高效地交互，必然需要去中心化地本地通信机制和协议。顺便说一句，利用物联网的技术，很多传统行驶场景中的难题都变得迎刃而解，以后有时间我将专门写一篇文章探讨这些问题。同时，利用区块链技术进行去中心化的身份认证和共识，也是解决信任安全的有效手段之一。</li><li>软件平台内部各个模块的互联：现代软件平台必然是采用模块化设计，一个模块的信号消息（摄像头感知）如何能被另一模块（刹车制动）有效接受并得到及时正确的处理。虽说传统操作系统中这些问题已经研究很多（如IPC，message passing等），但是都较少涉及到消息传递的延迟（而较关注于吞吐量），但延迟才是无人车关注的重点，这些都是需要考虑的新问题。</li></ul></li></ol><p>当然，除了安全性的考虑，其他的一些软件平台的必要特性，诸如对硬件的全面支持、良好的模块设计、稳定且一致的文档和API、成熟的工具链和开发环境以及软件生态等， 都是题中应有之义，也都是未来需要逐一考量的方面。但是除了一些专业领域，如航天、飞行和军事等，还没有哪一个通用软件平台应该将安全性提升到如此高度，而这将极大地改变该软件平台的重心。</p><p>虽然最近不断有媒体在宣传某某公司将在一年，甚至几个月之内提供无人驾驶服务，但是我依然认同李飞飞教授在2015年的一个观点，L5的自动驾驶仅仅有望在10年内成为现实。因此还有很长的路要走，这反而对于定位于长远未来的软件平台设计来讲是一件好事，提供了充足的时间用来思考、演化、验证和试错。而对于传统软件架构设计、开发的方法论来说，这也是一次自我进化的机会。未来我也将不定期地更新博客，探讨我对自动驾驶软件平台设计的思考，以及它和其他技术、概念融合的一些想法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;（本文写于2018年初，但由于原博客不再维护，原始的Markdown文件也已丢失，于是我重新整理并添加了一些新的内容，再次发布到本博客中）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;从2014年开始，互联网公司、传统车企以及大批创业公司纷纷进入自动驾驶领域。因为自动驾驶的5级中（L1
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="操作系统" scheme="https://blog.zhangpf.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="安全" scheme="https://blog.zhangpf.com/tags/%E5%AE%89%E5%85%A8/"/>
    
      <category term="自动驾驶" scheme="https://blog.zhangpf.com/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"/>
    
  </entry>
  
  <entry>
    <title>使用Windows的UMS(User-Mode Scheduling)实现轻量级线程</title>
    <link href="https://blog.zhangpf.com/2018/08/27/using-Windows-UMS-for-lightweight-threading-model/"/>
    <id>https://blog.zhangpf.com/2018/08/27/using-Windows-UMS-for-lightweight-threading-model/</id>
    <published>2018-08-27T13:04:23.000Z</published>
    <updated>2020-01-29T02:19:30.149Z</updated>
    
    <content type="html"><![CDATA[<p>前几天在看关于Rust取消M:N线程模型背后的理性选择时候，看到<a href="https://github.com/thestinger" target="_blank" rel="noopener">Daniel Micay</a>的一篇帖子中提到了Windows的UMS功能，了解之后s觉得有点意思。所以就花了几天研究一下，分析了它的优缺点和基本性能情况，于是总结成此文。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://docs.microsoft.com/en-us/windows/desktop/procthread/user-mode-scheduling" target="_blank" rel="noopener">UMS (User-Mode Scheduling)</a>是微软在其Windows 7及其以后版本的64位操作系统中添加的一个功能，其目的是支持应用对申请的线程进行自定义管理，以此方式实现类似于轻量级线程的高效并发。</p><p>虽然在云计算领域，Linux成了事实上的基础操作系统平台，但没想到却是微软首先在其自家系统上实现了此功能，Google的开发者在Linux尝试了<a href="https://blog.linuxplumbersconf.org/2013/ocw/system/presentations/1653/original/LPC%20-%20User%20Threading.pdf" target="_blank" rel="noopener">类似的实现</a>，不过据我所了解，到目前为止都尚未进入Linux的主分支。</p><p>我们都知道，根据用户空间和内核空间的映射关系不同，线程模型可以分为以下三种：</p><ol><li>1:1，即内核态线程：用户空间中的不同线程，分别对应到内核中的一个线程。所有的线程上下文都是由内核来管理，并且所有线程状态的改变，包括调度、I/O阻塞、page fault和信号事件等，都需要进行一次“上下文切换(context switch)”到内核空间中才能处理。Linux和Windows原生线程都是这种类型，它的优点是模型简单，而且能够透明地利用多核(multicore)进行并行处理，缺点是应对大规模的线程管理和并发乏力，因为大量的开销都花在了上下文切换上。</li><li><p>N:1，即用户态线程：用户空间中的多个线程，都对应于内核的用一上下文(context)，线程的切换不再需要进入到内核处理，可以直接由用户态的runtime进行管理，在多种编程语言（如Python，PHP等）中又称为“绿色线程(green thread)”。与内核态线程相反，这类线程的优点是可以支持大量并发，但是却没法直接扩展到多核上进行处理。另外，如果有某个线程需要I/O操作，或因缺页中断而被阻塞，所有的用户态线程都将一同被阻塞。所以，通常的语言级虚拟机都对此情况进行补救措施，例如使用专门的线程进行I/O操作，以及使用mmap等系统调用避免意外的缺页中断。</p></li><li><p>N:M，即混合线程：N个不同的用户态线程对应到M个内核线程上。这种模型是介于上述两种模型之间，看似可以兼顾两者的优点，实际上却产生了更多的问题。首先是模型变得复杂，不仅要考虑单个内核线程上的用户态线程的执行同步情况，而且还要考虑用户态线程可能在多个内核线程上的调度和同步，并且实际上并没有消除用户态线程的阻塞问题。所以在现实中使用混合模型的系统不是很多，比较典型的有golang和<a href="https://ghc.haskell.org/trac/ghc/wiki/LightweightConcurrency" target="_blank" rel="noopener">Haskell</a>。</p></li></ol><p>另外，一个比较容易想到的消除I/O同步阻塞的解决方案是使用异步IO，即线程不等待I/O处理子程序的完成便返回，事后再通过其他方式进行确认。不过这么好的方法怎么不去使用呢？Daniel Micay在[帖子][rust-thread]里提到，其原因有两点：</p><ol><li>历史原因，大多数的依赖库都是采用同步阻塞I/O，并利用原生内核线程进行并发的方式，在它们的基础上还无法简单地进行异步的改造；</li><li>在操作系统发展的过程中，CPU的性能也在不断的改进，其中就包括上下文切换的性能，在该帖子中提到的他作的一个对比实验的结果表明，Rust的协程（我们知道，后来Rust把协程移除了标准库）和OS Thread在创建的开销上其实性能差别不大。</li></ol><p>那么两种在上下文切换时开销差不都，但为什么实际系统中内核态线程在高并发上的性能又比协程差很多？这是因为<strong>大量的开销实际消耗在了位于内核态的线程调度上</strong>。通用操作系统内核为了支持多种不同计算场景（不只是高并发，还有实时计算场景等）下的综合性能，所以也需要较长的时间来调度（考虑是否要实时抢断，是否公平调度等）。但实际上，在云计算中，服务响应模型是比较简单的，通常简单轮询的调度都可以达到目的。</p><p>基于这样的事实，这就导致了Windows操作系统中UMS的产生。UMS中的线程依然是原生线程，但是在切换时，内核无需进入调度流程，直接将CPU控制权交给用户态我们自定义的调度器上。调度器可以根据应用的特点和需求做出更适合且高效的调度策略，选择一个线程执行，并将CPU的执行权交给它。在本文中，我们主要分析了一个基于<a href="https://gist.github.com/pervognsen/8cbde6ea71da8256865e05bf4fcdfa7d" target="_blank" rel="noopener">pervognsen的代码片段</a>的UMS轻量级线程调度的简单实现，并和基于系统调度的普通原生线程，以及Windows的Fiber库实现的轻量级线程，三者进行性能对比测试，从中了解UMS的一些基础性能情况。具体的实现请见<a href="https://github.com/zhangpf/cloud-demos/tree/master/windows-ums" target="_blank" rel="noopener">Github仓库</a>。</p><h2 id="UMS概览"><a href="#UMS概览" class="headerlink" title="UMS概览"></a>UMS概览</h2><h3 id="相关数据结构"><a href="#相关数据结构" class="headerlink" title="相关数据结构"></a>相关数据结构</h3><ol><li><strong>UMS工作线程(Worker thread)</strong>：执行具体计算任务的线程，它们和普通的原生线程几乎具有相同的行为方式，并且进行系统调用，I/O或异常处理等不会阻塞其他线程。</li><li><strong>UMS调度器线程(Scheduler thread)</strong>：UMS调度器线程本质上也是一个普通的线程，它负责对其他工作线程进行调度，执行调度策略，但它的执行时机还是需要由Windows来确定。</li><li><strong>线程上下文(Context)</strong>：UMS线程上下文表示工作线程的状态，用于标识UMS函数调用中的工作线程。它通过调用<code>CreateUmsThreadContext</code>进行创建的。</li><li><strong>完成列表(Completion List)</strong>：完成列表接收已在内核中完成执行并准备在用户模式下运行的UMS工作线程。只有Windows才能将工作线程排队到完成列表中。新的UMS工作线程自动排队到创建线程时指定的完成列表，以前阻塞的工作线程在不再被阻止时也会排队到完成列表。调度器线程可以查询完成列表，从而知道哪些线程已经处于就绪状态，然后再将这些线程加入自己私有的就绪队列中。<!-- % 每个UMS调度器线程与单个完成列表相关联。但是，相同的完成列表可以与任意数量的UMS调度程序线程相关联，并且调度程序线程可以从具有指针的任何完成列表中检索UMS上下文。每个完成列表都有一个关联事件，当系统将一个或多个工作线程排入空列表时，系统会通过该事件发出信号。 `GetUmsCompletionListEvent`函数检索指定完成列表的事件句柄。应用程序可以等待多个完成列表事件以及对应用程序有意义的其他事件。EnterUmsSchedulingMode的调用者指定完成列表和UmsSchedulerProc入口点函数以与UMS调度程序线程关联。完成将调用线程转换为UMS后，系统将调用指定的入口点函数。调度程序入口点函数负责确定指定线程的适当下一个操作。 --></li></ol><h3 id="UMS相关的API"><a href="#UMS相关的API" class="headerlink" title="UMS相关的API"></a>UMS相关的API</h3><ul><li><a href="https://docs.microsoft.com/en-us/windows/desktop/api/WinBase/nf-winbase-enterumsschedulingmode" target="_blank" rel="noopener"><code>EnterUmsSchedulingMode</code></a>：将调用线程转换为UMS的调度器线程。</li><li><a href="https://docs.microsoft.com/en-us/windows/desktop/api/WinBase/nf-winbase-createumsthreadcontext" target="_blank" rel="noopener"><code>CreateUmsThreadContext</code></a>：创建UMS工作线程的上下文。</li><li><a href="https://docs.microsoft.com/en-us/windows/desktop/api/WinBase/nf-winbase-createumscompletionlist" target="_blank" rel="noopener"><code>CreateUmsCompletionList</code></a>：创建UMS完成列表。</li><li><a href="https://docs.microsoft.com/en-us/windows/desktop/api/WinBase/nf-winbase-getumscompletionlistevent" target="_blank" rel="noopener"><code>GetUmsCompletionListEvent</code></a>：检索与指定的UMS完成列表关联的事件的句柄。</li><li><a href="https://docs.microsoft.com/en-us/windows/desktop/api/WinBase/nf-winbase-umsthreadyield" target="_blank" rel="noopener"><code>UmsThreadYield</code></a>：在工作线程中调用，放弃CPU控制权，并触发CPU进入UMS调度器线程。</li><li><a href="https://docs.microsoft.com/en-us/windows/desktop/api/WinBase/nf-winbase-executeumsthread" target="_blank" rel="noopener"><code>ExecuteUmsThread</code></a>：运行指定的UMS工作线程。</li><li><a href="https://docs.microsoft.com/en-us/windows/desktop/api/WinBase/nf-winbase-dequeueumscompletionlistitems" target="_blank" rel="noopener"><code>DequeueUmsCompletionListItems</code></a>：从UMS完成列表中将一个事件移出队列。</li><li><a href="https://docs.microsoft.com/en-us/windows/desktop/api/WinBase/nf-winbase-queryumsthreadinformation" target="_blank" rel="noopener"><code>QueryUmsThreadInformation</code></a>：检索有关指定的UMS工作线程的信息。</li><li><a href="https://docs.microsoft.com/en-us/windows/desktop/api/WinBase/nf-winbase-setumsthreadinformation" target="_blank" rel="noopener"><code>SetUmsThreadInformation</code></a>：为指定的UMS工作线程设置特定于应用程序的上下文信息。</li></ul><h2 id="核心流程的实现"><a href="#核心流程的实现" class="headerlink" title="核心流程的实现"></a>核心流程的实现</h2><h2 id="UMS工作线程和调度器线程"><a href="#UMS工作线程和调度器线程" class="headerlink" title="UMS工作线程和调度器线程"></a>UMS工作线程和调度器线程</h2><p>UMS工作线程的创建需要通过<code>CreateRemoteThreadEx</code>函数，这个跟普通线程没什么区别。不过在创建的<code>attribute</code>参数中需要设置<code>PROC_THREAD_ATTRIBUTE_UMS_THREAD</code>属性，并将通过<code>CreateUmsCompletionList</code>创建的完成列表，传递给<code>UMS_CREATE_THREAD_ATTRIBUTES</code>类型的参数。例如：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">PPROC_THREAD_ATTRIBUTE_LIST attribute_list = </span><br><span class="line">    (PPROC_THREAD_ATTRIBUTE_LIST) HeapAlloc(GetProcessHeap(), </span><br><span class="line">                                            <span class="number">0</span>, </span><br><span class="line">                                            attribute_list_size);</span><br><span class="line">InitializeProcThreadAttributeList(attribute_list, <span class="number">1</span>, <span class="number">0</span>, &amp;attribute_list_size);</span><br><span class="line"></span><br><span class="line">UMS_CREATE_THREAD_ATTRIBUTES ums_thread_attributes;</span><br><span class="line">ums_thread_attributes.UmsVersion = UMS_VERSION;</span><br><span class="line">ums_thread_attributes.UmsContext = ums_context;</span><br><span class="line">ums_thread_attributes.UmsCompletionList = scheduler_completion_list;</span><br><span class="line">UpdateProcThreadAttribute(attribute_list, </span><br><span class="line">                          <span class="number">0</span>, </span><br><span class="line">                          PROC_THREAD_ATTRIBUTE_UMS_THREAD, </span><br><span class="line">                          &amp;ums_thread_attributes, </span><br><span class="line">                          <span class="keyword">sizeof</span>(ums_thread_attributes), </span><br><span class="line">                          <span class="literal">NULL</span>, </span><br><span class="line">                          <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">HANDLE thread = CreateRemoteThreadEx(GetCurrentProcess(), </span><br><span class="line">                                     <span class="literal">NULL</span>, </span><br><span class="line">                                     stack_size, </span><br><span class="line">                                     function, </span><br><span class="line">                                     parameter, </span><br><span class="line">                                     STACK_SIZE_PARAM_IS_A_RESERVATION, </span><br><span class="line">                                     attribute_list, </span><br><span class="line">                                     <span class="literal">NULL</span>);</span><br></pre></td></tr></table></figure><p>应用的UMS调度器线程负责创建，管理和删除UMS工作线程并调度运行的UMS线程。它的创建过程是：通过<code>CreateThread</code>启动普通的线程，然后调用<code>EnterUmsSchedulingMode</code>函数将自身转换为UMS调度器线程类型：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">DWORD WINAPI <span class="title">SchedulerThreadFunction</span><span class="params">(<span class="keyword">void</span> *parameter)</span> </span>&#123;</span><br><span class="line">    UMS_SCHEDULER_STARTUP_INFO scheduler_info;</span><br><span class="line">    scheduler_info.UmsVersion = UMS_VERSION;</span><br><span class="line">    scheduler_info.CompletionList = scheduler_completion_list;</span><br><span class="line">    scheduler_info.SchedulerProc = SchedulerCallback;</span><br><span class="line">    scheduler_info.SchedulerParam = <span class="literal">NULL</span>;</span><br><span class="line">    BOOL result = EnterUmsSchedulingMode(&amp;scheduler_info);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们都知道，普通的原生线程在通过<code>CreateThread</code>创建后默认会直接参与调度并执行，而在UMS模式下，新创建的工作线程默认时不会马上运行，需要等到调度器线程选择它，并通过<code>ExecuteUmsThread</code>函数运行，例如：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (ready_queue.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    PUMS_CONTEXT runnable_thread = ready_queue.front();</span><br><span class="line">    ready_queue.pop_front();</span><br><span class="line"></span><br><span class="line">    BOOLEAN terminated = FALSE;</span><br><span class="line">    ExecuteUmsThread(runnable_thread);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="调度子程序入口"><a href="#调度子程序入口" class="headerlink" title="调度子程序入口"></a>调度子程序入口</h3><p>刚才我们的调度器线程函数中，设定了调度回调函数，<code>SchedulerCallback</code>，该函数是<code>UmsSchedulerProc</code>类型，具有如下的原型。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">void WINAPI UmsSchedulerProc(</span><br><span class="line">    UMS_SCHEDULER_REASON reason, </span><br><span class="line">    ULONG_PTR payload, </span><br><span class="line">    void *parameter) &#123;</span><br></pre></td></tr></table></figure></p><p>该函数在如下三个时刻由系统自动的触发执行：</p><ol><li><p>通过调用<code>EnterUmsSchedulingMode</code>将非UMS线程转换为UMS调度线程时：</p> <img src="/2018/08/27/using-Windows-UMS-for-lightweight-threading-model/creation.svg" title="转换成调度线程时"></li><li><p>当UMS工作线程调用<code>UmsThreadYield</code>，主动放弃CPU的执行权时：</p> <img src="/2018/08/27/using-Windows-UMS-for-lightweight-threading-model/yield.svg" title="线程调用UmsThreadYield时"></li><li><p>当UMS工作线程调用阻塞的系统服务（如系统调用或页面错误）时：</p> <img src="/2018/08/27/using-Windows-UMS-for-lightweight-threading-model/syscall.svg" title="线程调用阻塞的系统服务"></li></ol><p><code>UmsSchedulerProc</code>函数的<code>Reason</code>参数指定调用入口点函数的上述三种不同的原因之一，以便于调度子程序能够根据不同的原因，进行不同的后续调度策略，例如：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span> (reason) &#123;</span><br><span class="line">    <span class="keyword">case</span> UmsSchedulerStartup:</span><br><span class="line">        SetEvent(scheduler_initialized_event);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> UmsSchedulerThreadBlocked: &#123;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> UmsSchedulerThreadYield: &#123;</span><br><span class="line">        PUMS_CONTEXT yielded_thread = (PUMS_CONTEXT) payload;</span><br><span class="line">        <span class="keyword">void</span> *yielded_parameter = parameter;</span><br><span class="line">        ready_queue.push_back(yielded_thread);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="UMS最佳实践"><a href="#UMS最佳实践" class="headerlink" title="UMS最佳实践"></a>UMS最佳实践</h3><p>在实现UMS的应用程序时应遵循以下最佳实践：</p><ol><li>UMS线程上下文的基础结构需由系统进行管理，不应直接修改，而是使用<code>QueryUmsThreadInformation</code>和<code>SetUmsThreadInformation</code>来检索和设置有关UMS工作线程的信息。</li><li>为了防止死锁，UMS调度器线程不应与UMS工作线程共享锁，这包括应用程序创建的锁和通过诸如从堆分配或加载DLL等操作间接获取的系统锁。</li><li>当大多数处理和计算在用户模式下完成时，UMS是最高效的，因为它尽可能避免在UMS工作线程中进行系统调用。</li><li>UMS工作线程不应假设正在使用系统调度程序，而应该考虑是被UMS调度器线程所调度。因此，不应使用系统API设置线程的优先级或亲和性。</li><li>系统可能需要锁定UMS工作线程的线程上下文。如果调度器线程在工作线程被锁定时尝试执行该线程，则调用将失败。所以调度器线程设计为，重试对该工作线程上下文的访问。</li></ol><h2 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h2><p>按照仓库中的<a href="https://github.com/zhangpf/cloud-demos/blob/master/windows-ums/README.md" target="_blank" rel="noopener">文档</a>进行编译并运行测试程序。在我的笔记本（Lenovo Thinkpad X270，Intel i5-6200U的4核处理器和8G主存）上:</p><ol><li><strong>10</strong>个线程并发：</li></ol><table><thead><tr><th style="text-align:center">yield数量</th><th style="text-align:center">100</th><th style="text-align:center">1000</th><th style="text-align:center">10000</th><th style="text-align:center">100000</th></tr></thead><tbody><tr><td style="text-align:center">Native thread</td><td style="text-align:center">1201ns</td><td style="text-align:center">633ns</td><td style="text-align:center">640ns</td><td style="text-align:center">632ns</td></tr><tr><td style="text-align:center"><strong>UMS</strong></td><td style="text-align:center">2752ns</td><td style="text-align:center">400ns</td><td style="text-align:center">148ns</td><td style="text-align:center">118ns</td></tr><tr><td style="text-align:center">Fiber</td><td style="text-align:center">96ns</td><td style="text-align:center">105ns</td><td style="text-align:center">101ns</td><td style="text-align:center">88ns</td></tr></tbody></table><ol start="2"><li><strong>100</strong>个线程并发：</li></ol><table><thead><tr><th style="text-align:center">yield数量</th><th style="text-align:center">100</th><th style="text-align:center">1000</th><th style="text-align:center">10000</th><th style="text-align:center">100000</th></tr></thead><tbody><tr><td style="text-align:center">Native thread</td><td style="text-align:center">769ns</td><td style="text-align:center">610ns</td><td style="text-align:center">601ns</td><td style="text-align:center">591ns</td></tr><tr><td style="text-align:center"><strong>UMS</strong></td><td style="text-align:center">1428ns</td><td style="text-align:center">245ns</td><td style="text-align:center">152ns</td><td style="text-align:center">128ns</td></tr><tr><td style="text-align:center">Fiber</td><td style="text-align:center">130ns</td><td style="text-align:center">105ns</td><td style="text-align:center">102ns</td><td style="text-align:center">102ns</td></tr></tbody></table><ol start="3"><li><strong>1000</strong>个线程并发：</li></ol><table><thead><tr><th style="text-align:center">yield数量</th><th style="text-align:center">100</th><th style="text-align:center">1000</th><th style="text-align:center">10000</th><th style="text-align:center">100000</th></tr></thead><tbody><tr><td style="text-align:center">Native thread</td><td style="text-align:center">941ns</td><td style="text-align:center">790ns</td><td style="text-align:center">793ns</td><td style="text-align:center">785ns</td></tr><tr><td style="text-align:center"><strong>UMS</strong></td><td style="text-align:center">1400ns</td><td style="text-align:center">276ns</td><td style="text-align:center">146ns</td><td style="text-align:center">127ns</td></tr><tr><td style="text-align:center">Fiber</td><td style="text-align:center">175ns</td><td style="text-align:center">167ns</td><td style="text-align:center">177ns</td><td style="text-align:center">180ns</td></tr></tbody></table><p>在少量线程（10或100）的情况下，Fiber要比UMS性能好一些，不过在1000个线程的情况下UMS的实现比Fiber有一定的提升。不过两者比原生的线程（Native thread）相比，还是有很大的提高。另外要值得说的是，从资源管理器里看，UMS似乎是只能在单核上并发，无法像原生线程那样直接利用多核，如果将原生线程那样也</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过一个简单的性能对比，我们可以看到UMS在提升并发性能上比原生线程的调度要高出不少。但是，我们还是需要看到，UMS还是由一些方面的不足：</p><ol><li>自定义的线程调度使编码变得复杂，相对于原生线程和Fiber的实现，UMS的代码量大大增加了。所以改进方式是把相关的系统调用函数封装成易于调用的库，对上层提供透明的编程模块。</li><li>性能对比里已经提到，UMS还是像Fiber那样只能利用单核，多核的扩展还是需要更多的支持。</li></ol><p>从Win7开始，Windows提供UMS相关的功能已经有10年的时间，不过尚未看到该技术有大规模使用的案例，这也是其比较遗憾的一方面。不过这种通过自定义调度器的解决方法是值得借鉴的，因为它为利用原生线程提供大规模并发访问找到了一条可行的方式，并且给我们提供更多的思路来实现轻量级线程。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前几天在看关于Rust取消M:N线程模型背后的理性选择时候，看到&lt;a href=&quot;https://github.com/thestinger&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Daniel Micay&lt;/a&gt;
的一篇帖子中提到了Windows的
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="并发" scheme="https://blog.zhangpf.com/tags/%E5%B9%B6%E5%8F%91/"/>
    
      <category term="云计算" scheme="https://blog.zhangpf.com/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Windows" scheme="https://blog.zhangpf.com/tags/Windows/"/>
    
      <category term="UMS" scheme="https://blog.zhangpf.com/tags/UMS/"/>
    
  </entry>
  
  <entry>
    <title>使用Rust编写用户态驱动程序</title>
    <link href="https://blog.zhangpf.com/2018/08/19/write-userspace-driver-in-rust/"/>
    <id>https://blog.zhangpf.com/2018/08/19/write-userspace-driver-in-rust/</id>
    <published>2018-08-19T14:06:46.000Z</published>
    <updated>2020-01-29T02:19:30.156Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>在云计算技术的发展史上，如何提高单个服务器的并发度，一直是热门的研究课题。在20年前，就有著名的“<a href="http://www.kegel.com/c10k.html" target="_blank" rel="noopener">C10K</a>”问题，即如何利用单个服务器每秒应对10K个客户端的同时访问。这么多年大量的实践证明，异步处理和基于事件（即epoll，kqueue和iocp）的响应方式成为处理这类问题的事实上标准方法。</p><p>不过，人类的追求是永无至今的。15年后，当摩尔定律在硬件上的理论提升有1000倍时，有人对并发数量提出了更高的要求，”C10K”升级为”<a href="http://c10m.robertgraham.com/p/manifesto.html" target="_blank" rel="noopener">C10M</a>“问题，即每秒应对10M个客户端的同时访问。咋眼一看，怎么会有这样的服务，需要每秒处理上千万的并发？实际上这样的需求是广泛存在的，典型的例子就是DNS服务器、网络授时服务以及基于内存的key-value服务器。这种服务的特点是，一次客户请求涉及的计算量可能会很少，大部分时间均花在了IO上。所以根据<a href="https://zh.wikipedia.org/wiki/%E9%98%BF%E5%A7%86%E8%BE%BE%E5%B0%94%E5%AE%9A%E5%BE%8B" target="_blank" rel="noopener">Amdahl定律</a>，优化的重点需放在如何减少I/O路径上的开销。</p><p>最早提出”C10M”问题的Robert Graham认为，减少开销的关键之一在于绕过操作系统，即”kernel bypass”，因为我们使用的操作系统在设计之初并没有考虑高并发的场景，而I/O路径上的大部分例程又在内核空间中，大量无谓的消耗花在了内核空间和用户空间上下文的切换上。解决的方法就是将I/O路径（对于网络请求来讲，就是驱动和网络协议栈）全部实现在用户空间，这样可以最大程度的减少内核的干预，并且通过轮询(polling)而不是硬件中断的方法来获取网卡上的请求（而对于存储器来讲，就是complete信息）。再结合其他优化方法，例如协程和零拷贝技术，可以将并发性能优化到极致，具体请见<a href="https://www.oschina.net/translate/the-secret-to-10-million-concurrent-connections-the-kernel?cmp&amp;p=1" target="_blank" rel="noopener">“内核才是问题的根本”</a>。</p><p>基于这样的背景，一种未来的趋势是出更多的硬件驱动将在用户空间中实现，而这种趋势似乎正在慢慢成为现实。例如Intel的DPDK相关的技术，以及RDMA和XDP，都是此类思路的具体实践。在本文中，我们将尝试用<a href="https://www.rust-lang.org/zh-CN/" target="_blank" rel="noopener">Rust</a>语言来实现一个极其简单的Intel ixgbe 10G网络控制器驱动，并在编写测试程序测试其基础性能。</p><p>需要特别说明的是，本文的目的之一是探寻Rust语言编写驱动的优缺点，所以对于具体的网络接口的硬件细节关注较少，所以实现基本上是在<a href="https://github.com/emmericp/ixy" target="_blank" rel="noopener">C语言版本的驱动emmericp/ixy</a>的基础上进行Rust移植。本文的相关代码请移步<a href="https://github.com/zhangpf/nic-drivers" target="_blank" rel="noopener">Github仓库</a>。</p><h2 id="为什么用Rust？"><a href="#为什么用Rust？" class="headerlink" title="为什么用Rust？"></a>为什么用Rust？</h2><p>Rust是一款能够保证安全和较高性能的静态编译型语言，其目标在于取代C，成为系统软件的主要实现语言。Rust充分利用了LLVM等最新的编译优化和静态分析技术，能够将<strong>安全和性能</strong>，这两个看似矛盾的目标很好的结合在一起，而我认为<strong>这正是驱动程序所不断追求的两个目标</strong>。几乎所有的安全检查都是在编译的过程中通过静态分析加以解决，如有违反，则编译立刻停止返回失败，因此避免了运行时的额外开销。Rust提供如下三个方面的安全性：</p><ul><li>内存安全：Rust具有完整的内存生命周期检查，保证了一块区域的内存不会在其生命周期之外被引用，同时引入了所有权和<code>borrow</code>机制，使得变量要么处于共享只读，要么处于互斥写状态。另外，Rust也不允许空指针和悬空指针，所有变量需经过初始化才能使用；</li><li>类型安全：Rust是强类型语言，任何形式的类型转换都需要开发者进行显式的实现；</li><li>并发安全：因为Rust的所有权机制，使得变量和内存能够在多个线程之间进行传递和共享，而不用担心数据竞争的问题。</li></ul><p>不过要指出的是，Rust为了能够与C中的函数进行互操作，以及更好地进行其他“非安全”的操作（例如指针运算，裸指针的解引用等），提供了<code>unsafe</code>关键字进行支持，同时也在代码中显式地指出这个地方可能会出现安全性问题。</p><h2 id="驱动实现"><a href="#驱动实现" class="headerlink" title="驱动实现"></a>驱动实现</h2><p>对于高性能计算中，通常的一种内存使用的优化方法是使用页面大小为2MB或1GB的巨页（hugepage），其好处在于：</p><ul><li>减少缺页中断的次数，减少前文中提到内核空间和用户空间的上下文切换带来的开销；</li><li>AMD64位处理器上的hugepage页表只有2-3层，可以减少MMU巡表时间，同时也能减少页表项的个数，便于TLB的缓存。</li></ul><p>对于ixgbe驱动，同样也需要hugepage的支持。不过在Linux下，需要手动通过写sys文件系统进行开启，例如：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /mnt/huge</span><br><span class="line">mount -t hugetlbfs hugetlbfs /mnt/huge</span><br><span class="line">echo 512 &gt; /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages</span><br></pre></td></tr></table></figure></p><h3 id="关键数据结构"><a href="#关键数据结构" class="headerlink" title="关键数据结构"></a>关键数据结构</h3><h4 id="DeviceInfo"><a href="#DeviceInfo" class="headerlink" title="DeviceInfo"></a><code>DeviceInfo</code></h4><p>对于大多数的NIC网卡来讲，需要不同的方式来分别处理流入(receive, RX)和流出(transport, TX)的流量。而为了增加数据的并行度，对于高速网卡而言，通常每种模式可以设置多个队列（例如64）以流水线的方式存储数据，因此顶层的数据结构<code>DeviceInfo</code>包含这两种模式的不同队列：</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">pub</span> <span class="class"><span class="keyword">struct</span> <span class="title">DeviceInfo</span></span> &#123;</span><br><span class="line">    num_rx_queues: <span class="built_in">u32</span>,</span><br><span class="line">    num_tx_queues: <span class="built_in">u32</span>,</span><br><span class="line">    rx_queues: <span class="built_in">Vec</span>&lt;RxQueue&gt;,</span><br><span class="line">    tx_queues: <span class="built_in">Vec</span>&lt;TxQueue&gt;,</span><br><span class="line">    addr: *<span class="keyword">mut</span> <span class="built_in">u8</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>addr</code>中存放的是网卡信息在内存中的映射地址，需要通过libc中的mmap操作获取得到。而在mmap操作之前，需要先知道到网卡在内核中的文件的句柄fd值，一种比较标准的做法是通过sys文件系统去读取对于pci地址上的设备信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">let file = open(format!(&quot;/sys/bus/pci/devices/&#123;&#125;/resource0&quot;, pci_addr));</span><br><span class="line">let addr = libc::mmap(..., file.as_raw_fd(), ...);</span><br></pre></td></tr></table></figure><h4 id="RxQueue和TxQueue"><a href="#RxQueue和TxQueue" class="headerlink" title="RxQueue和TxQueue"></a><code>RxQueue</code>和<code>TxQueue</code></h4><p><code>RxQueue</code>和<code>TxQueue</code>结构体的定义分别如下：<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">RxQueue</span></span> &#123;</span><br><span class="line">    descriptors: *<span class="keyword">const</span> <span class="built_in">u8</span>,</span><br><span class="line">    mempool: RefCell&lt;Mempool&gt;,</span><br><span class="line">    num_entries: <span class="built_in">u16</span>,</span><br><span class="line">    <span class="comment">// position we are reading from</span></span><br><span class="line">    rx_index: <span class="built_in">u16</span>,</span><br><span class="line">    <span class="comment">// virtual addresses to map descriptors back to their mbuf for freeing</span></span><br><span class="line">    virtual_addresses: <span class="built_in">Vec</span>&lt;*<span class="keyword">mut</span> Buffer&gt;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TxQueue</span></span> &#123;</span><br><span class="line">    descriptors: *<span class="keyword">const</span> <span class="built_in">u8</span>,</span><br><span class="line">    num_entries: <span class="built_in">u16</span>,</span><br><span class="line">    <span class="comment">// position to clean up descriptors that where sent out by the nic</span></span><br><span class="line">    clean_index: <span class="built_in">u16</span>,</span><br><span class="line">    <span class="comment">// position to insert packets for transmission</span></span><br><span class="line">    tx_index: <span class="built_in">u16</span>,</span><br><span class="line">    <span class="comment">// virtual addresses to map descriptors back to their mbuf for freeing</span></span><br><span class="line">    virtual_addresses: <span class="built_in">Vec</span>&lt;*<span class="keyword">mut</span> Buffer&gt;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>对<code>RxQueue</code>而言：</p><ul><li><code>descriptors</code>：分配的DMA内存的起始地址，</li><li><code>mempool</code>：全局内存池的地址</li><li><code>num_entries</code>：RX中队列数量</li><li><code>rx_index</code>: 当前处理的队列序号</li><li><code>virtual_address</code>：队列集合</li></ul><p>同样地，对于<code>TxQueue</code>也有相似的数据项。</p><h4 id="Buffer和Mempool"><a href="#Buffer和Mempool" class="headerlink" title="Buffer和Mempool"></a><code>Buffer</code>和<code>Mempool</code></h4><p>为了能够很好地管理DMA内存，对于通过hugepage申请到的内存页面，我们通过<code>Mempool</code>数据结构进行管理，其内部的结构非常简单，并且对外有明确的结构，即分配(alloc)和回收(free)网卡数据包内存。</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">pub</span> <span class="class"><span class="keyword">struct</span> <span class="title">Mempool</span></span> &#123;</span><br><span class="line">    free_stack: <span class="built_in">Vec</span>&lt;*<span class="keyword">mut</span> Buffer&gt;,</span><br><span class="line">    free_stack_top: <span class="built_in">u32</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> Mempool &#123;</span><br><span class="line">    <span class="keyword">pub</span> <span class="function"><span class="keyword">fn</span> <span class="title">alloc_buf</span></span>(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>) -&gt; <span class="built_in">Option</span>&lt;*<span class="keyword">mut</span> Buffer&gt;;</span><br><span class="line">    <span class="keyword">pub</span> <span class="function"><span class="keyword">fn</span> <span class="title">free_buf</span></span>(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>, buf: *<span class="keyword">mut</span> Buffer);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从网卡流入和流出的数据包，以及存放数据的具体位置，在<code>Buffer</code>结构体中定义：</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#[repr(C)]</span></span><br><span class="line"><span class="keyword">pub</span> <span class="class"><span class="keyword">struct</span> <span class="title">Buffer</span></span> &#123;</span><br><span class="line">    <span class="comment">// physical address to pass a buffer to a nic</span></span><br><span class="line">    buf_addr_phys: <span class="built_in">usize</span>,</span><br><span class="line">    <span class="keyword">pub</span> mempool: *<span class="keyword">mut</span> Mempool,</span><br><span class="line">    idx: <span class="built_in">u32</span>,</span><br><span class="line">    <span class="keyword">pub</span> size: <span class="built_in">u32</span>,</span><br><span class="line">    head_room: [<span class="built_in">u8</span>; SIZE_PKT_BUF_HEADROOM <span class="keyword">as</span> <span class="built_in">usize</span>],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="其他数据结构"><a href="#其他数据结构" class="headerlink" title="其他数据结构"></a>其他数据结构</h4><p>Stat统计RX和TX分别处理了多少个数据包和相应的字节数。<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">pub</span> <span class="class"><span class="keyword">struct</span> <span class="title">Stats</span></span> &#123;</span><br><span class="line">    rx_pkts: <span class="built_in">u32</span>,</span><br><span class="line">    tx_pkts: <span class="built_in">u32</span>,</span><br><span class="line">    rx_bytes: <span class="built_in">u64</span>,</span><br><span class="line">    tx_bytes: <span class="built_in">u64</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>另外，还有一些数据结构，主要是封装了硬件相关的数据，举个例子：<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#[repr(C)]</span></span><br><span class="line"><span class="meta">#[derive(Clone, Copy)]</span></span><br><span class="line"><span class="class"><span class="keyword">union</span> <span class="title">AdvTxDesc</span></span> &#123;</span><br><span class="line">    read: TxAddr,</span><br><span class="line">    wb: TxWriteback,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这里，<code>read</code>和<code>wb</code>分别表示同一块内存地址在不同模式下，具有不同的状态信息。所以这里我们使用了union结构。</p><h3 id="使用宏（Macro）简化底层操作"><a href="#使用宏（Macro）简化底层操作" class="headerlink" title="使用宏（Macro）简化底层操作"></a>使用宏（Macro）简化底层操作</h3><p>硬件驱动的另一个主要职责是，以合乎硬件手册的规范的方式来操纵寄存器和内存映射地址，而它需要大量的繁琐的代码。在C语言中，通常使用<code>#define</code>来定义这些宏，例如：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* Split and Replication Receive Control Registers</span></span><br><span class="line"><span class="comment">* 00-15 : 0x02100 + n*4</span></span><br><span class="line"><span class="comment">* 16-64 : 0x01014 + n*0x40</span></span><br><span class="line"><span class="comment">* 64-127: 0x0D014 + (n-64)*0x40</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> IXGBE_SRRCTL(_i)(((_i) &lt;= 15) ? (0x02100 + ((_i) * 4)) : \</span></span><br><span class="line"> (((_i) &lt; <span class="number">64</span>) ? (<span class="number">0x01014</span> + ((_i) * <span class="number">0x40</span>)) : \</span><br><span class="line"> (<span class="number">0x0D014</span> + (((_i) - <span class="number">64</span>) * <span class="number">0x40</span>))))</span><br></pre></td></tr></table></figure><p>而在Rust语言中，宏的定义也有相应的方式，即关键字<code>macro_rules</code>，所以上面的内存地址的访问，在Rust中等价的表达如下：</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">macro_rules!</span> IXGBE_SRRCTL &#123;</span><br><span class="line">    ($_i:expr) =&gt; &#123;</span><br><span class="line">        <span class="keyword">match</span> ($_i) &lt;= <span class="number">15</span> &#123;</span><br><span class="line">            <span class="literal">true</span> =&gt; (<span class="number">0x02100</span> + (($_i) * <span class="number">4</span>)),</span><br><span class="line">            <span class="literal">false</span> =&gt; <span class="keyword">match</span> ($_i) &lt; <span class="number">64</span> &#123;</span><br><span class="line">                <span class="literal">true</span> =&gt; (<span class="number">0x01014</span> + (($_i) * <span class="number">0x40</span>)),</span><br><span class="line">                <span class="literal">false</span> =&gt; (<span class="number">0x0D014</span> + ((($_i) - <span class="number">64</span>) * <span class="number">0x40</span>)),</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外，相对于C，Rust中定义宏还有个好处是，它具有清晰的语义，所有传入宏里的表达式参数均是<a href="https://bit.ly/2LgAAtD" target="_blank" rel="noopener">先<code>eval</code>之后再参与计算</a>，避免了诸如C中的下列歧义问题，所以建议大家多使用Rust中的宏来简化和更清晰地表达。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> test(i) i * 2</span></span><br><span class="line">test(<span class="number">1</span> + <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="性能测试和总结"><a href="#性能测试和总结" class="headerlink" title="性能测试和总结"></a>性能测试和总结</h2><p>按照仓库中的<a href="https://github.com/zhangpf/nic-drivers#build-and-run" target="_blank" rel="noopener">文档</a>进行编译并运行pktgen测试程序，代码基本上重现了<a href="https://github.com/emmericp/ixy" target="_blank" rel="noopener">ixy</a>的实验结果。</p><p>在我的实验机器（2* Xeon E5-2640 + 64GB mem + Intel 82599ES网卡）上，<code>pktgen</code>运行的结果如下所示：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">virt: 7fccda600000, phys: 816400000</span><br><span class="line">No driver loaded</span><br><span class="line">Resetting device 0000:01:00.0</span><br><span class="line">Initializing device 0000:01:00.0</span><br><span class="line">initializing rx queue 0</span><br><span class="line">virt: 7fccda400000, phys: 816800000</span><br><span class="line">rx ring 0 phy addr:  816800000</span><br><span class="line">rx ring 0 virt addr: 7FCCDA400000</span><br><span class="line">virt: 7fccd9c00000, phys: 816e00000</span><br><span class="line">initializing tx queue 0</span><br><span class="line">virt: 7fccd9a00000, phys: 817000000</span><br><span class="line">tx ring 0 phy addr:  817000000</span><br><span class="line">tx ring 0 virt addr: 7FCCD9A00000</span><br><span class="line">starting rx queue 0</span><br><span class="line">starting queue 0</span><br><span class="line">enabling promisc mode</span><br><span class="line">Waiting for link...</span><br><span class="line">Link speed is 10000 Mbit/s</span><br><span class="line">RX: 0 Mbit/s 0 Mpps</span><br><span class="line"></span><br><span class="line">TX: 9901.384292164801 Mbit/s 14.734186286261325 Mpps</span><br><span class="line"></span><br><span class="line">RX: 0.0024058573361193836 Mbit/s 0.000001991603755065715 Mpps</span><br><span class="line"></span><br><span class="line">TX: 9999.00754202517 Mbit/s 14.879477785084605 Mpps</span><br><span class="line"></span><br><span class="line">RX: 0.0011392588353670422 Mbit/s 0.000000995855625320841 Mpps</span><br><span class="line"></span><br><span class="line">TX: 9999.552267294279 Mbit/s 14.880286870792201 Mpps</span><br><span class="line"></span><br><span class="line">RX: 0 Mbit/s 0 Mpps</span><br><span class="line"></span><br><span class="line">TX: 9998.990842343424 Mbit/s 14.879450658249143 Mpps</span><br></pre></td></tr></table></figure></p><p>由结果可以看出，TX基本上跑满10Gb的带宽，所以由Rust实现驱动在性能上能够和C不相上下。</p><p>但是，当前的实现中还有许多值得改进的地方，比如：</p><ul><li>在Rust中，通常不应该有自己实现的内存分配器，更不应该有显式地的<code>free</code>类型的操作。不过因为我们使用了Hugepage来处理底层内存管理，所以这部分必须要自己实现，一种更优雅的做法是实现Rust中的<a href="https://doc.rust-lang.org/alloc/alloc/trait.Alloc.html" target="_blank" rel="noopener"><code>alloc::alloc::Alloc</code></a>类型的trait，以及相应的函数实现，以便于与其它的库很好的兼容，实现内存的自动管理。</li></ul><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Alloc</span></span> &#123;</span><br><span class="line">    <span class="keyword">unsafe</span> <span class="function"><span class="keyword">fn</span> <span class="title">alloc</span></span>(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>, layout: Layout) -&gt; <span class="built_in">Result</span>&lt;NonNull&lt;<span class="built_in">u8</span>&gt;, AllocErr&gt;;</span><br><span class="line">    <span class="keyword">unsafe</span> <span class="function"><span class="keyword">fn</span> <span class="title">dealloc</span></span>(&amp;<span class="keyword">mut</span> <span class="keyword">self</span>, ptr: NonNull&lt;<span class="built_in">u8</span>&gt;, layout: Layout);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>代码中的有些地方并不符合Rust的风格，例如在<code>DeviceInfo</code>中，<code>rx_queues: Vec&lt;RxQueue&gt;</code>项已经包含了队列的长度信息，不应该再添加重复的<code>num_rx_queues: u32</code>。</li></ul><p>以上问题在以后优化中将持续改进。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概览&quot;&gt;&lt;a href=&quot;#概览&quot; class=&quot;headerlink&quot; title=&quot;概览&quot;&gt;&lt;/a&gt;概览&lt;/h2&gt;&lt;p&gt;在云计算技术的发展史上，如何提高单个服务器的并发度，一直是热门的研究课题。在20年前，就有著名的“&lt;a href=&quot;http://www.k
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="Rust" scheme="https://blog.zhangpf.com/tags/Rust/"/>
    
      <category term="云计算" scheme="https://blog.zhangpf.com/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="驱动" scheme="https://blog.zhangpf.com/tags/%E9%A9%B1%E5%8A%A8/"/>
    
      <category term="Kernel bypass" scheme="https://blog.zhangpf.com/tags/Kernel-bypass/"/>
    
      <category term="ixgbe" scheme="https://blog.zhangpf.com/tags/ixgbe/"/>
    
  </entry>
  
  <entry>
    <title>利用Capnproto优化RPC组合操作</title>
    <link href="https://blog.zhangpf.com/2018/08/13/using-capnproto-for-operation-pipeline/"/>
    <id>https://blog.zhangpf.com/2018/08/13/using-capnproto-for-operation-pipeline/</id>
    <published>2018-08-13T00:44:34.000Z</published>
    <updated>2020-01-29T02:19:30.155Z</updated>
    
    <content type="html"><![CDATA[<!-- *原文地址： [http://blog.zhangpf.com/2018/08/13/using-capnproto-for-operation-pipeline/](http://blog.zhangpf.com/2018/08/13/using-capnproto-for-operation-pipeline/)* --><h2 id="Capnproto简介"><a href="#Capnproto简介" class="headerlink" title="Capnproto简介"></a>Capnproto简介</h2><p>Capnproto是一款号称具有”infinity faster”的RPC框架。你可以认为它是JSON，只是它直接生成了二进制格式的消息；你可以认为它是Protocol Buffer，只是它更快而已。Capnproto有多快呢？它<a href="https://capnproto.org/index.html" target="_blank" rel="noopener">主页</a>上有张图是这样的：</p><p><img src="https://capnproto.org/images/infinity-times-faster.png" alt="Capnproto宣传的&quot;infinity faster&quot;" title="The infinity faster of Capnproto"></p><p>不过连官方也表示这样的比较是不公平的，因为它仅测量了在内存中编码和解码消息的时间。Capnproto这里获得了满分，因为它根本就没有编码/解码步骤，所以我认为更合理的性能对比如下图所示。</p><p><img src="https://github.com/thekvs/cpp-serializers/raw/master/images/time.png" alt="和其他框架的对比（1）"><img src="https://github.com/thekvs/cpp-serializers/raw/master/images/time2.png" alt="和其他框架的对比（2）"></p><p>相比之下，还是相当快的。Capnproto编码既适用于数据交换格式，也适用于在内存中表示，因此一旦构建了结构，便可以直接将字节写入磁盘中。</p><h2 id="Schema语言"><a href="#Schema语言" class="headerlink" title="Schema语言"></a>Schema语言</h2><p>Capnproto通过自定义语言来实现RPC接口和相应的操作，在这一点上和ProtoBuf以及Thrift很像，如果你用过这两种语言，那么对此应该很熟悉。例如：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">@0xdbb9ad1f14bf0b36;  # unique file ID, generated by `capnp id`</span><br><span class="line"></span><br><span class="line">struct Person &#123;</span><br><span class="line">  name @0 :Text;</span><br><span class="line">  birthdate @3 :Date;</span><br><span class="line"></span><br><span class="line">  email @1 :Text;</span><br><span class="line">  phones @2 :List(PhoneNumber);</span><br><span class="line"></span><br><span class="line">  struct PhoneNumber &#123;</span><br><span class="line">    number @0 :Text;</span><br><span class="line">    type @1 :Type;</span><br><span class="line"></span><br><span class="line">    enum Type &#123;</span><br><span class="line">      mobile @0;</span><br><span class="line">      home @1;</span><br><span class="line">      work @2;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">struct Date &#123;</span><br><span class="line">  year @0 :Int16;</span><br><span class="line">  month @1 :UInt8;</span><br><span class="line">  day @2 :UInt8;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其内置的类型包括：</p><ul><li>Void: Void</li><li>Boolean: Bool</li><li>Integers: Int8, Int16, Int32, Int64</li><li>Unsigned integers: UInt8, UInt16, UInt32, UInt64</li><li>Floating-point: Float32, Float64</li><li>Blobs: Text, Data</li><li>Lists: List(T)</li></ul><p>以及常量，<code>struct</code>，<code>union</code>，<code>enum</code>，<code>group</code>等组合结构。另外，接口函数还支持泛型和泛函数等，可以说是相当强大。具体的语法请参考<a href="https://capnproto.org/language.html。" target="_blank" rel="noopener">官方文档</a>。</p><h2 id="优化RPC组合操作"><a href="#优化RPC组合操作" class="headerlink" title="优化RPC组合操作"></a>优化RPC组合操作</h2><p>不过我认为，Capnproto的优势还是体现在优化RPC组合操作上。</p><p>我们都知道，接口文件（Interface）描述了客户端和服务端之间所有的交互方式。设想一种场景，随着一个系统的不断演化，客户端新的行为需要之前从来没有过的接口操作，而这个时候，服务端相应的RPC方法，以及接口文件无法马上得到，而这个操作又恰恰可以是多个旧操作的组合。</p><p>举个例子，服务端维护一个数据库，保存的是某个网站上所有博客的内容，暴露给客户端的RPC操作仅有：</p><ul><li>根据ID获取一篇博客信息: <code>get(key)</code></li><li>根据ID删除博客内容： <code>remove(key)</code></li><li>根据ID存储相应博客信息： <code>store(key, blog)</code></li></ul><p>现在客户端需要马上实现一个新的操作：copy某个博文从key1到key2，<code>copy(key1, key2)</code>。在接口不变的情况下，我们当然可以先用<code>get</code>将blog传回客户端，再用新ID和blog进行<code>store</code>操作。</p><p>不过在Capnproto框架下，可以采取不太一样的方式。Capnproto的RPC采取一种类似于<code>Promise</code>的方法，将所有接口操作流水化，中间结果不用传回客户端，因此这样减少了一次中间结果的往返传递，同时也减少了调用延迟。</p><p><img src="https://capnproto.org/images/time-travel.png" alt="Capnproto的RPC计算可以不用等待中间结果的返回" title="The RPC procedure of Capnproto"></p><p>也就是说，原来的需要如下方式实现的<code>copy</code>操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">value = get(key1);</span><br><span class="line">store(key2, value);</span><br></pre></td></tr></table></figure><p>变成了类似如下的形式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">getPromise = get(key1);</span><br><span class="line">storePromise = store(key2, getPromise);</span><br><span class="line">storePromise.then(...);</span><br></pre></td></tr></table></figure><p>这里的中间步骤，将不再有blog数据传输。</p><p>在本文接下来的部分，我将用代码片段演示capnp接口的实现过程。完整的示例代码，请查看<a href="https://github.com/zhangpf/cloud-demos/tree/master/capnproto" target="_blank" rel="noopener">github仓库</a>。</p><h3 id="capnp接口"><a href="#capnp接口" class="headerlink" title="capnp接口"></a>capnp接口</h3><p>为了使得客户端可以惰性地获取<code>get(key)</code>操作得结果，首先定义Blog信息的interface结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">interface Blog &#123;</span><br><span class="line">    read @0 () -&gt; (blog :Text);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Blog接口具有一个操作：<code>read()</code>，调用的结果是实际的blog数据。因此，<code>get</code>不再返回<code>:Text</code>类型的数据，而是返回一个<code>:Blog</code>类型的接口。只有在调用这个接口的<code>read</code>函数之后才获取其中的值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">interface BlogStore &#123;</span><br><span class="line"></span><br><span class="line">    interface Blog &#123;</span><br><span class="line">        read @0 () -&gt; (blog :Text);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    get @0 (key :UInt64) -&gt; (blog :Blog);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同时，为了使得<code>store(key, blog)</code>操作中的blog值，能够既支持从客户端传来的数据，又支持上次<code>get</code>操作返回的Blog接口，需要定义一个Store结构体：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">struct Store &#123;</span><br><span class="line">    union &#123;</span><br><span class="line">        blog @0 :Text;</span><br><span class="line">        previousGet @1 :Blog;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个结构体中只有一个union项，表示可能的值是二者之一（<em>capnp语言中的union不能单独定义，只能在struct中出现</em>）。因此<code>store</code>操作的定义变成了如下形式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store @1 (key :UInt64, blog :Store);</span><br></pre></td></tr></table></figure><p>最后，我们再加上<code>remove(key)</code>操作的定义，整个<code>blogstore.capnp</code>文件的内容就是下面这个样子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">@0xf79af02aadd13d6d;</span><br><span class="line"></span><br><span class="line">interface BlogStore &#123;</span><br><span class="line"></span><br><span class="line">    interface Blog &#123;</span><br><span class="line">        read @0 () -&gt; (blog :Text);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    struct Store &#123;</span><br><span class="line">        union &#123;</span><br><span class="line">            blog @0 :Text;</span><br><span class="line">            previousGet @1 :Blog;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    get @0 (key :UInt64) -&gt; (blog :Blog);</span><br><span class="line"></span><br><span class="line">    store @1 (key :UInt64, blog :Store);</span><br><span class="line"></span><br><span class="line">    remove @2 (key :UInt64);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>利用capnp编译器编译<code>blogstore.capnp</code>，生成相应的<code>blogstore.capnp.h</code>和<code>blogstore.capnp.c++</code>：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">capnpc -oc++ blogstore.capnp</span><br></pre></td></tr></table></figure></p><p>在此基础上，还需要实现<a href="https://github.com/zhangpf/cloud-demos/blob/master/capnproto/client.cpp" target="_blank" rel="noopener">客户端代码</a>和<a href="https://github.com/zhangpf/cloud-demos/blob/master/capnproto/server.cpp" target="_blank" rel="noopener">服务端代码</a>，具体的教程可以参考官方的<a href="https://capnproto.org/cxxrpc.html" target="_blank" rel="noopener">RPC教程</a>。</p><h3 id="性能对比"><a href="#性能对比" class="headerlink" title="性能对比"></a>性能对比</h3><p>设定所有的blog数据均是4096字节大小的UTF-8字符串数据。在AWS的c3.large主机上，我的代码实现在不同网络结构下的性能对比：</p><table><thead><tr><th style="text-align:center">Operation</th><th style="text-align:center">Get</th><th style="text-align:center">Store</th><th style="text-align:center">Remove</th><th style="text-align:center">Copy</th></tr></thead><tbody><tr><td style="text-align:center">Unix domain socket</td><td style="text-align:center">207µs</td><td style="text-align:center">161µs</td><td style="text-align:center">152µs</td><td style="text-align:center">232µs</td></tr><tr><td style="text-align:center">Loopback device</td><td style="text-align:center">246µs</td><td style="text-align:center">163µs</td><td style="text-align:center">152µs</td><td style="text-align:center">267µs</td></tr><tr><td style="text-align:center">Local network</td><td style="text-align:center">446µs</td><td style="text-align:center">372µs</td><td style="text-align:center">301µs</td><td style="text-align:center">381µs</td></tr></tbody></table><p>一次<code>copy</code>操作所用的时间大致和<code>get</code>相当，但是远小于<code>get</code>和<code>store</code>之和。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- *原文地址： [http://blog.zhangpf.com/2018/08/13/using-capnproto-for-operation-pipeline/](http://blog.zhangpf.com/2018/08/13/using-capnproto-
      
    
    </summary>
    
      <category term="中文" scheme="https://blog.zhangpf.com/categories/%E4%B8%AD%E6%96%87/"/>
    
    
      <category term="云计算" scheme="https://blog.zhangpf.com/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Capnproto" scheme="https://blog.zhangpf.com/tags/Capnproto/"/>
    
      <category term="RPC" scheme="https://blog.zhangpf.com/tags/RPC/"/>
    
  </entry>
  
  <entry>
    <title>hello</title>
    <link href="https://blog.zhangpf.com/2018/08/11/hello/"/>
    <id>https://blog.zhangpf.com/2018/08/11/hello/</id>
    <published>2018-08-11T07:17:25.000Z</published>
    <updated>2020-01-29T02:19:30.148Z</updated>
    
    <content type="html"><![CDATA[<p>Hello, Vincent Vega.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hello, Vincent Vega.&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
</feed>
